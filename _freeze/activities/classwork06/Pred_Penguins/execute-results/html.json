{
  "hash": "1dd8cfe5d6099815cdfb16e511b93781",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning using tidymodels\"\nauthor: \"Lindsay N. Hayes\"\ndate: 2025-07-16\ndraft: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n## About the activity\n\n1)  Access the Quarto document [here](https://github.com/lindsaynhayes/introR2025/blob/main/activities/classwork06/Pred_Penguins.qmd).\n\n2)  Download the raw file.\n\n3) Open it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will using 2 classification models to predict the species of penguin based on the penguin biometric data. \n\n## Load the Packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(ranger)\n```\n:::\n\n\n## Explore the Data\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n\n```{.r .cell-code}\npenguins |> count(species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n```\n\n\n:::\n:::\n\n\n## Prep the Data\n\n::: {.cell}\n\n```{.r .cell-code}\n# set a seed in order to make the analysis reproducible.\nset.seed(462)\n\n# split the data into training and testing sets. We will train the model on the training set and then test how well it worked on the testing data.\n\n# split the data 70% for training and 30% for testing. The bulk of the data is usually used for training the models. \nsplit_data <- initial_split(penguins, prop=0.7, strata = species) \ndata_training <- training(split_data)\ndata_testing <- testing(split_data)\n\n# lets check it did the split correctly, if a different seed was used a the splits would be slightly different.  \n\ndata_training |> \n  group_by(species) |>\n  summarise( count = n(),\n             percent = n()/nrow(data_training) * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  species   count percent\n  <fct>     <int>   <dbl>\n1 Adelie      106    44.4\n2 Chinstrap    47    19.7\n3 Gentoo       86    36.0\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_testing |> \n  group_by(species) |>\n  summarise( count = n(),\n             percent = n()/nrow(data_testing) * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  species   count percent\n  <fct>     <int>   <dbl>\n1 Adelie       46    43.8\n2 Chinstrap    21    20  \n3 Gentoo       38    36.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# The recipe sets up what data we are going to use and how it to be treated before doing the modeling.\npenguin_recipe <-\n  recipe( species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins) %>%\n  step_normalize(all_predictors())\n\npenguin_recipe\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: all_predictors()\n```\n\n\n:::\n\n```{.r .cell-code}\n# The prep step pulls in all the variables from the recipe based on the dataset we give it. \ndata_prep <- prep(penguin_recipe, data_training)\ndata_prep\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Training information \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nTraining data contained 239 data points and no incomplete rows.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: bill_length_mm bill_depth_mm, ... | Trained\n```\n\n\n:::\n\n```{.r .cell-code}\n# the bake steps preforms the prep steps and in this case normalizes all the data.\ndata_bake <- bake(data_prep, new_data = NULL)\ndata_bake\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 239 × 5\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g species\n            <dbl>         <dbl>             <dbl>       <dbl> <fct>  \n 1         -0.808        0.120             -1.10      -0.501  Adelie \n 2         -0.660        0.426             -0.447     -1.21   Adelie \n 3         -0.845        1.75              -0.812     -0.695  Adelie \n 4         -0.918        0.324             -1.47      -0.727  Adelie \n 5         -0.863        1.24              -0.447      0.625  Adelie \n 6         -1.80         0.477             -0.593     -0.920  Adelie \n 7         -0.346        1.55              -0.812      0.0780 Adelie \n 8         -1.12        -0.0333            -1.10      -1.15   Adelie \n 9         -1.12         0.0687            -1.54      -0.630  Adelie \n10         -0.974        2.06              -0.739     -0.501  Adelie \n# ℹ 229 more rows\n```\n\n\n:::\n:::\n\n\n\n## Define the models\n\n**Random Forest** uses the command `rand_forest()` which takes the following arguments. We will use the defaults for some values. \n\n- **mode** options are \"unknown\", \"regression\", \"classification\", or \"censored regression\"\n- **engine** options are \"ranger\", \"randomForest\", or \"spark\"\n- **mtry** the number of predictors that will be randomly sampled at each split when creating the tree model. \n- **trees** the number of trees to build. \n- **min_n** the minimum number of data points in a node to stop splitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MODEL 1 Random Forest\nrf_model <- \n  \n  # specify model\n  rand_forest() |>\n  \n  # mode as classification not continuous\n  set_mode(\"classification\") |>\n  \n  # engine/package that underlies the model (ranger is default)\n  set_engine(\"ranger\") |>\n  \n  # we only have 4 predictors so mtry can't be more than 4\n  set_args(mtry = 4, trees = 200)\n  \n\n# Put everything together \nrf_wflow <- \n  workflow() |>\n  add_recipe(penguin_recipe) |>\n  add_model(rf_model)\n\n\n# train the model\nrf_fit <- fit(rf_wflow, data_training)\n```\n:::\n\n\n\n**Logistic Regression** uses the command `multinom_reg()` which takes the following arguments. We will use the defaults for some values. \n\n- **mode** only \"classification\" is available\n- **engine** options are \"nnet\", \"brulee\", \"glmnet\", \"h2o\", \"keras\", \"spark\"\n- **penalty** only used in keras models\n- **mixture** only used in keras models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MODEL 2 Logistic Regression\nlr_model <- \n  \n  # specify that the model is a multinom_reg\n  multinom_reg() |>\n  \n  # mode as classification not continuous\n  set_mode(\"classification\") |>\n  \n  # select the engine/package that underlies the model (nnet is default)\n  set_engine(\"nnet\")\n  \n\n# Put everything together \nlr_wflow <- \n  workflow() |>\n  add_recipe(penguin_recipe) |>\n  add_model(lr_model)\n\n# train the model\nlr_fit <- fit(lr_wflow, data_training)\n```\n:::\n\n\n## Compare the performance of the two models\n\n::: {.cell}\n\n```{.r .cell-code}\n# predict the species of the testing data we held back for each model\nrf.predict <- predict(rf_fit, data_testing)\nlr.predict <- predict(lr_fit, data_testing)\n\n# create a table comparing the predicted species from the true species\nrf.outcome <- rf.predict %>%\n  transmute(pred = .pred_class,\n            truth = data_testing$species)\n\n# confusion matrix\nrf.outcome |> conf_mat(pred, truth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        46         0      0\n  Chinstrap      1        20      0\n  Gentoo         1         0     37\n```\n\n\n:::\n\n```{.r .cell-code}\n# accuracy\nrf.outcome |> accuracy(pred, truth) -> rf.acc\n\n# specificity\nrf.outcome |> spec(pred, truth) -> rf.spec\n\n# sensitivity\nrf.outcome |> sens(pred, truth) -> rf.sens\n\n# precision\nrf.outcome |> precision(pred, truth) -> rf.prec\n\nrf.eval <- c(rf.acc$.estimate, rf.spec$.estimate, rf.sens$.estimate, rf.prec$.estimate)\nnames(rf.eval) <- c(\"accuracy\", \"specificity\", \"sensitivity\", \"precision\")\n\n# create a table comparing the predicted species from the true species\nlr.outcome <- lr.predict %>%\n  transmute(pred = .pred_class,\n            truth = data_testing$species)\n\n# confusion matrix\nlr.outcome |> conf_mat(pred, truth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        44         1      0\n  Chinstrap      0        21      0\n  Gentoo         0         0     37\n```\n\n\n:::\n\n```{.r .cell-code}\n# accuracy\nlr.outcome |> accuracy(pred, truth) -> lr.acc\n\n# specificity\nlr.outcome |> spec(pred, truth) -> lr.spec\n\n# sensitivity\nlr.outcome |> sens(pred, truth) -> lr.sens\n\n# precision\nlr.outcome |> precision(pred, truth) -> lr.prec\n\nlr.eval = c(lr.acc$.estimate, lr.spec$.estimate, lr.sens$.estimate, lr.prec$.estimate)\nnames(lr.eval) <- c(\"accuracy\", \"specificity\", \"sensitivity\", \"precision\")\n\nrbind(rf.eval, lr.eval)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         accuracy specificity sensitivity precision\nrf.eval 0.9809524   0.9911765   0.9861111 0.9753551\nlr.eval 0.9902913   0.9943503   0.9848485 0.9925926\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ranger_0.17.0        palmerpenguins_0.1.1 yardstick_1.3.2     \n [4] workflowsets_1.1.1   workflows_1.2.0      tune_1.3.0          \n [7] rsample_1.3.0        recipes_1.3.1        parsnip_1.3.2       \n[10] modeldata_1.4.0      infer_1.0.8          dials_1.4.0         \n[13] scales_1.3.0         broom_1.0.7          tidymodels_1.3.0    \n[16] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n[19] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n[22] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n[25] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    timeDate_4041.110   fastmap_1.2.0      \n [4] digest_0.6.37       rpart_4.1.24        timechange_0.3.0   \n [7] lifecycle_1.0.4     survival_3.8-3      magrittr_2.0.3     \n[10] compiler_4.4.1      rlang_1.1.5         tools_4.4.1        \n[13] utf8_1.2.4          yaml_2.3.10         data.table_1.17.0  \n[16] knitr_1.50          htmlwidgets_1.6.4   DiceDesign_1.10    \n[19] withr_3.0.2         nnet_7.3-20         grid_4.4.1         \n[22] sparsevctrs_0.3.4   colorspace_2.1-1    future_1.49.0      \n[25] globals_0.18.0      iterators_1.0.14    MASS_7.3-64        \n[28] cli_3.6.4           rmarkdown_2.29      generics_0.1.3     \n[31] rstudioapi_0.17.1   future.apply_1.11.3 tzdb_0.4.0         \n[34] splines_4.4.1       parallel_4.4.1      vctrs_0.6.5        \n[37] hardhat_1.4.1       Matrix_1.7-2        jsonlite_1.9.1     \n[40] hms_1.1.3           listenv_0.9.1       foreach_1.5.2      \n[43] gower_1.0.2         glue_1.8.0          parallelly_1.45.0  \n[46] codetools_0.2-20    stringi_1.8.4       gtable_0.3.6       \n[49] munsell_0.5.1       GPfit_1.0-9         pillar_1.10.1      \n[52] furrr_0.3.1         htmltools_0.5.8.1   ipred_0.9-15       \n[55] lava_1.8.1          R6_2.6.1            lhs_1.2.0          \n[58] evaluate_1.0.3      lattice_0.22-6      backports_1.5.0    \n[61] class_7.3-23        Rcpp_1.0.14         prodlim_2024.06.25 \n[64] xfun_0.51           pkgconfig_2.0.3    \n```\n\n\n:::\n:::\n",
    "supporting": [
      "Pred_Penguins_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}