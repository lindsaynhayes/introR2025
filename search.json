[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Location: In person, Summer 2025\nDates: 7/1/2025 - 7/28/2025\nCourse time: 10:30am - noon CST\nCourse location: BMSB 324\nOffice hours: Tuesdays from 12:00-1:00pm CST\n\n\n\n\nProf. Lindsay Hayes (https://lindsaynhayes.github.io)\n\nOffice Location: BMSB 103\nEmail: Lindsay-Hayes@ouhsc.edu\n\n\nI am an Assistant Professor in the Department of Cell Biology at the University of Oklahoma Health Sciences Center. I am affiliated with the Graduate Programs in Cell Biology and Neuroscience as part of the Graduate Program in Biomedical Sciences.\nMy research field is in developmental neurobiology and neuroimmunology. My lab studies how developmental inflammation leads to impaired brain development and reprogramming of the brain resident immune cells called microglia. We primarily use mouse model systems, but also use human resources to study how neuroinflammation impacts brain function and contributes to risks for neurodevelopmental and psychiatric disorders. My lab uses big data science primary based on genomics data we generate in the lab, but I encourage my lab members to organize and analyze ALL data generated using the principles of reproducible and literate coding.\nIf you want, you can find me on Bluesky.\n\n\n\n\nEleana Cabello\n\nOffice Location: IRCF\nEmail: eleana-cabello@ouhsc.edu\n\n\n\n\n\n\nCourse website: https://lindsaynhayes.github.io/introR2025\nGitHub repository with all course material: https://github.com/lindsaynhayes/introR2025\n\n\n\n\nIntro to Reproducible Data Analysis Using R will prepare students to explore large datasets, generate publication-quality graphics, perform reproducible and literate data analysis pipelines, and communicate analysis results. The ultimate goal is to teach novice programmers to write modular and descriptive code using R to address relevant scientific questions. The students will use publicly available data as well as apply reproducible scripts to data generated in their laboratories.\n\n\n\nUpon successfully completing this course, students will be able to:\n\nImport and summarize data in R using reproducible and literate programming principles\nAnalyze and evaluate data manipulations and outcomes\nSeek help for roadblocks, debug problems, and communicate results to a broad audience\nTake appropriate steps to evaluate data quality and choose appropriate analyzes\n\nThe goal of this course is to teach novice programmers to write modular code and impart best practices for using R for data analysis. R is commonly used across many scientific disciplines and will be useful to a wide array of students across campus. The course will focus on teaching the fundamentals of the programming language R including data analysis and plotting but will not teach statistical analyses."
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "",
    "text": "Prof. Lindsay Hayes (https://lindsaynhayes.github.io)\n\nOffice Location: BMSB 103\nEmail: Lindsay-Hayes@ouhsc.edu\n\n\nI am an Assistant Professor in the Department of Cell Biology at the University of Oklahoma Health Sciences Center. I am affiliated with the Graduate Programs in Cell Biology and Neuroscience as part of the Graduate Program in Biomedical Sciences.\nMy research field is in developmental neurobiology and neuroimmunology. My lab studies how developmental inflammation leads to impaired brain development and reprogramming of the brain resident immune cells called microglia. We primarily use mouse model systems, but also use human resources to study how neuroinflammation impacts brain function and contributes to risks for neurodevelopmental and psychiatric disorders. My lab uses big data science primary based on genomics data we generate in the lab, but I encourage my lab members to organize and analyze ALL data generated using the principles of reproducible and literate coding.\nIf you want, you can find me on Bluesky."
  },
  {
    "objectID": "syllabus.html#teaching-assistants",
    "href": "syllabus.html#teaching-assistants",
    "title": "Syllabus",
    "section": "",
    "text": "Eleana Cabello\n\nOffice Location: IRCF\nEmail: eleana-cabello@ouhsc.edu"
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Syllabus",
    "section": "",
    "text": "Course website: https://lindsaynhayes.github.io/introR2025\nGitHub repository with all course material: https://github.com/lindsaynhayes/introR2025"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Intro to Reproducible Data Analysis Using R will prepare students to explore large datasets, generate publication-quality graphics, perform reproducible and literate data analysis pipelines, and communicate analysis results. The ultimate goal is to teach novice programmers to write modular and descriptive code using R to address relevant scientific questions. The students will use publicly available data as well as apply reproducible scripts to data generated in their laboratories."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "",
    "text": "Upon successfully completing this course, students will be able to:\n\nImport and summarize data in R using reproducible and literate programming principles\nAnalyze and evaluate data manipulations and outcomes\nSeek help for roadblocks, debug problems, and communicate results to a broad audience\nTake appropriate steps to evaluate data quality and choose appropriate analyzes\n\nThe goal of this course is to teach novice programmers to write modular code and impart best practices for using R for data analysis. R is commonly used across many scientific disciplines and will be useful to a wide array of students across campus. The course will focus on teaching the fundamentals of the programming language R including data analysis and plotting but will not teach statistical analyses."
  },
  {
    "objectID": "syllabus.html#computer-requirements",
    "href": "syllabus.html#computer-requirements",
    "title": "Syllabus",
    "section": "Computer Requirements",
    "text": "Computer Requirements\nA laptop is required for the course. The laptop requirements set forth by the College of Medicine can be found here. Specifically, laptops must be encrypted in accordance with University policy. It is the student’s responsibility to ensure that one’s laptop is connected to the OUHSC wireless network.\nIn addition, students need to have R and R Studio installed on their computers before the first class session. Detailed information on this can be found on the course Canvas site.\n\n\n\n\n\n\nBe Prepared\n\n\n\nSince this is a short course it is essential your computers are ready to start working on the first day of class."
  },
  {
    "objectID": "syllabus.html#class-preparation",
    "href": "syllabus.html#class-preparation",
    "title": "Syllabus",
    "section": "Class Preparation",
    "text": "Class Preparation\nBefore each class, there will be a set of pre-lecture activities and/or readings to complete. It is important these activities be done before class so that the students can benefit the most from the in-class activities. Students are responsible for completing all assigned readings and materials, whether discussed in class or not (including any written or verbal updates, all lecture material, case studies, independent study, other information provided, etc.). Students are responsible for processing and analyzing data outside of the instruction time, so the student and the data are ready for in-class activities.\n\n\n\n\n\n\nTip\n\n\n\nYou will get as much out of the course as you put into it!"
  },
  {
    "objectID": "syllabus.html#attendance",
    "href": "syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nThe course design is as a traditional, face-to-face contact where instructors interact with students in the same physical space.\n\n\n\n\n\n\nThis is important\n\n\n\nAttendance is required for all class sessions.\n\n\nAll anticipated absences should be requested in writing at least 24 hours in advance. A limited number of reasons will justify excused absences. These include, but are not limited to, an illness, a personal emergency, a family emergency, or presentation or representation in a leadership capacity at a professional meeting. Students may be required to provide documentation of the reason for the absence and may be required to complete supplementary assignments to make up for missed activities, but the course director and/or instructors are not required to provide make-up opportunities for missed discussions. Each unexcused absence will trigger a 10 point reduction in attendance points (see evaluation and grading below). If a session cannot be in person due to illness or weather, the session may be given by zoom. In such cases, every effort will be made to communicate any changes and to record the session for those that cannot attend synchronously.\nUnexcused lateness of more than 10 minutes will be recorded as an absence. Egregious lack of engagement (e.g., work unrelated to the course, messaging/texting, or using social media for more than 10 minutes of class) will be counted as an absence."
  },
  {
    "objectID": "syllabus.html#participation",
    "href": "syllabus.html#participation",
    "title": "Syllabus",
    "section": "Participation",
    "text": "Participation\nStudents are required to process and analyze data provided during the course. Students are required to write their own scripts and analyze data independently. Students will be required to submit a final project processing their own data or publicly available data using literate programming concepts."
  },
  {
    "objectID": "syllabus.html#canvas",
    "href": "syllabus.html#canvas",
    "title": "Syllabus",
    "section": "Canvas",
    "text": "Canvas\nThe primary communication for the class will go through Canvas. That is where we will post course announcements, grading, course discussions, and as the primary means of communication between course participants and course instructors."
  },
  {
    "objectID": "syllabus.html#getting-help",
    "href": "syllabus.html#getting-help",
    "title": "Syllabus",
    "section": "Getting Help",
    "text": "Getting Help\nIn order of preference, here is a preferred list of ways to get help:\n\nWe strongly encourage you to use Canvas to ask questions first, before joining office hours. The reason for this is so that other students in the class (who likely have similar questions) can also benefit from the questions and answers asked by your colleagues.\nYou are welcome to join office hours to get feedback.\nIf you are not able to make the office hours, appointments can be made by email with the instructor."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other online materials. All course materials will be provided. Required readings will be posted online before the relevant course sessions."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software",
    "text": "Software\nWe will make heavy use of R in this course, so you should have R installed before the first class. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.4.1. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have this version of R installed, go to CRAN and download and install the latest version. Newer versions may also work but considerably older versions should be avoided.\nWe will work primarily in RStudio which is an interactive development environment (IDE) for R. RStudio requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. In particular, we will make heavy use of it when developing R packages. It is also essential that you have the latest release of RStudio. You can determine the version of RStudio by looking at menu item Help &gt; About RStudio. You should be using RStudio version 2023.09.1+494 (2023.09.1+494) or higher, which requires R version 3.3.0 or higher."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\n\nHomework assignments and final projects will be completed independently by students.\nTurn in Homework on canvas as a quarto markdown file and the corresponding html report.\nThe Final project is to identify a data processing problem or question, develop a script to solve the problem, and present the process in a final 10-minute presentation.\nThere are no exams in the course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades will be issued via the Canvas grade book.\n\nRelative weights\n\n\n\nAssignment\n% of Final Grade\n\n\n\n\nHomework 1\n15\n\n\nHomework 2\n15\n\n\nHomework 3\n15\n\n\nFinal Project\n35\n\n\nAttendance\n20\n\n\nTotal\n100\n\n\n\n\n\nGrading Rubrics\nFor homework & final project:\n\nDid the student successfully complete the analysis and script? Was the correct output generated?\nDid the final project have a clearly stated analysis goal?\nDid the final project successfully achieve that goal? Why or why not?\n\nA == Phenomenal/Excellent\nB == Passing\nC == Needs Improvement\nD/F == Did not complete the assignment\n\n\n\n\nCourse evaluation\nWe will allocate 15 minutes in the last course session for students to perform the standard course evaluation, along with comments for the instructor."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\n\n\n\n\n\n\n\n\n\nDate\nClass\nTopic\nHomework\n\n\n\n\n7/1\n01\nIntro to R/RStudio\n\n\n\n7/3\n02\nNavigating Data Types in R\nHomework 1 (due 7/25)\n\n\n7/8\n03\nData Wrangling with tidyverse\n\n\n\n7/10\n04\nData Visualization with ggplot2\nHomework 2 (due 7/25)\n\n\n7/14\n05\nDimension Reduction with tidymodels\nHomework 3 (due 7/25)\n\n\n7/16\n06\nMachine Learning with tidymodels\n\n\n\n7/18\n07\nBuilding a Portfolio in GitHub\n\n\n\n7/22\n08\nIn-Class Work on Final\nFinal project proposal due\n\n\n7/24\n09\nFinal Project Presentations\npresentation\n\n\n7/28\n10\nFinal Project Presentations\npresentation and evaluation"
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that assignments will be implemented and written up independently. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late Work",
    "text": "Late Work\nDue to the short duration of the course and the compounding nature of the materials late work will not be accepted."
  },
  {
    "objectID": "syllabus.html#use-of-ai-tools",
    "href": "syllabus.html#use-of-ai-tools",
    "title": "Syllabus",
    "section": "Use of AI Tools",
    "text": "Use of AI Tools\nUse of AI tools (including ChatGPT, Bard, Microsoft Copilot, etc) to assist in completing this assignment/exam is permitted with your writing and/or programming. Be aware, however, that such tools often introduce errors or fabricate information; it is your responsibility to ensure the factual accuracy of whatever you claim as your writing/code. I recommend using such tools particularly for learning to code, just make sure the code does what it is supposed to, and that you understand what the code does.\nWith respect to writing, as with all sources, proper references and use of quotation marks should be used (if precise language generated by the software is used). The reference must include the website and specific prompts used to generate the referenced output.\n\n\n\n\n\n\nFor more information:\n\n\n\nRefer to the LLM section of the syllabus on canvas for more information on use of LLMs."
  },
  {
    "objectID": "syllabus.html#course-code-of-conduct",
    "href": "syllabus.html#course-code-of-conduct",
    "title": "Syllabus",
    "section": "Course Code of Conduct",
    "text": "Course Code of Conduct\nWe are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter/X and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter/X.\nCourse participants violating these rules will be referred the Title IX coordinator and may face expulsion from the class.\nAll class participants agree to:\n\nBe considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members.\nBe respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course.\nRefrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Lindsay Hayes.\nTake care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Lindsay Hayes if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential."
  },
  {
    "objectID": "syllabus.html#copyright",
    "href": "syllabus.html#copyright",
    "title": "Syllabus",
    "section": "Copyright",
    "text": "Copyright\nThis syllabus and all related course material are protected under US Copyright Law and may not be further disseminated in any form or format without the prior explicit written consent of the faculty member. Failure to comply with this provision may subject the student to disciplinary action and/or state or federal action."
  },
  {
    "objectID": "syllabus.html#student-professional-behavior-in-an-academic-program",
    "href": "syllabus.html#student-professional-behavior-in-an-academic-program",
    "title": "Syllabus",
    "section": "Student Professional Behavior in an Academic Program",
    "text": "Student Professional Behavior in an Academic Program\nEthical and professional behaviors are considered a core competency in an academic program and thus are key factors in a student’s good academic standing. Upon acceptance of an offer of admission, the student commits to comply with all professional conduct regulations established by the University, respective college, and program. The complete Student Professional Behavior in an Academic Program policy."
  },
  {
    "objectID": "syllabus.html#academic-misconduct-code",
    "href": "syllabus.html#academic-misconduct-code",
    "title": "Syllabus",
    "section": "Academic Misconduct Code",
    "text": "Academic Misconduct Code\nThe Academic Misconduct Code describes academic misconduct as any acts intended to improperly affect the evaluation of a student’s academic performance or achievement. Academic Misconduct includes but is not limited to cheating, plagiarism, fabrication, fraud, destruction, bribery or intimidation, assisting others in any act proscribed by the Code, or attempting to engage in such acts. The policy and procedures related to academic misconduct are detailed in the Academic Misconduct Code found in Appendix C of the Faculty Handbook."
  },
  {
    "objectID": "syllabus.html#academic-appeals",
    "href": "syllabus.html#academic-appeals",
    "title": "Syllabus",
    "section": "Academic Appeals",
    "text": "Academic Appeals\nThe Academic Appeals policy outlines the procedure a student must follow to request a hearing for appeals related to evaluation in a course, thesis or dissertation defense, or general or comprehensive exam. It also outlines the appeal process for a suspension or dismissal or under the Student Professional Behavior in an Academic Program Policy and for appeals of decisions resulting in dismissal, expulsion, or suspension from a program or of being required to repeat a semester of year. The sole basis for an academic appeal is an alleged prejudiced or capricious academic evaluation or decision. Policy and procedure details are in Appendix C of the Faculty Handbook."
  },
  {
    "objectID": "syllabus.html#accommodation-on-the-basis-of-disability",
    "href": "syllabus.html#accommodation-on-the-basis-of-disability",
    "title": "Syllabus",
    "section": "Accommodation on the Basis of Disability",
    "text": "Accommodation on the Basis of Disability\nThe University of Oklahoma is committed to the goal of achieving equal educational opportunity and full participation for students with disabilities. Accommodations on the basis of disability are available by contacting the Accessibility and Disability Resource Center (ADRC) by email at adrc@ou.edu or by calling (405) 325-3852 or Voice (405) 217-3494 (VP). Information on policies and registration with the Accessibility and Disability Resource Center may be found on the ADRC website. Students requesting accommodations related to work in a course must contact the ADRC as soon as possible; accommodations are not made retroactively."
  },
  {
    "objectID": "syllabus.html#sexual-misconduct",
    "href": "syllabus.html#sexual-misconduct",
    "title": "Syllabus",
    "section": "Sexual Misconduct",
    "text": "Sexual Misconduct\nFor issues regarding gender-based discrimination, sexual harassment, sexual misconduct, stalking, or intimate partner violence, the University offers a variety of resources, including Advocates-On-Call 24/7, counseling services, mutual “No Contact orders,” scheduling adjustments, and disciplinary sanctions against the perpetrator. Information is available from the Institutional Equity Office at (405) 325-2215 (8AM-5PM) or the OU Advocates at (405) 615-0013 (24/7)."
  },
  {
    "objectID": "syllabus.html#adjustment-for-pregnancychildbirth-related-issues",
    "href": "syllabus.html#adjustment-for-pregnancychildbirth-related-issues",
    "title": "Syllabus",
    "section": "Adjustment for Pregnancy/Childbirth Related Issues",
    "text": "Adjustment for Pregnancy/Childbirth Related Issues\nStudents needing modifications or adjustments to course requirements because of documented pregnancy-related or childbirth-related issues should contact the college’s Assistant/Associate Dean for Student Affairs (or academic advisor) or the Accessibility and Disability Resources Center as soon as possible to discuss. Generally, modifications will be made where medically necessary and similar in scope to accommodations based on temporary disability. See FAQs."
  },
  {
    "objectID": "syllabus.html#course-dropuniversity-withdrawal",
    "href": "syllabus.html#course-dropuniversity-withdrawal",
    "title": "Syllabus",
    "section": "Course Drop/University Withdrawal",
    "text": "Course Drop/University Withdrawal\nThe student is responsible to submit required University paperwork before the deadlines to drop or withdraw from a course, shown in the Academic Calendar. Missed homework and examination grades will be entered as a grade of zero if a student fails to formally drop the course or withdraw from the University."
  },
  {
    "objectID": "syllabus.html#laptopdevice-encryption-and-anti-virus-software",
    "href": "syllabus.html#laptopdevice-encryption-and-anti-virus-software",
    "title": "Syllabus",
    "section": "Laptop/Device Encryption and Anti-Virus Software",
    "text": "Laptop/Device Encryption and Anti-Virus Software\nIn advance of examinations, students must check that their laptop or PC includes up-to-date encryption software and the necessary programs for securing the device. Students who obtain new or replacement devices at any time can request access to the Student Virtual Desktop. OU IT will grant student access within 24 hours of request submission. Students can log in here. All students should continue to encrypt their devices with Windows 10 and MacOS encryption tools and install anti-virus software. Instructions and recommendations are linked at: Windows 10 Encryption, MacOS Encryption, and Anti-Virus Software."
  },
  {
    "objectID": "syllabus.html#absences",
    "href": "syllabus.html#absences",
    "title": "Syllabus",
    "section": "Absences",
    "text": "Absences\nIf you will be absent from a course activity for any reason, it is your responsibility to notify the instructor as specified by the course syllabus."
  },
  {
    "objectID": "syllabus.html#covid-19",
    "href": "syllabus.html#covid-19",
    "title": "Syllabus",
    "section": "COVID-19",
    "text": "COVID-19\nSee Related Academic Policy Addendum."
  },
  {
    "objectID": "syllabus.html#responsible-conduct-of-research",
    "href": "syllabus.html#responsible-conduct-of-research",
    "title": "Syllabus",
    "section": "Responsible Conduct of Research",
    "text": "Responsible Conduct of Research\nStudents, as members of the University community, have the responsibility to ensure the integrity and ethical standards of any research activity with which they are associated directly or of which they have sufficient knowledge to determine its appropriateness. Students are governed by the Policy on Ethics in Research Faculty Handbook Section 3.25.\n\nLicense and attribution\nThis Code of Conduct is distributed under a Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s ‘’how to design a code of conduct for your community’’ and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Jeff Leek group, and Stephanie C. Hicks."
  },
  {
    "objectID": "readings/index_R06.html",
    "href": "readings/index_R06.html",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "In the last class, we talked about using tidymodels to perform dimension reduction, specifically using principal component analysis (PCA) to reduce the data from multiple features into a few number of summary features (PCs) that best represented the variance in the dataset. Variance is an important aspect of statistical analysis. Most statistical tests ask whether sources of variance in your data is a result of some manipulation or group comparison you are performing.\nIn this class, we will expand on these concepts my exploring some machine learning algorithms to predict outcomes in data. PCA is actually a type of machine learning too. I snuck it up on you before you even knew you were doing machine learning!\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nUnderstand the purpose of data modeling, specifically machine learning classification\nIdentify the packages and tools used to implement machine learning algorithms for data analysis\n\n\n\n\n\nML is a disciple of artificial intelligence (AI) that develops statistical algorithms that can learn from data and past experiences to identify patterns, draw inferences, and make predictions on unseen data.\n image credit\n\n\n\nMachine learning is useful when the biologic dataset for analysis is 1) large with many individual data points, 2) complex with many distinct features or measurements, or 3) when automated and reproducible analysis needs to be performed on the data or datasets.\nBiologic data is getting more and more complex and relying only on simplistic statistical tests to evaluate the data may leave a lot of discovery behind because the investigator didn’t know what to look for. By applying machine learning algorithms to the data, the explorer can discover new sources of variance and thus biologic meaning in data.\n\n\n\nHadley Wickham highlights in his R for Data Science book that data modeling can be used in exploratory data analysis that help to generate new hypotheses about relationships in your data. However, data modeling can also be used to go beyond exploratory data analysis and into hypothesis-confirmation that seeks to test if a relationship between variables and an outcome is true or to determine the disease probability of a new patient.\nFor example, predicting the outcome or group membership of a new observation in a model classification trained algorithm is considered hypothesis confirmation. The question does this model predict diagnosis, for example, is being asked.\nAs a result, each observation (i.e. sample) can either be used for exploration or confirmation, not both. you can repeat different types of comparisons, correlations, or statistics on all the data as many times as you like in exploratory data analysis. In contrast, hypothesis confirmation can only use an observation ONCE. As soon as you use an observation twice, you’ve switched from confirmation to exploration.\nFor example, when trying to determine if the BRCA mutation was linked to breast cancer data was collected and used for hypothesis confirmation thus all the samples collected and used only to test this hypothesis.\nThe separation of hypothesis generation from hypothesis confirmation is necessary because to confirm a hypothesis you must use data independent of the data used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.”\n– Refernce: Hadley Wickham in (R for Data Science)\nTo get around this problem, datasets are often split into separate independent “pools” of data: a training set, and a testing set, and sometimes a third set called a validation set. We didn’t do that for PCA because that was exploratory data analysis not hypothesis confirmation. However, in this lecture we will predict classification which is a hypothesis confirmation analysis and thus needs independent data to generate the model and then to test (or confirm) the model.\n\n\n\n\nSupervised learning occurs when the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. Supervised learning itself is composed of classification, where the output is categorical, and regression, where the output is numerical.\nUnsupervised learning occurs when no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data such as in PCA analysis and clustering single cell RNA sequencing data.\nNote that there are also semi-supervised learning approaches that use labelled data to inform unsupervised learning on the unlabelled data to identify and annotate new classes in the dataset (also called novelty detection).\nReinforcement learning occurs when the learning algorithm performs a task using feedback from operating in a real or synthetic environment. For example, teaching an algorithm to play chess without instructions based on the results of each decision as wins and losses.\n\nWe will explore supervised learning algorithms.\n\n\n\n\nClassification: predicting categorical data. For example, using data to determine if a patient had heart disease based on a blood work panel. ML Algorithms used for Classification include:\n\nlogistic regression\ndecision tree\nrandom forest (a type of decision tree)\nk nearest neighbor\nand many more\n\n\nRegression: predicting numeric data. For example, using data to estimate an individual’s weight based on other biometic data (such as height and activity) or to predict the temperature tomorrow based on weather data. ML algorithms used for regression include:\n\nLinear Regression\nLasso Regression\nPolynomial Regression\nand many more\n\n\n\nFor simplicity, we will only be looking at classification predictions.\n\n\n\nClassification Prediction is one of the more simple ML pipelines and it can be further broken down into binary or multi-class depending on if it is a two group classification or if there are more than 2 groups. With more than two groups, there are a few alternative approaches that are used even for a binary model.\nIn simple terms, for example, in a three group classification\n\nGroup 1 vs all other groups –&gt; yes or no\n\nYes –&gt; assign Group 1\nNo –&gt; Group 2 vs Group 3 –&gt; yes or no\n\nYes -&gt; assign Group 2\nNo –&gt; assign Group 3\n\n\nAlternatively,\n\nGroup 1 vs all other groups –&gt; yes or no\nGroup 2 vs all other groups –&gt; yes or no\nGroup 3 vs all other groups –&gt; yes or no\n\n\nLets look at a little more detail into a few different classification algorithms.\n\n\n\nLogistics regression is a classifier that uses the sigmoid function (∫) above to return the probability of a label as segregated based on some threshold. It calculates a probability that a observation falls into a specific category based on the pre-determined threshold. It is widely used when the classification problem is binary.\n\n\n\n\nRandom forest is a collection of decision trees. The steps of random forest prediction are:\n\nSubsample the data to make multiple training data sets so that the data is randomly sampled making it more robust and less dependent on the training dataset.\nBuild multiple decision trees based on a subset of the data features. This way good predictors and bad predictors will be spread out and lead to more heavy weighting of the good predictors.\nAverage the outcomes of each decision tree in a winner takes all approach to determine the classification (i.e. from 10 trees, 8 classifeid as group A and 2 classified as group B, thus group A “wins” as the classification). Some trees will be based on good predictors and some trees on bad predictors, but on average the correct classification wins out.\n\nRandom Forest has better generalization but can be less interpretable because of more layers that are added to each decision tree.\n\n\n\n\n\n\nTip\n\n\n\nThere are LOTS of different models and often data scientist test many models then compare them to one another to build a machine learning algorithm that performs well for a specific outcome.\nWe are just scratching the surface of what is out there. Check out the references below if you want to explore the details of more ML algorithms.\n\n\n\n\n\n\nLastly is model validation to determine how well the ML algorithm did at predicting the ground truth. As I mentioned at the beginning you have one set of data to train a ML model, and then you can test the reliability of the model based on data the model has never seen, thus the testing or validation datasets.\nThere are several metrics used to test the validity of a predictive model. Depending on what is being tested each of the validation metrics are weighted differently. For example, if you are developing an HIV test, you want to make sure false positives are extremely low because you don’t want people to think they have HIV when they indeed do not (called specificity). In other cases, you may be ok with some degree of false positives because you do not want to miss any true positives (i.e. you want to minimize false negatives, called precision). An example of this could be prophylactic treatment or preventative measures. It won’t hurt someone to get the intervention that is not actually positive, but would have large benefits to true positives and you don’t want someone to miss the intervention that it could really help.\nFirst you start with a confusion matrix which is a matrix that counts the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\nConfusion Matrix\n\n\n\nconfusion matrix\nTRUTH: Positive\nTRUTH: Negative\n\n\n\n\nPredicted Positive\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nPredicted Negative\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\nFrom the confusion matrix you can calculate these performance metrics\n\nAccuracy: What percentage of all predictions are correct?\n\n(TP + TN) / total predictions\ncorrect predictions / total predictions\n\nPrecision: What percentage of predicted positives were correct?\n\nTP / (TP + FP)\ncorrectly predicted true positives / all predicted positives\n\nSensitivity: What percentage of TRUE positives were correct?\n\nTP / (TP + FN)\ncorrectly predicted true positives / actual true positives\n\nSpecificity: What percentage of TRUE negatives were correct?\n\nTN / (TN + FP)\ncorrectly predicted true negatives / actual true negatives\n\n\nFinally, another common tool to evaluate a model’s performance is the Receiver Operating Characteristic (ROC) curve and a calculation of the area under that curve (AUC). This plots the FP ratet on the x-axis and the TP rate on the y-axis and then plots that curve for multiple models to compare them to one another. The model with the largest AUC is the best predictor. Meaning that the most TPs are captured with the fewest FPs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to tidyverse, tidymodels is a suite of tools used to easily apply machine learning algorithms to your data applying similar tidy principles that we’ve covered in class. We won’t go into as much detail as with tidyverse, but below is an overview of some of the included packages.\n\nrsample: split data into training, testing, validation, and more\nrecipes: pre-process data for feeding into ML algorithms\nparsnip: build the machine learning algorithms\nworkflows: assemble workflows that integrates the recipes and algorithms\ntune & dials: refine the parameters of machine learning algorithms to help with optimization\nyardstick: calculate performance metrics of machine learning algorithms\nbroom: tidy the outputs machine learning algorithms and statistical measures into tidy-friendly syntax\n\nIn-class we will go through an example of these workflows. We will use the penguins dataset to predict the species based on the penguin biometric data.\n\n\n\nhttps://lgatto.github.io/IntroMachineLearningWithR/index.html\nhttps://www.datacamp.com/blog/classification-machine-learning\nhttps://www.stepbystepdatascience.com/ml-with-tidymodels\nhttps://towardsdatascience.com/top-machine-learning-algorithms-for-classification-2197870ff501/\nhttps://towardsdatascience.com/top-machine-learning-algorithms-for-regression-c67258a2c0ac/\nGreener, J.G., Kandathil, S.M., Moffat, L. et al. A guide to machine learning for biologists. Nat Rev Mol Cell Biol 23, 40–55 (2022). link"
  },
  {
    "objectID": "readings/index_R06.html#what-is-machine-learning-ml",
    "href": "readings/index_R06.html#what-is-machine-learning-ml",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "ML is a disciple of artificial intelligence (AI) that develops statistical algorithms that can learn from data and past experiences to identify patterns, draw inferences, and make predictions on unseen data.\n image credit"
  },
  {
    "objectID": "readings/index_R06.html#why-would-you-need-machine-learning",
    "href": "readings/index_R06.html#why-would-you-need-machine-learning",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Machine learning is useful when the biologic dataset for analysis is 1) large with many individual data points, 2) complex with many distinct features or measurements, or 3) when automated and reproducible analysis needs to be performed on the data or datasets.\nBiologic data is getting more and more complex and relying only on simplistic statistical tests to evaluate the data may leave a lot of discovery behind because the investigator didn’t know what to look for. By applying machine learning algorithms to the data, the explorer can discover new sources of variance and thus biologic meaning in data."
  },
  {
    "objectID": "readings/index_R06.html#hypothesis-testing-in-ml-and-data-modeling",
    "href": "readings/index_R06.html#hypothesis-testing-in-ml-and-data-modeling",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Hadley Wickham highlights in his R for Data Science book that data modeling can be used in exploratory data analysis that help to generate new hypotheses about relationships in your data. However, data modeling can also be used to go beyond exploratory data analysis and into hypothesis-confirmation that seeks to test if a relationship between variables and an outcome is true or to determine the disease probability of a new patient.\nFor example, predicting the outcome or group membership of a new observation in a model classification trained algorithm is considered hypothesis confirmation. The question does this model predict diagnosis, for example, is being asked.\nAs a result, each observation (i.e. sample) can either be used for exploration or confirmation, not both. you can repeat different types of comparisons, correlations, or statistics on all the data as many times as you like in exploratory data analysis. In contrast, hypothesis confirmation can only use an observation ONCE. As soon as you use an observation twice, you’ve switched from confirmation to exploration.\nFor example, when trying to determine if the BRCA mutation was linked to breast cancer data was collected and used for hypothesis confirmation thus all the samples collected and used only to test this hypothesis.\nThe separation of hypothesis generation from hypothesis confirmation is necessary because to confirm a hypothesis you must use data independent of the data used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.”\n– Refernce: Hadley Wickham in (R for Data Science)\nTo get around this problem, datasets are often split into separate independent “pools” of data: a training set, and a testing set, and sometimes a third set called a validation set. We didn’t do that for PCA because that was exploratory data analysis not hypothesis confirmation. However, in this lecture we will predict classification which is a hypothesis confirmation analysis and thus needs independent data to generate the model and then to test (or confirm) the model."
  },
  {
    "objectID": "readings/index_R06.html#categories-of-machine-learning-algorithms",
    "href": "readings/index_R06.html#categories-of-machine-learning-algorithms",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Supervised learning occurs when the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. Supervised learning itself is composed of classification, where the output is categorical, and regression, where the output is numerical.\nUnsupervised learning occurs when no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data such as in PCA analysis and clustering single cell RNA sequencing data.\nNote that there are also semi-supervised learning approaches that use labelled data to inform unsupervised learning on the unlabelled data to identify and annotate new classes in the dataset (also called novelty detection).\nReinforcement learning occurs when the learning algorithm performs a task using feedback from operating in a real or synthetic environment. For example, teaching an algorithm to play chess without instructions based on the results of each decision as wins and losses.\n\nWe will explore supervised learning algorithms."
  },
  {
    "objectID": "readings/index_R06.html#supervised-machine-learning-types",
    "href": "readings/index_R06.html#supervised-machine-learning-types",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Classification: predicting categorical data. For example, using data to determine if a patient had heart disease based on a blood work panel. ML Algorithms used for Classification include:\n\nlogistic regression\ndecision tree\nrandom forest (a type of decision tree)\nk nearest neighbor\nand many more\n\n\nRegression: predicting numeric data. For example, using data to estimate an individual’s weight based on other biometic data (such as height and activity) or to predict the temperature tomorrow based on weather data. ML algorithms used for regression include:\n\nLinear Regression\nLasso Regression\nPolynomial Regression\nand many more\n\n\n\nFor simplicity, we will only be looking at classification predictions."
  },
  {
    "objectID": "readings/index_R06.html#ml-algorithms-for-classification-prediction",
    "href": "readings/index_R06.html#ml-algorithms-for-classification-prediction",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Classification Prediction is one of the more simple ML pipelines and it can be further broken down into binary or multi-class depending on if it is a two group classification or if there are more than 2 groups. With more than two groups, there are a few alternative approaches that are used even for a binary model.\nIn simple terms, for example, in a three group classification\n\nGroup 1 vs all other groups –&gt; yes or no\n\nYes –&gt; assign Group 1\nNo –&gt; Group 2 vs Group 3 –&gt; yes or no\n\nYes -&gt; assign Group 2\nNo –&gt; assign Group 3\n\n\nAlternatively,\n\nGroup 1 vs all other groups –&gt; yes or no\nGroup 2 vs all other groups –&gt; yes or no\nGroup 3 vs all other groups –&gt; yes or no\n\n\nLets look at a little more detail into a few different classification algorithms.\n\n\n\nLogistics regression is a classifier that uses the sigmoid function (∫) above to return the probability of a label as segregated based on some threshold. It calculates a probability that a observation falls into a specific category based on the pre-determined threshold. It is widely used when the classification problem is binary.\n\n\n\n\nRandom forest is a collection of decision trees. The steps of random forest prediction are:\n\nSubsample the data to make multiple training data sets so that the data is randomly sampled making it more robust and less dependent on the training dataset.\nBuild multiple decision trees based on a subset of the data features. This way good predictors and bad predictors will be spread out and lead to more heavy weighting of the good predictors.\nAverage the outcomes of each decision tree in a winner takes all approach to determine the classification (i.e. from 10 trees, 8 classifeid as group A and 2 classified as group B, thus group A “wins” as the classification). Some trees will be based on good predictors and some trees on bad predictors, but on average the correct classification wins out.\n\nRandom Forest has better generalization but can be less interpretable because of more layers that are added to each decision tree.\n\n\n\n\n\n\nTip\n\n\n\nThere are LOTS of different models and often data scientist test many models then compare them to one another to build a machine learning algorithm that performs well for a specific outcome.\nWe are just scratching the surface of what is out there. Check out the references below if you want to explore the details of more ML algorithms."
  },
  {
    "objectID": "readings/index_R06.html#model-validation",
    "href": "readings/index_R06.html#model-validation",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Lastly is model validation to determine how well the ML algorithm did at predicting the ground truth. As I mentioned at the beginning you have one set of data to train a ML model, and then you can test the reliability of the model based on data the model has never seen, thus the testing or validation datasets.\nThere are several metrics used to test the validity of a predictive model. Depending on what is being tested each of the validation metrics are weighted differently. For example, if you are developing an HIV test, you want to make sure false positives are extremely low because you don’t want people to think they have HIV when they indeed do not (called specificity). In other cases, you may be ok with some degree of false positives because you do not want to miss any true positives (i.e. you want to minimize false negatives, called precision). An example of this could be prophylactic treatment or preventative measures. It won’t hurt someone to get the intervention that is not actually positive, but would have large benefits to true positives and you don’t want someone to miss the intervention that it could really help.\nFirst you start with a confusion matrix which is a matrix that counts the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\nConfusion Matrix\n\n\n\nconfusion matrix\nTRUTH: Positive\nTRUTH: Negative\n\n\n\n\nPredicted Positive\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nPredicted Negative\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\nFrom the confusion matrix you can calculate these performance metrics\n\nAccuracy: What percentage of all predictions are correct?\n\n(TP + TN) / total predictions\ncorrect predictions / total predictions\n\nPrecision: What percentage of predicted positives were correct?\n\nTP / (TP + FP)\ncorrectly predicted true positives / all predicted positives\n\nSensitivity: What percentage of TRUE positives were correct?\n\nTP / (TP + FN)\ncorrectly predicted true positives / actual true positives\n\nSpecificity: What percentage of TRUE negatives were correct?\n\nTN / (TN + FP)\ncorrectly predicted true negatives / actual true negatives\n\n\nFinally, another common tool to evaluate a model’s performance is the Receiver Operating Characteristic (ROC) curve and a calculation of the area under that curve (AUC). This plots the FP ratet on the x-axis and the TP rate on the y-axis and then plots that curve for multiple models to compare them to one another. The model with the largest AUC is the best predictor. Meaning that the most TPs are captured with the fewest FPs."
  },
  {
    "objectID": "readings/index_R06.html#performing-machine-learning-in-r",
    "href": "readings/index_R06.html#performing-machine-learning-in-r",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Similar to tidyverse, tidymodels is a suite of tools used to easily apply machine learning algorithms to your data applying similar tidy principles that we’ve covered in class. We won’t go into as much detail as with tidyverse, but below is an overview of some of the included packages.\n\nrsample: split data into training, testing, validation, and more\nrecipes: pre-process data for feeding into ML algorithms\nparsnip: build the machine learning algorithms\nworkflows: assemble workflows that integrates the recipes and algorithms\ntune & dials: refine the parameters of machine learning algorithms to help with optimization\nyardstick: calculate performance metrics of machine learning algorithms\nbroom: tidy the outputs machine learning algorithms and statistical measures into tidy-friendly syntax\n\nIn-class we will go through an example of these workflows. We will use the penguins dataset to predict the species based on the penguin biometric data."
  },
  {
    "objectID": "readings/index_R06.html#resources",
    "href": "readings/index_R06.html#resources",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "https://lgatto.github.io/IntroMachineLearningWithR/index.html\nhttps://www.datacamp.com/blog/classification-machine-learning\nhttps://www.stepbystepdatascience.com/ml-with-tidymodels\nhttps://towardsdatascience.com/top-machine-learning-algorithms-for-classification-2197870ff501/\nhttps://towardsdatascience.com/top-machine-learning-algorithms-for-regression-c67258a2c0ac/\nGreener, J.G., Kandathil, S.M., Moffat, L. et al. A guide to machine learning for biologists. Nat Rev Mol Cell Biol 23, 40–55 (2022). link"
  },
  {
    "objectID": "readings/index_R04.html",
    "href": "readings/index_R04.html",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will be able to:\n\nDistinguish between good and bad plots\nMatch the type of plots with the type of data input\nList the features of the grammer of graphics\nBuild complex graphics in ggplot2\n\n\n\nBefore class, read chapter 4 titled “Visualizing Data in the Tidyverse” from the Tidyverse Skills for Data Science Book by Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks, and Roger D. Peng.\nReading: Visualizing Data in the Tidyverse\nFor more examples, you can checkout a YouTube video series on ggplot from Dr. Lace Padilla.\n\nMastering ggplot2 in R [17 min]\nMastering Axes in ggplot2 [16 min]\nColor Magic in ggplot2 [12.5 min]\nMastering Facet_Wrap in ggplot2 [15 min]"
  },
  {
    "objectID": "readings/index_R02.html",
    "href": "readings/index_R02.html",
    "title": "Navigating Data Types in R",
    "section": "",
    "text": "The last class introduced important concepts in data science such as directory structure, working directories, projects, and quarto documents. This lesson will dig deeper into working with data in R. We will cover importing and exporting, data types, and data objects\nSpreadsheets are good for data entry. As a scientists, we have a lot of data in spreadsheets. However, data exploration, manipulation, and visualization tools in spreadsheets are extremely limited and cumbersome when performed manually. Much of your time as a researcher will be spent data wrangling to generate the analyses you desire. R makes this process easy and most importantly reproducible."
  },
  {
    "objectID": "readings/index_R02.html#what-is-a-package",
    "href": "readings/index_R02.html#what-is-a-package",
    "title": "Navigating Data Types in R",
    "section": "What is a package?",
    "text": "What is a package?\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. There are tens of thousands of R packages that exist. Two main repositories of R packages exist. CRAN or the Comprehensive R Archive Network is the public clearing house for R packages and where you went to install R. Bioconductor is an open source project that develops and shares open source software for precise and repeatable analysis of biological data. It is this huge variety of packages that R is so successful because the chances are high that someone has already solved a problem that you’re working on."
  },
  {
    "objectID": "readings/index_R02.html#how-to-get-a-package",
    "href": "readings/index_R02.html#how-to-get-a-package",
    "title": "Navigating Data Types in R",
    "section": "How to get a package?",
    "text": "How to get a package?\nTo install packages from CRAN use the install.packages() function. Once the package is installed you can load the package using the library() function. Installing packages from Bioconductor take a few more steps which we won’t cover just yet.\nIn Lecture 2, we will use data from the tidyverse and taylor packages."
  },
  {
    "objectID": "readings/index_R02.html#tidyverse-package",
    "href": "readings/index_R02.html#tidyverse-package",
    "title": "Navigating Data Types in R",
    "section": "Tidyverse package",
    "text": "Tidyverse package\nThe Tidyverse which is a collection of R packages that share an underlying design, syntax, and grammer to streamline many main functions used in data science. You can install the complete tidyverse with install.packages(\"tidyverse\"), once the package is installed you can load it using library(tidyverse).\nWe will go over more details about all the functions of the tidyverse package next class!"
  },
  {
    "objectID": "readings/index_R02.html#taylor-package",
    "href": "readings/index_R02.html#taylor-package",
    "title": "Navigating Data Types in R",
    "section": "Taylor package",
    "text": "Taylor package\nThe taylor package is for accessing and exploring data related to Taylor Swift’s discography, including lyrics and audio characteristics. The data comes from Genius and the Spotify."
  },
  {
    "objectID": "readings/index_R02.html#import",
    "href": "readings/index_R02.html#import",
    "title": "Navigating Data Types in R",
    "section": "Import",
    "text": "Import\nFirst we need to import data into R. I prefer to import .csv files because it is a good way to indicate a data file and it eliminates any weird formatting that may exist in excel. Think of your .csv files as immutable, the data stays the same and the code is the reproducible record of data manipulation. If you go and change the raw .csv there is no record of that modification, with code you can always know exactly how the data was treated.\nI am showing 2 ways to import data one using the base R function read.csv() and the other using the readr function read_csv().\nIn base R, the header field determines if the first row of the data had column header titles or not. The stringsAsFactors determines if the character variables in your data set should be set as a factor variable (set as TRUE) or as character (set to FALSE). The row.names field specifies if the first column is data or the names of the rows.\nIn tidyverse, the col_names field specifies if the first row is a header. If you want to specify the type of data for each field you have to specify it using the col_types field. I’ll show you a cool trick here in class. There are a lot more options you can look at in the Help section. Try typing read_csv in the Help query box.\n\nlibrary(readr)\n\n# reading in csv is for importing comma-separated files\n\n# base R\ndata_base &lt;- read.csv(file = \"data/taylorswift.csv\", \n                 header = TRUE, \n                 sep = \",\", \n                 stringsAsFactors = TRUE)\n\n\n# Tidyverse\ndata_tidy &lt;- read_csv(file = \"data/taylorswift.csv\",\n                 col_names = TRUE)\n\nUsing readr you can also import data directly from excel spreadsheets using read_excel or read_xls and even google sheets using read_sheet. However, we won’t use these function in the course. But you are free to explore them on your own from the readr package."
  },
  {
    "objectID": "readings/index_R02.html#export",
    "href": "readings/index_R02.html#export",
    "title": "Navigating Data Types in R",
    "section": "Export",
    "text": "Export\nYou can also save files using the write functions for both csv or tsv files.\n\n# base R\nwrite.csv(data, file = \"data/taylorswift.csv\")\n\n# Tidyverse\nwrite_csv(data, file = \"data/taylorswift.csv\")"
  },
  {
    "objectID": "readings/index_R02.html#explore",
    "href": "readings/index_R02.html#explore",
    "title": "Navigating Data Types in R",
    "section": "Explore",
    "text": "Explore\nAnytime you get a data set the first thing you have to do is look at it! Some basic things to look at are:\n\nUse the dim() function to check the size of the data set or nrow to count the rows and ncol to count the columns.\nUse the class() function to determine if an R object is a vector, matrix, or dataframe. And if the data type is in character, numeric, factor, or logic, for example.\nUse thetable() or count() functions to tally the data to see how many types of any variable\nglimpse() is a quick way to view a snapshot of top of the data.\n\n\n\n\n\n\n\nDATA TYPES\n\n\n\nRemember these are the main types of data in R:\n\ncharacter\nnumeric\ninteger\nfactor\nlogical\n\n\n\n\ndata size and types\nLets look at data types in the taylorswift data in R\n\n# first load the packages we are going to need\nlibrary(tidyverse)\n\n# next import some data, as an example we are using the taylorswift data\ndata &lt;- read_csv(file = \"data/taylorswift.csv\",\n                 col_names = TRUE)\n\n# What kind of data is it? Its a dataframe, in tidyverse a tidy table is called a tibble.\nclass(data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n# Its good to start any data analysis with basic data exploration.\n# how many columns or measurements are there in the data frame?\nncol(data)\n\n[1] 21\n\n# how many rows or observations are there in the data frame?\nnrow(data)\n\n[1] 314\n\n# when we imported the data, what is the type of each measurement in the data columns?\n# one way to look at this is to use the map function which is actually a loop that runs the function class on each column of data.\nas.data.frame(map_chr(data, class))\n\n                 map_chr(data, class)\nalbum_name                  character\nalbum_release                    Date\ntrack_number                  numeric\ntrack_name                  character\ndanceability                  numeric\nenergy                        numeric\nkey                           numeric\nloudness                      numeric\nmode                          numeric\nspeechiness                   numeric\nacousticness                  numeric\ninstrumentalness              numeric\nliveness                      numeric\nvalence                       numeric\ntempo                         numeric\ntime_signature                numeric\nduration_ms                   numeric\nexplicit                      logical\nkey_name                    character\nmode_name                   character\nkey_mode                    character\n\n# another way to to use glimpse to preview each column of data in a tidy way and it also gives an abbreviation of the data class. \nglimpse(data)\n\nRows: 314\nColumns: 21\n$ album_name       &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"Tayl…\n$ album_release    &lt;date&gt; 2006-10-24, 2006-10-24, 2006-10-24, 2006-10-24, 2006…\n$ track_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1,…\n$ track_name       &lt;chr&gt; \"Tim McGraw\", \"Picture To Burn\", \"Teardrops On My Gui…\n$ danceability     &lt;dbl&gt; 0.580, 0.658, 0.621, 0.576, 0.418, 0.589, 0.479, 0.59…\n$ energy           &lt;dbl&gt; 0.491, 0.877, 0.417, 0.777, 0.482, 0.805, 0.578, 0.62…\n$ key              &lt;dbl&gt; 0, 7, 10, 9, 5, 5, 2, 8, 4, 2, 2, 8, 7, 4, 10, 5, 7, …\n$ loudness         &lt;dbl&gt; -6.462, -2.098, -6.941, -2.881, -5.769, -4.055, -4.96…\n$ mode             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ speechiness      &lt;dbl&gt; 0.0251, 0.0323, 0.0231, 0.0324, 0.0266, 0.0293, 0.029…\n$ acousticness     &lt;dbl&gt; 0.57500, 0.17300, 0.28800, 0.05100, 0.21700, 0.00491,…\n$ instrumentalness &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.0…\n$ liveness         &lt;dbl&gt; 0.1210, 0.0962, 0.1190, 0.3200, 0.1230, 0.2400, 0.084…\n$ valence          &lt;dbl&gt; 0.425, 0.821, 0.289, 0.428, 0.261, 0.591, 0.192, 0.50…\n$ tempo            &lt;dbl&gt; 76.009, 105.586, 99.953, 115.028, 175.558, 112.982, 1…\n$ time_signature   &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ duration_ms      &lt;dbl&gt; 232107, 173067, 203040, 199200, 239013, 207107, 24810…\n$ explicit         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ key_name         &lt;chr&gt; \"C\", \"G\", \"A#\", \"A\", \"F\", \"F\", \"D\", \"G#\", \"E\", \"D\", \"…\n$ mode_name        &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"major\", \"major\",…\n$ key_mode         &lt;chr&gt; \"C major\", \"G major\", \"A# major\", \"A major\", \"F major…\n\n\n\n\ntable() & count()\nLets look at the distribution of some of the data using table() and count() functions. These functions counts the number of times each variable appears in the data. table() and count() work great for factor and character variables, but poorly for continuous and numeric variables unless there are only a few options.\nThe differences between table() and count() is the package they are part of and the format of the output. count() is part of the tidyvse and uses tidyverse syntax and outputs the results as a tibble. table() is part of base R and the output is a table. The count() function uses the tidyverse pipe syntax, “|&gt;”. The table() function uses the data subsetting synatx “$”. Both of these syntaxes are very common.\n\n\n\n\n\n\nNote\n\n\n\nRemember: Piping syntax\n|&gt; is the same as %&gt;%\n\n\nLets look at some examples.\n\n# Here is an example using a character vector.\ntable(data$key_name)\n\n\n A A#  B  C C#  D D#  E  F F#  G G# \n21 13  9 64 13 37  7 35 32 14 56 10 \n\ndata |&gt; count(key_name)\n\n# A tibble: 13 × 2\n   key_name     n\n   &lt;chr&gt;    &lt;int&gt;\n 1 A           21\n 2 A#          13\n 3 B            9\n 4 C           64\n 5 C#          13\n 6 D           37\n 7 D#           7\n 8 E           35\n 9 F           32\n10 F#          14\n11 G           56\n12 G#          10\n13 &lt;NA&gt;         3\n\n# Here is an example using a numeric vector. \ntable(data$time_signature)\n\n\n  1   3   4   5 \n  2  11 294   4 \n\ndata |&gt; count(time_signature)\n\n# A tibble: 5 × 2\n  time_signature     n\n           &lt;dbl&gt; &lt;int&gt;\n1              1     2\n2              3    11\n3              4   294\n4              5     4\n5             NA     3\n\n# Here is an example using a logical vector.\ntable(data$explicit)\n\n\nFALSE  TRUE \n  278    33 \n\ndata |&gt; count(explicit)\n\n# A tibble: 3 × 2\n  explicit     n\n  &lt;lgl&gt;    &lt;int&gt;\n1 FALSE      278\n2 TRUE        33\n3 NA           3\n\n\nAs you can see the NA values are excluded in the table() function and the output is shaped differently. The output form changes how you use it for downstream analyses.\n\n# For example, we can the base R plot() or barplot() functions to look at the table output, but not the count output. \nbarplot(table(data$key_name))\n\n\n\n\n\n\n\ndata |&gt; count(key_name) |&gt; plot()\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\n\n\n\n\nError in plot.window(...): need finite 'xlim' values\n\n# For the count() output we use the tidyverse plotting package called ggplot2(). This gives tons more flexibility in how the plot is rendered than the plot() function. \n\ndata |&gt; count(key_name) |&gt; ggplot(aes(x = key_name, y = n)) + geom_col() + theme_cowplot() + ylab(\"Number of songs\") + xlab(\"key\")\n\n\n\n\n\n\n\n\nIf we want to look at the distribution numeric vectors table() and count() won’t work that well instead we can use the summary() function.\n\n\nsummarise and distributions\n\n# Base R\nsummary(data$tempo)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  68.53   97.07  120.03  124.40  146.97  208.92       3 \n\nboxplot(data$tempo, horizontal=TRUE)\n\n\n\n\n\n\n\nhist(data$tempo, breaks = 20)\n\n\n\n\n\n\n\n# ggplot\na &lt;- ggplot(data, aes(y = tempo)) + geom_boxplot(fill = \"deepskyblue\") + theme_cowplot() + scale_x_discrete( )\nb &lt;- ggplot(data, aes(x = tempo)) + geom_histogram(fill = \"deepskyblue\") + theme_cowplot()\nplot_grid(a,b, nrow = 1, labels = c(\"A\", \"B\"))\n\n\n\n\n\n\n\n\nNote: We’ll go over the details of ggplot in a future lecture. For not just go along with the magic.\n\n\nconvert\nOftentimes text is imported as a character variable. However, you may want that to be a factor to indicate there are 4 groups, or a numeric variable should actually be a factor. The most common data type conversions are between:\n\ncharacters –&gt; factors\nnumeric –&gt; factors\n\nFor example, if you measured something across time: 1, 5, 15, 30, 60, 120, then plotting the data across the expansive axis will make each groups not evenly divided so it can be useful to make the numeric variable a factor so all the data groups plot next to each other.\nWe can look at this in the taylorswift data. If we plot the album release date as a date we can see the time represented in the distance and sometimes we may want that but other times we may not.\n\n# sometimes we want numeric variables as numbers and sometimes we want them to be factors. In this case the x axis is spaced based on the time between album releases.\n\nggplot(data, aes(x=album_release, y = track_number, color = album_name)) + geom_point()\n\n\n\n\n\n\n\n# However, if we look at the data in a different way having all the data scrunched at one end is hard to visualize when the time variable isn't needed to be viewed over time.\n\nggplot(data, aes(x=album_release, y = loudness, fill = album_name)) + geom_boxplot()\n\n\n\n\n\n\n\n# It would look better to see the data more evenly distrubuted. To do that we can convert the date variable to a factor variable\n\ndata$album_release &lt;- as.factor(data$album_release)\n\nggplot(data, aes(x=album_release, y = loudness, fill = album_name)) + \n  geom_boxplot() + \n  scale_x_discrete(guide = guide_axis(angle = 90))\n\n\n\n\n\n\n\n\nIts now easier to see the relationship of loudness over time and the variance in each album.\n\nAnother example\nIn another example, can see in the plots above the “album_name” is colored alphabetically, and with so many groups its hard to tell which bar is which album.\nWe can instead plot the album_name on the x-axis, but the album_names are plotted alphabetically so you loose the sense of the loudness change over time.\n\nggplot(data, aes(x=album_name, y = loudness)) +\n  geom_boxplot() + \n  scale_x_discrete(guide = guide_axis(angle = 45)) + theme_cowplot() \n\n\n\n\n\n\n\n\nSolution: Convert the album name to a factor with a specified order of release date!\n\n# Convert the character vector of \"album_name\" into a factor variable and specify the levels by the album release date.  \n\ndata$album_name &lt;- factor(data$album_name, levels = unique(data$album_name[order(data$album_release)]))\n\nggplot(data, aes(x=album_name, y = loudness, fill=album_name)) +\n  geom_boxplot() + scale_fill_manual(values=album_compare) +\n  scale_x_discrete(guide = guide_axis(angle = 45)) + theme_cowplot() + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nIn sum, you’ve learned about importing and exporting data into/out of R. About how to look at the distribution of different types of data such as categorical and numerical. We looked at a few different plotting strategies to visualize the data distrubution. And finally how to convert to a factor variable to better handle the data. Having numeric variables formated at numeric or factor can also make a huge difference in how they are treated in certain statistical tests. Keep this in mind during data analysis.\n\n\n\n\n\n\nWant More?\n\n\n\nThere are so many great tools to learn R and data science!\n\nTo explore the tidyverse and data science in more detail check out the R for Data Science textbook written by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund.\nWant more stats and data science conversations?! Check out the not so standard deviations podcast by Roger Peng and Hilary Parker."
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Important reading materials",
    "section": "",
    "text": "The following contains relevant reading materials you should read prior to each class:\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-07-01\n\n\nR & RStudio\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-03\n\n\nNavigating Data Types in R\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-08\n\n\nData Wrangling with tidyverse\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-10\n\n\nData Visualization with ggplot2\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-14\n\n\nPCA with tidymodels\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-16\n\n\nMachine Learning with tidymodels\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-18\n\n\nBuilding a Quarto Website and Using GitHub\n\n\nLindsay Hayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index_A03.html",
    "href": "projects/index_A03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Overview\nDue date: 07/25/25 at 11:59pm\nThis week we will use a different package called LearnR to gain practice in dimension reduction using tidymodels.\nThe lesson will cover using tidymodels for:\n\nPCA Dimension reduction\nUMAP Dimension reduction\n\n\n\nInstall the Tutorial Course Setup\n\nThe course is available on GitHub.\nOpen RStudio.\nInstall devtools using the command below on the R Console\ninstall.packages(\"devtools\")\nInstall the github lesson using the command below on the R Console\ndevtools::install_github(\"tidymodels/learntidymodels\")\n\n\n\n\n\n\n\nTip\n\n\n\n\nYou will be asked to do a lot of updates. Not all of these are necessary. Try selecting no to the first one (option 3). Then it will ask you again with a shorter list then you can say All (option 1).\nA lot of text with fly by during the installation of the packages, that is ok and expected.\n\n\n\n\n\nRun the Tutorial Course Setup\n\nRun the tutorial by running the command below on the R Console. You may be asked to install an additional package say “yes”.\nlearnr::run_tutorial(\"pca_recipes\", package = \"learntidymodels\")\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIt is best to complete the tutorial in one setting because each step builds on the other.\nSince you submitting the final plot and not through google forms, you can comeback and do the tutorial across multiple settings.\n\n\n\n\n\nSubmission\n\nSubmit a screenshot of the final UMAP plot to canvas for homework #3.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have problems installing or deploying the tutorial please contact your instructors as soon as possible."
  },
  {
    "objectID": "projects/index_A01.html",
    "href": "projects/index_A01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Overview\nDue date: 07/08/25 at 11:59pm\nMost programming languages are similar in their underlying logic, but differ in execution, efficiency, and, more importantly, their syntax. R is one of the most popular languages used for data analysis in biomedical research. Its programming syntax is unique and differs from other popular languages in the same category as it like Python. Gaining a better understanding of R’s core features and becoming familiar with its syntax will help you perform analyses more effectively and efficiently.\nIn the Swirl lessons for this week, you will learn the following:\n\nNavigating files and directories\nAssigning and using variables\nCreating, modifying, and subsetting vectors\nCreating and modifying sequences\nHandling missing values\nSimple matrices and dataframes\nLogics (TRUE, FALSE, AND, OR, etc.) in R\nGain experience with base features in R\n\n\n\nSwirl Course Setup\n\nIn RStudio, install and load the swirl package\ninstall.packages(\"swirl\")\nlibrary(\"swirl\")\nNavigate to this link. Download the Swirl lesson file 1_R_Programming.swc. Move it into your working directory for the class.\nOnce you have the file in your working directory, in the console of RStudio type the command below and after the swc_path = type \" and then hit tab to interactively choose the 1_R_Programming.swc file you just downloaded from your working directory. The command will look something like install_course(swc_path = \"1_R_Programming.swc\"). Run the command to install the course.\ninstall_course(swc_path = )\nStart Swirl by typing the following command\nswirl()\n\n\n\nWorking through the Lessons\n\nPlease enter your full name together when asked what you want to be called by e.g. JaneDoe\nComplete all lessons:\n\n\n1: Basic Building Blocks\n2: Workspace and Files\n3: Sequences of Numbers\n4: Vectors\n5: Missing Values\n6: Subsetting Vectors\n7: Matrices and Data Frames\n8: Logic\n\n\n\n\n\n\n\nImportant\n\n\n\n\nYou can take breaks between lessons (like shutting down Swirl and exiting RStudio), just use the same name when logging in for documenting purposes on the instructor’s end.\nIf you start a lesson, please complete it fully before taking a break. If you stop RStudio in the middle of a lesson, your progress for that lesson may still be saved in the history for that package locally, but will not be saved or sent to us on the instructor’s end.\nIf you do not get to the Google Form end of any lesson, please email one of the instructors as soon as possible.\n\n\n\n\n\nSubmission\n\n\n\n\n\n\nNote\n\n\n\n\nFYI: You need a google account to submit the assignment.\nSwirl will navigate you to the submission Google Form of each lesson. However, review the instructions below before submitting to make sure you are able to submit correctly.\nWhen submitting please make sure the box is not empty, if it is please email one of the instructors.\n\n\n\n\nAt the end of each lesson the following question will appear, enter the selection Yes to be redirected to your autofilled Google Form submission\n\nOnce you’ve been redirected, your Swirl activity will be autofilled into the submission box, just hit the Submit button\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf you do not get to the Google Form at the end of any lesson, please email one of the instructors as soon as possible."
  },
  {
    "objectID": "lectures/index_L05.html",
    "href": "lectures/index_L05.html",
    "title": "Dimension Reduction with tidymodels",
    "section": "",
    "text": "Before class, watch this video from Data Scientists Julie Silge from RStudio. She does a real-time data analysis using Principal Component Analysis (PCA) of the best hip hop songs of all time according to critics ratings. This video is not a detailed explanation of what PCA is or how it works (there is a lot of that maths on the internet if you want). In contrast, it is a live, real-time, analysis that asks the question what song features make a hip hop song the best. I want you to see that PCA is often a first line of inquiry when exploring a dataset.\n\nFor background, TidyTuesday is a weekly podcast and community activity that brings an interesting dataset to the data science community each week to do some cool plotting or analysis on. It provides interesting data to use for teaching purposes or code testing.\nThis video goes pretty fast but all the code she uses is below the video."
  },
  {
    "objectID": "lectures/index_L05.html#learning-objectives",
    "href": "lectures/index_L05.html#learning-objectives",
    "title": "Dimension Reduction with tidymodels",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nDefine dimension reduction\nExplain why dimension reduction is used\nBuild a model to perform PCA on a dataset"
  },
  {
    "objectID": "lectures/index_L05.html#slides",
    "href": "lectures/index_L05.html#slides",
    "title": "Dimension Reduction with tidymodels",
    "section": "Slides",
    "text": "Slides\n\nLecture 05: Dimension Reduction in tidymodels\nIn-Class Activity"
  },
  {
    "objectID": "lectures/index_L03.html",
    "href": "lectures/index_L03.html",
    "title": "Data Wrangling with tidyverse",
    "section": "",
    "text": "Before class, read the pre-reading materials."
  },
  {
    "objectID": "lectures/index_L03.html#learning-objectives",
    "href": "lectures/index_L03.html#learning-objectives",
    "title": "Data Wrangling with tidyverse",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nRecognize and define an untidy dataset\nRecognize and convert between long and wide data formats\nUnderstand when to use both formats\nPipe functions together to perform multiple commands in sequence"
  },
  {
    "objectID": "lectures/index_L03.html#slides",
    "href": "lectures/index_L03.html#slides",
    "title": "Data Wrangling with tidyverse",
    "section": "Slides",
    "text": "Slides\n\nLecture 03: Data Wrangling\nIn-Class Activity"
  },
  {
    "objectID": "lectures/index_L01.html",
    "href": "lectures/index_L01.html",
    "title": "Welcome to the Course!",
    "section": "",
    "text": "The course will exclusively use R in the integrated development environment (IDE) called R Studio. Since the summer semester is only 1 month (10 classes), it is critical to have your computer set up and ready to go before the first day of class.\nThroughout the course, there will be a set of pre-lecture activities and/or readings for you to complete. These activities prepare you for the in-class activities. Failure to complete them will put you behind in an already compressed course.\nTo prepare for the first class:\n\nInstall R and R Studio\n\ninstallation video tutorial\nDetailed instructions on installing R and R Studio are available on swirl.\n\nComplete step 1 and step 2\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSwirl is a great tool to learn R interactively that we will use in the course. Feel free to explore more.\n\n\n\nIn the first class, we will begin to get familiar with R. Complete the Pre-reading material about R. Pre-reading materials for each class are located under Resources. Sometimes, pre-reading materials are direct links to external resources for you to read in advance of the lecture."
  },
  {
    "objectID": "lectures/index_L01.html#learning-objectives",
    "href": "lectures/index_L01.html#learning-objectives",
    "title": "Welcome to the Course!",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nGet an overview of the course (including staff, format, grading, etc)\nBe able to recognize and operate an Integrated Developer Environment (IDE).\nBe able to recognize a Quarto document and describe reasons why to use a unified authoring framework.\nBe able to create a R project & Quarto document."
  },
  {
    "objectID": "lectures/index_L01.html#slides",
    "href": "lectures/index_L01.html#slides",
    "title": "Welcome to the Course!",
    "section": "Slides",
    "text": "Slides\n\nLecture 01: Welcome and Introductions\nIn-Class Activity"
  },
  {
    "objectID": "lectures/index_L01.html#acknowledgements",
    "href": "lectures/index_L01.html#acknowledgements",
    "title": "Welcome to the Course!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nsw carpentry: https://carpentries-incubator.github.io/bioc-intro/20-r-rstudio.html\ndata carpentry: https://datacarpentry.github.io/genomics-r-intro/00-introduction.html"
  },
  {
    "objectID": "lectures/index_L01.html#summary",
    "href": "lectures/index_L01.html#summary",
    "title": "Welcome to the Course!",
    "section": "Summary",
    "text": "Summary\n\nAccess all course material on the course website (https://lindsaynhayes.github.io/introR2025/).\nRStudio makes you a more efficient programmer and gives you easy access to all the components of data analysis.\nQuarto is an open-source scientific and technical publishing system to create reproducible, production quality data analysis reports and much more. During the course we will primarily use the HTML output format."
  },
  {
    "objectID": "lectures/index_L01.html#additional-practice",
    "href": "lectures/index_L01.html#additional-practice",
    "title": "Welcome to the Course!",
    "section": "Additional practice",
    "text": "Additional practice\nHere are some additional practice questions to help you think about the material discussed.\nCheck out this article on the 10 Best Practices for Data Science here\n\n\n\n\n\n\nPractice Tasks\n\n\n\n\nIn RStudio, create a new R project and then create a new Quarto document in that R project. Read the instructions. Practice running the chunks individually using the “play” button and the keyboard short cut.\nChange the settings of the Quarto document to display the output “inline” or in the “console”.\nRender the document and view the HTML output. Verify that you can modify the code, re-run it, and see modified output."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the Course Website for Cell 6351 Intro to Reproducible Data Analysis Using R!"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome",
    "section": "Overview",
    "text": "Overview\nCurrent biomedical research regularly generates large datasets that require increasing computational skills to process and analyze including in genomics, behavior, imaging, and molecular biology. As a result, scientists today require fundamental computational and programming skill-sets to efficiently perform data analyses, generate complex models for testing, develop reproducible data analyses, and effectively communicate results through clear and meaningful graphics. However, these skills are often challenging due to a lack of structured training in data science.\nIntro to Reproducible Data Analysis Using R is a hands-on course that will introduce the R statistical software including data structures, wrangling data, plotting data, basic statistics, data modeling, and literate coding practices. There is no textbook for the course. All course materials will be provided on Canvas or this website. While no formal background in statistics or programming is necessary, a general knowledge of basic statistics, experimental design, and computer file storage systems will be helpful. All classes will involve active learning/coding activities so a laptop with power cord is required for all sessions. Students will be expected to complete readings prior to class, attend lectures to dive deeper into those topics in class, participate in classroom activities, complete projects practicing and reinforcing the computing and programming skills. There will be a final project, which will include a class presentation."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Welcome",
    "section": "Course Information",
    "text": "Course Information\n\nCourse Staff: Prof. Lindsay Hayes\nCourse Staff: TA. Eleana Cabello\nLectures: 10:30am-noon\nLocation: BMSB 324\nOffice Hours: by email"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "Welcome",
    "section": "Course Details",
    "text": "Course Details\nAll course details are available on the Course page. The syllabus is available online here, download here, or on Canvas."
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html",
    "href": "activities/classwork06/Pred_Penguins.html",
    "title": "Machine Learning using tidymodels",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will using 2 classification models to predict the species of penguin based on the penguin biometric data."
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#about-the-activity",
    "href": "activities/classwork06/Pred_Penguins.html#about-the-activity",
    "title": "Machine Learning using tidymodels",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will using 2 classification models to predict the species of penguin based on the penguin biometric data."
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#load-the-packages",
    "href": "activities/classwork06/Pred_Penguins.html#load-the-packages",
    "title": "Machine Learning using tidymodels",
    "section": "Load the Packages",
    "text": "Load the Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(ranger)"
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#explore-the-data",
    "href": "activities/classwork06/Pred_Penguins.html#explore-the-data",
    "title": "Machine Learning using tidymodels",
    "section": "Explore the Data",
    "text": "Explore the Data\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\npenguins |&gt; count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124"
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#prep-the-data",
    "href": "activities/classwork06/Pred_Penguins.html#prep-the-data",
    "title": "Machine Learning using tidymodels",
    "section": "Prep the Data",
    "text": "Prep the Data\n\n# set a seed in order to make the analysis reproducible.\nset.seed(462)\n\n# split the data into training and testing sets. We will train the model on the training set and then test how well it worked on the testing data.\n\n# split the data 70% for training and 30% for testing. The bulk of the data is usually used for training the models. \nsplit_data &lt;- initial_split(penguins, prop=0.7, strata = species) \ndata_training &lt;- training(split_data)\ndata_testing &lt;- testing(split_data)\n\n# lets check it did the split correctly, if a different seed was used a the splits would be slightly different.  \n\ndata_training |&gt; \n  group_by(species) |&gt;\n  summarise( count = n(),\n             percent = n()/nrow(data_training) * 100)\n\n# A tibble: 3 × 3\n  species   count percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Adelie      106    44.4\n2 Chinstrap    47    19.7\n3 Gentoo       86    36.0\n\ndata_testing |&gt; \n  group_by(species) |&gt;\n  summarise( count = n(),\n             percent = n()/nrow(data_testing) * 100)\n\n# A tibble: 3 × 3\n  species   count percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Adelie       46    43.8\n2 Chinstrap    21    20  \n3 Gentoo       38    36.2\n\n# The recipe sets up what data we are going to use and how it to be treated before doing the modeling.\npenguin_recipe &lt;-\n  recipe( species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins) %&gt;%\n  step_normalize(all_predictors())\n\npenguin_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_predictors()\n\n# The prep step pulls in all the variables from the recipe based on the dataset we give it. \ndata_prep &lt;- prep(penguin_recipe, data_training)\ndata_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Training information \n\n\nTraining data contained 239 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: bill_length_mm bill_depth_mm, ... | Trained\n\n# the bake steps preforms the prep steps and in this case normalizes all the data.\ndata_bake &lt;- bake(data_prep, new_data = NULL)\ndata_bake\n\n# A tibble: 239 × 5\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g species\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1         -0.808        0.120             -1.10      -0.501  Adelie \n 2         -0.660        0.426             -0.447     -1.21   Adelie \n 3         -0.845        1.75              -0.812     -0.695  Adelie \n 4         -0.918        0.324             -1.47      -0.727  Adelie \n 5         -0.863        1.24              -0.447      0.625  Adelie \n 6         -1.80         0.477             -0.593     -0.920  Adelie \n 7         -0.346        1.55              -0.812      0.0780 Adelie \n 8         -1.12        -0.0333            -1.10      -1.15   Adelie \n 9         -1.12         0.0687            -1.54      -0.630  Adelie \n10         -0.974        2.06              -0.739     -0.501  Adelie \n# ℹ 229 more rows"
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#define-the-models",
    "href": "activities/classwork06/Pred_Penguins.html#define-the-models",
    "title": "Machine Learning using tidymodels",
    "section": "Define the models",
    "text": "Define the models\nRandom Forest uses the command rand_forest() which takes the following arguments. We will use the defaults for some values.\n\nmode options are “unknown”, “regression”, “classification”, or “censored regression”\nengine options are “ranger”, “randomForest”, or “spark”\nmtry the number of predictors that will be randomly sampled at each split when creating the tree model.\ntrees the number of trees to build.\nmin_n the minimum number of data points in a node to stop splitting\n\n\n# MODEL 1 Random Forest\nrf_model &lt;- \n  \n  # specify model\n  rand_forest() |&gt;\n  \n  # mode as classification not continuous\n  set_mode(\"classification\") |&gt;\n  \n  # engine/package that underlies the model (ranger is default)\n  set_engine(\"ranger\") |&gt;\n  \n  # we only have 4 predictors so mtry can't be more than 4\n  set_args(mtry = 4, trees = 200)\n  \n\n# Put everything together \nrf_wflow &lt;- \n  workflow() |&gt;\n  add_recipe(penguin_recipe) |&gt;\n  add_model(rf_model)\n\n\n# train the model\nrf_fit &lt;- fit(rf_wflow, data_training)\n\nLogistic Regression uses the command multinom_reg() which takes the following arguments. We will use the defaults for some values.\n\nmode only “classification” is available\nengine options are “nnet”, “brulee”, “glmnet”, “h2o”, “keras”, “spark”\npenalty only used in keras models\nmixture only used in keras models\n\n\n# MODEL 2 Logistic Regression\nlr_model &lt;- \n  \n  # specify that the model is a multinom_reg\n  multinom_reg() |&gt;\n  \n  # mode as classification not continuous\n  set_mode(\"classification\") |&gt;\n  \n  # select the engine/package that underlies the model (nnet is default)\n  set_engine(\"nnet\")\n  \n\n# Put everything together \nlr_wflow &lt;- \n  workflow() |&gt;\n  add_recipe(penguin_recipe) |&gt;\n  add_model(lr_model)\n\n# train the model\nlr_fit &lt;- fit(lr_wflow, data_training)"
  },
  {
    "objectID": "activities/classwork06/Pred_Penguins.html#compare-the-performance-of-the-two-models",
    "href": "activities/classwork06/Pred_Penguins.html#compare-the-performance-of-the-two-models",
    "title": "Machine Learning using tidymodels",
    "section": "Compare the performance of the two models",
    "text": "Compare the performance of the two models\n\n# predict the species of the testing data we held back for each model\nrf.predict &lt;- predict(rf_fit, data_testing)\nlr.predict &lt;- predict(lr_fit, data_testing)\n\n# create a table comparing the predicted species from the true species\nrf.outcome &lt;- rf.predict %&gt;%\n  transmute(pred = .pred_class,\n            truth = data_testing$species)\n\n# confusion matrix\nrf.outcome |&gt; conf_mat(pred, truth)\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        46         0      0\n  Chinstrap      1        20      0\n  Gentoo         1         0     37\n\n# accuracy\nrf.outcome |&gt; accuracy(pred, truth) -&gt; rf.acc\n\n# specificity\nrf.outcome |&gt; spec(pred, truth) -&gt; rf.spec\n\n# sensitivity\nrf.outcome |&gt; sens(pred, truth) -&gt; rf.sens\n\n# precision\nrf.outcome |&gt; precision(pred, truth) -&gt; rf.prec\n\nrf.eval &lt;- c(rf.acc$.estimate, rf.spec$.estimate, rf.sens$.estimate, rf.prec$.estimate)\nnames(rf.eval) &lt;- c(\"accuracy\", \"specificity\", \"sensitivity\", \"precision\")\n\n# create a table comparing the predicted species from the true species\nlr.outcome &lt;- lr.predict %&gt;%\n  transmute(pred = .pred_class,\n            truth = data_testing$species)\n\n# confusion matrix\nlr.outcome |&gt; conf_mat(pred, truth)\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        44         1      0\n  Chinstrap      0        21      0\n  Gentoo         0         0     37\n\n# accuracy\nlr.outcome |&gt; accuracy(pred, truth) -&gt; lr.acc\n\n# specificity\nlr.outcome |&gt; spec(pred, truth) -&gt; lr.spec\n\n# sensitivity\nlr.outcome |&gt; sens(pred, truth) -&gt; lr.sens\n\n# precision\nlr.outcome |&gt; precision(pred, truth) -&gt; lr.prec\n\nlr.eval = c(lr.acc$.estimate, lr.spec$.estimate, lr.sens$.estimate, lr.prec$.estimate)\nnames(lr.eval) &lt;- c(\"accuracy\", \"specificity\", \"sensitivity\", \"precision\")\n\nrbind(rf.eval, lr.eval)\n\n         accuracy specificity sensitivity precision\nrf.eval 0.9809524   0.9911765   0.9861111 0.9753551\nlr.eval 0.9902913   0.9943503   0.9848485 0.9925926\n\n\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ranger_0.17.0        palmerpenguins_0.1.1 yardstick_1.3.2     \n [4] workflowsets_1.1.1   workflows_1.2.0      tune_1.3.0          \n [7] rsample_1.3.0        recipes_1.3.1        parsnip_1.3.2       \n[10] modeldata_1.4.0      infer_1.0.8          dials_1.4.0         \n[13] scales_1.3.0         broom_1.0.7          tidymodels_1.3.0    \n[16] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n[19] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n[22] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n[25] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    timeDate_4041.110   fastmap_1.2.0      \n [4] digest_0.6.37       rpart_4.1.24        timechange_0.3.0   \n [7] lifecycle_1.0.4     survival_3.8-3      magrittr_2.0.3     \n[10] compiler_4.4.1      rlang_1.1.5         tools_4.4.1        \n[13] utf8_1.2.4          yaml_2.3.10         data.table_1.17.0  \n[16] knitr_1.50          htmlwidgets_1.6.4   DiceDesign_1.10    \n[19] withr_3.0.2         nnet_7.3-20         grid_4.4.1         \n[22] sparsevctrs_0.3.4   colorspace_2.1-1    future_1.49.0      \n[25] globals_0.18.0      iterators_1.0.14    MASS_7.3-64        \n[28] cli_3.6.4           rmarkdown_2.29      generics_0.1.3     \n[31] rstudioapi_0.17.1   future.apply_1.11.3 tzdb_0.4.0         \n[34] splines_4.4.1       parallel_4.4.1      vctrs_0.6.5        \n[37] hardhat_1.4.1       Matrix_1.7-2        jsonlite_1.9.1     \n[40] hms_1.1.3           listenv_0.9.1       foreach_1.5.2      \n[43] gower_1.0.2         glue_1.8.0          parallelly_1.45.0  \n[46] codetools_0.2-20    stringi_1.8.4       gtable_0.3.6       \n[49] munsell_0.5.1       GPfit_1.0-9         pillar_1.10.1      \n[52] furrr_0.3.1         htmltools_0.5.8.1   ipred_0.9-15       \n[55] lava_1.8.1          R6_2.6.1            lhs_1.2.0          \n[58] evaluate_1.0.3      lattice_0.22-6      backports_1.5.0    \n[61] class_7.3-23        Rcpp_1.0.14         prodlim_2024.06.25 \n[64] xfun_0.51           pkgconfig_2.0.3"
  },
  {
    "objectID": "activities/classwork03/Lecture_Commands3.html",
    "href": "activities/classwork03/Lecture_Commands3.html",
    "title": "Data Wrangling with tidyverse",
    "section": "",
    "text": "glimpse(starwars)\n\n# lets look at the first row of starwars\nstarwars[1,11:14]\n\nstarwars$starships[1]\n\nstarwars |&gt;\n  select(name, films) |&gt;\n  unnest_longer(films)\n\ntaylor |&gt; select(track_name, lyrics) |&gt; \n  unnest_longer(lyrics)\n\n\ntable(starwars$sex)\n\n\nstarwars |&gt;\n  group_by(sex) \n\n\nstarwars |&gt; count(sex)\n\nstarwars |&gt;\n  group_by(sex) |&gt;\n  summarize(mean_height = mean(height),\n            sd_height = sd(height))\n\n\n\nstarwars |&gt;\n  drop_na(height, sex) |&gt;\n  group_by(sex) |&gt;\n  summarize(mean_height = mean(height),\n            sd_height = sd(height))"
  },
  {
    "objectID": "activities/a/classwork_L02_answers.html",
    "href": "activities/a/classwork_L02_answers.html",
    "title": "Navigating Data Types in R - Answers",
    "section": "",
    "text": "Create a new Quarto document and erase the base text. You can keep the YAML header and rename it for yourself.\nAccess the Quarto document here from the github repository for the course.\nCopy and paste the content into your new Quarto Document. Make sure the code chunks copied correctly.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/a/classwork_L02_answers.html#about-the-activity",
    "href": "activities/a/classwork_L02_answers.html#about-the-activity",
    "title": "Navigating Data Types in R - Answers",
    "section": "",
    "text": "Create a new Quarto document and erase the base text. You can keep the YAML header and rename it for yourself.\nAccess the Quarto document here from the github repository for the course.\nCopy and paste the content into your new Quarto Document. Make sure the code chunks copied correctly.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/a/classwork_L02_answers.html#first-load-the-tidyverse-package",
    "href": "activities/a/classwork_L02_answers.html#first-load-the-tidyverse-package",
    "title": "Navigating Data Types in R - Answers",
    "section": "First Load the Tidyverse Package",
    "text": "First Load the Tidyverse Package\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "activities/a/classwork_L02_answers.html#ways-to-use-r",
    "href": "activities/a/classwork_L02_answers.html#ways-to-use-r",
    "title": "Navigating Data Types in R - Answers",
    "section": "Ways to Use R",
    "text": "Ways to Use R\n\n1. Arithmetic\n\n# example: addition/subtraction/multiplication/division\n\n193 + 45\n\n[1] 238\n\n2050 - 2025\n\n[1] 25\n\n50/250 * 100\n\n[1] 20\n\n# activity: assign the variable my_age as the current year minus your year of birth. \n\n\n\n2. Create R objects\n\n# vector\n# c(..., recursive = FALSE, use.names = TRUE)\nx &lt;- c(1:225)\nclass(x)\n\n[1] \"integer\"\n\n# matrix\n# matrix(data, nrow, ncol, byrow, dimnames)\ny &lt;- matrix(1:225, nrow=15, ncol=15, byrow = FALSE)\nclass(y)\n\n[1] \"matrix\" \"array\" \n\n# logical\n# testing each variable in the vector and outputting TRUE or FALSE\nover100 &lt;- x&gt;100\ntable(over100)\n\nover100\nFALSE  TRUE \n  100   125 \n\n\n\n\nYour Turn\n\n\nA. Create a vector of all the homeworlds in starwars using the starwars data.\n\n# create vector called \"homeworlds\" and assign it the value \"homeworld\" from the starwars data set\nhomeworlds &lt;- starwars$homeworld\n\n# how many worlds are there? hint: use the unique() function\nunique(homeworlds)\n\n [1] \"Tatooine\"       \"Naboo\"          \"Alderaan\"       \"Stewjon\"       \n [5] \"Eriadu\"         \"Kashyyyk\"       \"Corellia\"       \"Rodia\"         \n [9] \"Nal Hutta\"      \"Bestine IV\"     NA               \"Kamino\"        \n[13] \"Trandosha\"      \"Socorro\"        \"Bespin\"         \"Mon Cala\"      \n[17] \"Chandrila\"      \"Endor\"          \"Sullust\"        \"Cato Neimoidia\"\n[21] \"Coruscant\"      \"Toydaria\"       \"Malastare\"      \"Dathomir\"      \n[25] \"Ryloth\"         \"Aleen Minor\"    \"Vulpter\"        \"Troiken\"       \n[29] \"Tund\"           \"Haruun Kal\"     \"Cerea\"          \"Glee Anselm\"   \n[33] \"Iridonia\"       \"Iktotch\"        \"Quermia\"        \"Dorin\"         \n[37] \"Champala\"       \"Geonosis\"       \"Mirial\"         \"Serenno\"       \n[41] \"Concord Dawn\"   \"Zolan\"          \"Ojom\"           \"Skako\"         \n[45] \"Muunilinst\"     \"Shili\"          \"Kalee\"          \"Umbara\"        \n[49] \"Utapau\"        \n\n# is there a world called \"Ohio\"? how would you test this with code?\ntable(homeworlds == \"Ohio\")\n\n\nFALSE \n   77 \n\n# How many characters live on Naboo?\ntable(homeworlds == \"Naboo\")\n\n\nFALSE  TRUE \n   66    11 \n\nstarwars |&gt; filter(homeworld == \"Naboo\") |&gt; count(homeworld, .drop = FALSE)\n\n# A tibble: 1 × 2\n  homeworld     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Naboo        11\n\n# Who lives on Naboo? (hint use the \"name\" variable in the starwars data and the \"which\" function)\nunique(starwars$name[homeworlds == \"Naboo\"])\n\n [1] \"R2-D2\"         NA              \"Palpatine\"     \"Padmé Amidala\"\n [5] \"Jar Jar Binks\" \"Roos Tarpals\"  \"Rugor Nass\"    \"Ric Olié\"     \n [9] \"Quarsh Panaka\" \"Gregar Typho\"  \"Cordé\"         \"Dormé\"        \n\nwhich(homeworlds == \"Naboo\")\n\n [1]  3 20 34 35 36 37 38 41 59 60 65\n\nstarwars$name[which(homeworlds == \"Naboo\")]\n\n [1] \"R2-D2\"         \"Palpatine\"     \"Padmé Amidala\" \"Jar Jar Binks\"\n [5] \"Roos Tarpals\"  \"Rugor Nass\"    \"Ric Olié\"      \"Quarsh Panaka\"\n [9] \"Gregar Typho\"  \"Cordé\"         \"Dormé\"        \n\n\n\n\nB. Import the taylor package and explore the taylor_album_songs dataframe\n\n# Try loading in the taylor package and viewing the taylor_album_songs dataframe\nlibrary(taylor)\ntaylor_album_songs\n\n# A tibble: 240 × 29\n   album_name   ep    album_release track_number track_name     artist featuring\n   &lt;chr&gt;        &lt;lgl&gt; &lt;date&gt;               &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;    \n 1 Taylor Swift FALSE 2006-10-24               1 Tim McGraw     Taylo… &lt;NA&gt;     \n 2 Taylor Swift FALSE 2006-10-24               2 Picture To Bu… Taylo… &lt;NA&gt;     \n 3 Taylor Swift FALSE 2006-10-24               3 Teardrops On … Taylo… &lt;NA&gt;     \n 4 Taylor Swift FALSE 2006-10-24               4 A Place In Th… Taylo… &lt;NA&gt;     \n 5 Taylor Swift FALSE 2006-10-24               5 Cold As You    Taylo… &lt;NA&gt;     \n 6 Taylor Swift FALSE 2006-10-24               6 The Outside    Taylo… &lt;NA&gt;     \n 7 Taylor Swift FALSE 2006-10-24               7 Tied Together… Taylo… &lt;NA&gt;     \n 8 Taylor Swift FALSE 2006-10-24               8 Stay Beautiful Taylo… &lt;NA&gt;     \n 9 Taylor Swift FALSE 2006-10-24               9 Should've Sai… Taylo… &lt;NA&gt;     \n10 Taylor Swift FALSE 2006-10-24              10 Mary's Song (… Taylo… &lt;NA&gt;     \n# ℹ 230 more rows\n# ℹ 22 more variables: bonus_track &lt;lgl&gt;, promotional_release &lt;date&gt;,\n#   single_release &lt;date&gt;, track_release &lt;date&gt;, danceability &lt;dbl&gt;,\n#   energy &lt;dbl&gt;, key &lt;int&gt;, loudness &lt;dbl&gt;, mode &lt;int&gt;, speechiness &lt;dbl&gt;,\n#   acousticness &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, time_signature &lt;int&gt;, duration_ms &lt;int&gt;, explicit &lt;lgl&gt;,\n#   key_name &lt;chr&gt;, mode_name &lt;chr&gt;, key_mode &lt;chr&gt;, lyrics &lt;list&gt;\n\n# what is the \"class\" of the object taylor_album_songs?\nclass(taylor_album_songs)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# what types of data are in the object taylor_album_songs?\nglimpse(taylor_album_songs)\n\nRows: 240\nColumns: 29\n$ album_name          &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ ep                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ album_release       &lt;date&gt; 2006-10-24, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ track_number        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ track_name          &lt;chr&gt; \"Tim McGraw\", \"Picture To Burn\", \"Teardrops On My …\n$ artist              &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ featuring           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ bonus_track         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ promotional_release &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ single_release      &lt;date&gt; 2006-06-19, 2008-02-03, 2007-02-19, NA, NA, NA, N…\n$ track_release       &lt;date&gt; 2006-06-19, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ danceability        &lt;dbl&gt; 0.580, 0.658, 0.621, 0.576, 0.418, 0.589, 0.479, 0…\n$ energy              &lt;dbl&gt; 0.491, 0.877, 0.417, 0.777, 0.482, 0.805, 0.578, 0…\n$ key                 &lt;int&gt; 0, 7, 10, 9, 5, 5, 2, 8, 4, 2, 2, 8, 7, 4, 10, 5, …\n$ loudness            &lt;dbl&gt; -6.462, -2.098, -6.941, -2.881, -5.769, -4.055, -4…\n$ mode                &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ speechiness         &lt;dbl&gt; 0.0251, 0.0323, 0.0231, 0.0324, 0.0266, 0.0293, 0.…\n$ acousticness        &lt;dbl&gt; 0.57500, 0.17300, 0.28800, 0.05100, 0.21700, 0.004…\n$ instrumentalness    &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, …\n$ liveness            &lt;dbl&gt; 0.1210, 0.0962, 0.1190, 0.3200, 0.1230, 0.2400, 0.…\n$ valence             &lt;dbl&gt; 0.425, 0.821, 0.289, 0.428, 0.261, 0.591, 0.192, 0…\n$ tempo               &lt;dbl&gt; 76.009, 105.586, 99.953, 115.028, 175.558, 112.982…\n$ time_signature      &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ duration_ms         &lt;int&gt; 232107, 173067, 203040, 199200, 239013, 207107, 24…\n$ explicit            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ key_name            &lt;chr&gt; \"C\", \"G\", \"A#\", \"A\", \"F\", \"F\", \"D\", \"G#\", \"E\", \"D\"…\n$ mode_name           &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"major\", \"majo…\n$ key_mode            &lt;chr&gt; \"C major\", \"G major\", \"A# major\", \"A major\", \"F ma…\n$ lyrics              &lt;list&gt; [&lt;tbl_df[55 x 4]&gt;], [&lt;tbl_df[33 x 4]&gt;], [&lt;tbl_df[…\n\nas.data.frame(map_chr(taylor_album_songs, class))\n\n                    map_chr(taylor_album_songs, class)\nalbum_name                                   character\nep                                             logical\nalbum_release                                     Date\ntrack_number                                   integer\ntrack_name                                   character\nartist                                       character\nfeaturing                                    character\nbonus_track                                    logical\npromotional_release                               Date\nsingle_release                                    Date\ntrack_release                                     Date\ndanceability                                   numeric\nenergy                                         numeric\nkey                                            integer\nloudness                                       numeric\nmode                                           integer\nspeechiness                                    numeric\nacousticness                                   numeric\ninstrumentalness                               numeric\nliveness                                       numeric\nvalence                                        numeric\ntempo                                          numeric\ntime_signature                                 integer\nduration_ms                                    integer\nexplicit                                       logical\nkey_name                                     character\nmode_name                                    character\nkey_mode                                     character\nlyrics                                            list\n\n# change the \"key_mode\" from class \"character\" to class \"factor\"\ntaylor_album_songs$key_mode &lt;- factor(taylor_album_songs$key_mode)\n\n# How many albums are in the data set & how many songs on each album?\ntaylor_album_songs |&gt; count(album_name)\n\n# A tibble: 11 × 2\n   album_name                        n\n   &lt;chr&gt;                         &lt;int&gt;\n 1 1989 (Taylor's Version)          23\n 2 Fearless (Taylor's Version)      26\n 3 Lover                            18\n 4 Midnights                        26\n 5 Red (Taylor's Version)           30\n 6 Speak Now (Taylor's Version)     22\n 7 THE TORTURED POETS DEPARTMENT    31\n 8 Taylor Swift                     15\n 9 evermore                         17\n10 folklore                         17\n11 reputation                       15\n\n\n\n\nC. Which numeric song features are correlated with one another? Hint create a correlation matrix.\n\n# pick what features of the data you want to explore\ncolnames(taylor_album_songs)\n\n [1] \"album_name\"          \"ep\"                  \"album_release\"      \n [4] \"track_number\"        \"track_name\"          \"artist\"             \n [7] \"featuring\"           \"bonus_track\"         \"promotional_release\"\n[10] \"single_release\"      \"track_release\"       \"danceability\"       \n[13] \"energy\"              \"key\"                 \"loudness\"           \n[16] \"mode\"                \"speechiness\"         \"acousticness\"       \n[19] \"instrumentalness\"    \"liveness\"            \"valence\"            \n[22] \"tempo\"               \"time_signature\"      \"duration_ms\"        \n[25] \"explicit\"            \"key_name\"            \"mode_name\"          \n[28] \"key_mode\"            \"lyrics\"             \n\n# create a matrix of those features\nmat &lt;- taylor_album_songs[,c(1,10,11,12,13,14,15,17,18,19,20,21,22)]\ncolnames(mat)\n\n [1] \"album_name\"       \"single_release\"   \"track_release\"    \"danceability\"    \n [5] \"energy\"           \"key\"              \"loudness\"         \"speechiness\"     \n [9] \"acousticness\"     \"instrumentalness\" \"liveness\"         \"valence\"         \n[13] \"tempo\"           \n\n# evaluate the correlation matrix\nmat |&gt; GGally::ggpairs()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning: Removed 210 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 210 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 210 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 210 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n# Which values show a positive correlation? Which values show a negative correlation?\n\n#var1 &lt;-\n#var2 &lt;-\n#var3 &lt;- \n#var4 &lt;- \n\n\n# Plot the correlations\n\n#ggplot(taylor, aes(x=var1, y=var2)) + geom_point(size=3)\n#ggplot(taylor, aes(x=var3, y=var4)) + geom_point(size=3)\n\n# plot loudness vs energy\nggplot(taylor_album_songs, aes(x=loudness, y=energy)) + \n  geom_point(size=3)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# plot acousticness vs energy\nggplot(taylor_album_songs, aes(x=acousticness, y=energy)) + \n  geom_point(size=3)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# plot loudness vs energy, color by album, facet by album\nggplot(taylor_album_songs, aes(x=loudness, y=energy, color = album_name)) + \n  geom_point(size=3) + \n  scale_color_albums() + \n  facet_wrap(~album_name)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# plot acousticness vs energy, color by album, facet by album\nggplot(taylor_album_songs, aes(x=acousticness, y=energy, color = album_name)) +\n  geom_point(size=3) + \n  scale_color_albums() + \n  facet_wrap(~album_name)\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "activities.html",
    "href": "activities.html",
    "title": "In-Class Activities",
    "section": "",
    "text": "In-class activities are listed below:\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-07-01\n\n\nIntroduction to the Palmer Penguins\n\n\n \n\n\n\n\n\n\n2025-07-03\n\n\nNavigating Data Types in R - Answers\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-03\n\n\nNavigating Data Types in R\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-08\n\n\nData Wrangling with tidyverse - Answers\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-08\n\n\nData Wrangling with tidyverse\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-08\n\n\nData Wrangling with tidyverse\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-14\n\n\nDimension Reduction using tidymodels\n\n\nLindsay N. Hayes\n\n\n\n\n\n\n2025-07-16\n\n\nMachine Learning using tidymodels\n\n\nLindsay N. Hayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "activities/a/classwork_L03_answers.html",
    "href": "activities/a/classwork_L03_answers.html",
    "title": "Data Wrangling with tidyverse - Answers",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/a/classwork_L03_answers.html#about-the-activity",
    "href": "activities/a/classwork_L03_answers.html#about-the-activity",
    "title": "Data Wrangling with tidyverse - Answers",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/a/classwork_L03_answers.html#load-the-tidyverse-package",
    "href": "activities/a/classwork_L03_answers.html#load-the-tidyverse-package",
    "title": "Data Wrangling with tidyverse - Answers",
    "section": "Load the Tidyverse Package",
    "text": "Load the Tidyverse Package\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp"
  },
  {
    "objectID": "activities/a/classwork_L03_answers.html#reshaping-and-summarizing-data",
    "href": "activities/a/classwork_L03_answers.html#reshaping-and-summarizing-data",
    "title": "Data Wrangling with tidyverse - Answers",
    "section": "Reshaping and Summarizing Data",
    "text": "Reshaping and Summarizing Data\nA common type of data that requires reshaping is time course data.\nUsing tidyverse principles answer the questions below:\n1. Which month had the most and least passengers in the AirPassengers data? The AirPassengers data which is a time-series of data representing the monthly international airline passenger numbers from January 1949 to December 1960. Search for AirPassengers in the Help to learn more about the dataset.\n\n# Load and inspect the data, a little reshaping to get in to an easy to read format for you.\nAP_matrix &lt;- matrix(AirPassengers, nrow = length(unique(floor(time(AirPassengers)))), byrow = TRUE)\ncolnames(AP_matrix) &lt;- month.abb\nrownames(AP_matrix) &lt;- unique(floor(time(AirPassengers)))\nAP_df &lt;- as.data.frame(AP_matrix)\nAP_df$Year &lt;- rownames(AP_matrix)\ndim(AP_df)\n\n[1] 12 13\n\n# A. Is the data long or wide? What form does it need to be in?\n\ndat &lt;- AP_df |&gt;\n  pivot_longer(cols = 1:12, names_to = \"month\", values_to = \"count\")\n\nglimpse(dat)\n\nRows: 144\nColumns: 3\n$ Year  &lt;chr&gt; \"1949\", \"1949\", \"1949\", \"1949\", \"1949\", \"1949\", \"1949\", \"1949\", …\n$ month &lt;chr&gt; \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"…\n$ count &lt;dbl&gt; 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 115,…\n\ndat$month &lt;- factor(dat$month, levels = month.abb)\n\n\n# B. How can we extract the the most and least traveled months of the year? \n\ndat |&gt;\n  group_by(Year) |&gt;\n  summarise(max = max(count),\n            max_month = month[which.max(count)],\n            min = min(count),\n            min_month = month[which.min(count)],)\n\n# A tibble: 12 × 5\n   Year    max max_month   min min_month\n   &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;    \n 1 1949    148 Jul         104 Nov      \n 2 1950    170 Jul         114 Nov      \n 3 1951    199 Jul         145 Jan      \n 4 1952    242 Aug         171 Jan      \n 5 1953    272 Aug         180 Nov      \n 6 1954    302 Jul         188 Feb      \n 7 1955    364 Jul         233 Feb      \n 8 1956    413 Jul         271 Nov      \n 9 1957    467 Aug         301 Feb      \n10 1958    505 Aug         310 Nov      \n11 1959    559 Aug         342 Feb      \n12 1960    622 Jul         390 Nov      \n\ndat |&gt;\n  group_by(Year) |&gt;\n  ggplot(aes (x = month, y = count, group = Year, color = Year)) + geom_line()\n\n\n\n\n\n\n\n\n2. What was the percent increase in passengers each year between Aug and Nov?\n\n# To answer this question we need to find the ratio of Aug and Nov travelers. We need the data in the wide format.\n\nAP_df\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Year\n1949 112 118 132 129 121 135 148 148 136 119 104 118 1949\n1950 115 126 141 135 125 149 170 170 158 133 114 140 1950\n1951 145 150 178 163 172 178 199 199 184 162 146 166 1951\n1952 171 180 193 181 183 218 230 242 209 191 172 194 1952\n1953 196 196 236 235 229 243 264 272 237 211 180 201 1953\n1954 204 188 235 227 234 264 302 293 259 229 203 229 1954\n1955 242 233 267 269 270 315 364 347 312 274 237 278 1955\n1956 284 277 317 313 318 374 413 405 355 306 271 306 1956\n1957 315 301 356 348 355 422 465 467 404 347 305 336 1957\n1958 340 318 362 348 363 435 491 505 404 359 310 337 1958\n1959 360 342 406 396 420 472 548 559 463 407 362 405 1959\n1960 417 391 419 461 472 535 622 606 508 461 390 432 1960\n\n# how can we add the ratio?\nAP_df |&gt; mutate(ratio = ((Aug/Nov)-1)*100) |&gt;\n  ggplot(aes(x = Year, y = ratio)) + geom_point(size = 3)\n\n\n\n\n\n\n\n\n\nWhich diet caused the chicks to grow more? We will use the ChickWeight data. Use the help to read more about the data.\n\n\n# First lets just look at the data.\nglimpse(ChickWeight)\n\nRows: 578\nColumns: 4\n$ weight &lt;dbl&gt; 42, 51, 59, 64, 76, 93, 106, 125, 149, 171, 199, 205, 40, 49, 5…\n$ Time   &lt;dbl&gt; 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 0, 2, 4, 6, 8, 10, 1…\n$ Chick  &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n# Second, look at the distribution of the data. Count how many timepoints was measured nad how many chicks were in each Diet. \n\nChickWeight |&gt; count(Diet)\n\n  Diet   n\n1    1 220\n2    2 120\n3    3 120\n4    4 118\n\nChickWeight |&gt; count(Chick)\n\n   Chick  n\n1     18  2\n2     16  7\n3     15  8\n4     13 12\n5      9 12\n6     20 12\n7     10 12\n8      8 11\n9     17 12\n10    19 12\n11     4 12\n12     6 12\n13    11 12\n14     3 12\n15     1 12\n16    12 12\n17     2 12\n18     5 12\n19    14 12\n20     7 12\n21    24 12\n22    30 12\n23    22 12\n24    23 12\n25    27 12\n26    28 12\n27    26 12\n28    25 12\n29    29 12\n30    21 12\n31    33 12\n32    37 12\n33    36 12\n34    31 12\n35    39 12\n36    38 12\n37    32 12\n38    40 12\n39    34 12\n40    35 12\n41    44 10\n42    45 12\n43    43 12\n44    41 12\n45    47 12\n46    49 12\n47    46 12\n48    50 12\n49    42 12\n50    48 12\n\nn_distinct(ChickWeight$Chick)\n\n[1] 50\n\nChickWeight |&gt; count(Time)\n\n   Time  n\n1     0 50\n2     2 50\n3     4 49\n4     6 49\n5     8 49\n6    10 49\n7    12 49\n8    14 48\n9    16 47\n10   18 47\n11   20 46\n12   21 45\n\nChickWeight |&gt; \n  group_by(Diet) |&gt;\n  count(Chick)\n\n# A tibble: 50 × 3\n# Groups:   Diet [4]\n   Diet  Chick     n\n   &lt;fct&gt; &lt;ord&gt; &lt;int&gt;\n 1 1     18        2\n 2 1     16        7\n 3 1     15        8\n 4 1     13       12\n 5 1     9        12\n 6 1     20       12\n 7 1     10       12\n 8 1     8        11\n 9 1     17       12\n10 1     19       12\n# ℹ 40 more rows\n\nChickWeight |&gt; \n  filter(Time == 0) |&gt;\n  count(Diet)\n\n  Diet  n\n1    1 20\n2    2 10\n3    3 10\n4    4 10\n\nChickWeight |&gt; \n  group_by(Diet) |&gt;\n  summarize(no.chicks = n_distinct(Chick))\n\n# A tibble: 4 × 2\n  Diet  no.chicks\n  &lt;fct&gt;     &lt;int&gt;\n1 1            20\n2 2            10\n3 3            10\n4 4            10\n\n\nOk so we know there are 4 diets with 20 chicks on Diet 1, and 10 chicks each on diets 2-4.\nNow lets figure out which diet leads to the heaviest chicks.\n\n# we can plot it to get a first view\n\nChickWeight |&gt;\n  ggplot(aes(x = Time, y = weight, group = Chick, color = Diet)) + \n  geom_line() +\n  theme_cowplot()\n\n\n\n\n\n\n\nChickWeight |&gt;\n  pivot_wider(names_from = Time, names_prefix = \"day_\", values_from = weight) |&gt;\n  mutate(weight.gain = day_18 - day_0) |&gt;\n  ggplot(aes(x = Diet, y = weight.gain, fill = Diet)) + geom_boxplot()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\ndata &lt;- ChickWeight |&gt;\n  pivot_wider(names_from = Time, names_prefix = \"day_\", values_from = weight) |&gt;\n  mutate(weight.gain = day_18 - day_0)\n\nmod &lt;- aov(weight.gain ~ Diet, data = data)\nsummary(mod)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  37479   12493    4.63 0.00682 **\nResiduals   43 116029    2698                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n3 observations deleted due to missingness\n\nTukeyHSD(mod)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight.gain ~ Diet, data = data)\n\n$Diet\n         diff       lwr       upr     p adj\n2-1  29.64706 -25.67665  84.97077 0.4867633\n3-1  74.94706  19.62335 130.27077 0.0041372\n4-1  44.54706 -10.77665  99.87077 0.1533433\n3-2  45.30000 -16.78246 107.38246 0.2229497\n4-2  14.90000 -47.18246  76.98246 0.9179605\n4-3 -30.40000 -92.48246  31.68246 0.5626028\n\npairwise.t.test(data$weight.gain, data$Diet, p.adjust.method = \"BH\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  data$weight.gain and data$Diet \n\n  1      2      3     \n2 0.2371 -      -     \n3 0.0046 0.1154 -     \n4 0.1112 0.5247 0.2371\n\nP value adjustment method: BH \n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nmod &lt;- lmer(weight ~ Diet * Time + (1|Chick), data = ChickWeight)\nsummary(mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: weight ~ Diet * Time + (1 | Chick)\n   Data: ChickWeight\n\nREML criterion at convergence: 5466.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3158 -0.5900 -0.0693  0.5361  3.6024 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Chick    (Intercept) 545.7    23.36   \n Residual             643.3    25.36   \nNumber of obs: 578, groups:  Chick, 50\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  31.5143     6.1163  70.7030   5.152 2.23e-06 ***\nDiet2        -2.8807    10.5479  69.6438  -0.273    0.786    \nDiet3       -13.2640    10.5479  69.6438  -1.258    0.213    \nDiet4        -0.4016    10.5565  69.8601  -0.038    0.970    \nTime          6.7115     0.2584 532.8900  25.976  &lt; 2e-16 ***\nDiet2:Time    1.8977     0.4284 527.6886   4.430 1.15e-05 ***\nDiet3:Time    4.7114     0.4284 527.6886  10.998  &lt; 2e-16 ***\nDiet4:Time    2.9506     0.4340 528.0372   6.799 2.86e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) Diet2  Diet3  Diet4  Time   Dt2:Tm Dt3:Tm\nDiet2      -0.580                                          \nDiet3      -0.580  0.336                                   \nDiet4      -0.579  0.336  0.336                            \nTime       -0.426  0.247  0.247  0.247                     \nDiet2:Time  0.257 -0.431 -0.149 -0.149 -0.603              \nDiet3:Time  0.257 -0.149 -0.431 -0.149 -0.603  0.364       \nDiet4:Time  0.254 -0.147 -0.147 -0.432 -0.595  0.359  0.359"
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html",
    "href": "activities/classwork01/palmerpenguins_intro.html",
    "title": "Introduction to the Palmer Penguins",
    "section": "",
    "text": "This is a Quarto document that you can download from the github repository for the course, and as you see below there is a combination of narrative text, code chunks, code outputs, citations/references, embedded links, and even embedded static images. Quarto documents are highly customizable and the purpose is to generate a story-telling narrative of data by incorporating all these components into a single document.\nIn order to jump into data analysis we first need data! R has several datasets readily available to start explore including:\n\nAnderson’s Iris dataset (see datasets::iris). The iris data gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n1974 Motor Trend Cars dataset (see datasets::mtcars). The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\nand many more (see tab complete options for datasets::).\n\n\n\n\nThe palmerpenguins R package contains two datasets. In this introductory vignette, we’ll highlight some of the properties of these datasets that make them useful for statistics and data science education."
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#about-the-activity",
    "href": "activities/classwork01/palmerpenguins_intro.html#about-the-activity",
    "title": "Introduction to the Palmer Penguins",
    "section": "",
    "text": "This is a Quarto document that you can download from the github repository for the course, and as you see below there is a combination of narrative text, code chunks, code outputs, citations/references, embedded links, and even embedded static images. Quarto documents are highly customizable and the purpose is to generate a story-telling narrative of data by incorporating all these components into a single document.\nIn order to jump into data analysis we first need data! R has several datasets readily available to start explore including:\n\nAnderson’s Iris dataset (see datasets::iris). The iris data gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n1974 Motor Trend Cars dataset (see datasets::mtcars). The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\nand many more (see tab complete options for datasets::).\n\n\n\n\nThe palmerpenguins R package contains two datasets. In this introductory vignette, we’ll highlight some of the properties of these datasets that make them useful for statistics and data science education."
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#meet-the-penguins",
    "href": "activities/classwork01/palmerpenguins_intro.html#meet-the-penguins",
    "title": "Introduction to the Palmer Penguins",
    "section": "Meet the penguins",
    "text": "Meet the penguins\nThe palmerpenguins data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\nThese data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy."
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#installation",
    "href": "activities/classwork01/palmerpenguins_intro.html#installation",
    "title": "Introduction to the Palmer Penguins",
    "section": "Installation",
    "text": "Installation\nYou can install the released version of palmerpenguins from CRAN with:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#the-palmerpenguins-package",
    "href": "activities/classwork01/palmerpenguins_intro.html#the-palmerpenguins-package",
    "title": "Introduction to the Palmer Penguins",
    "section": "The palmerpenguins package",
    "text": "The palmerpenguins package\nThe curated palmerpenguins::penguins dataset contains 8 variables (n = 344 penguins). You can read more about the variables by typing ?penguins.\n\ndata &lt;- penguins\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe palmerpenguins::penguins data contains 333 complete cases, with 19 missing values."
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#lets-explore",
    "href": "activities/classwork01/palmerpenguins_intro.html#lets-explore",
    "title": "Introduction to the Palmer Penguins",
    "section": "Let’s Explore",
    "text": "Let’s Explore\nLets explore, visualization, and analyses some of the penguin data. Below are just a few examples to get you quickly waddling along with the penguins. You can check out more in vignette(\"examples\").\n\nExploring factors\nThe penguins data has three factor variables:\n\npenguins |&gt;\n  dplyr::select(where(is.factor)) |&gt;\n  glimpse()\n\nRows: 344\nColumns: 3\n$ species &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…\n$ island  &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgers…\n$ sex     &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, …\n\n\n\n# Count penguins for each species / island\npenguins %&gt;%\n  count(species, island, .drop = FALSE)\n\n# A tibble: 9 × 3\n  species   island        n\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;\n1 Adelie    Biscoe       44\n2 Adelie    Dream        56\n3 Adelie    Torgersen    52\n4 Chinstrap Biscoe        0\n5 Chinstrap Dream        68\n6 Chinstrap Torgersen     0\n7 Gentoo    Biscoe      124\n8 Gentoo    Dream         0\n9 Gentoo    Torgersen     0\n\ntable(penguins$species, penguins$island)\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n\nPlots are the easiest way to explore and understand your data.\n1. Lets first plot the number of penguins of each species across the 3 Palmer Archipelago islands.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(alpha = 0.8) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_minimal() +\n  facet_wrap(~species, ncol = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n2. Next lets look at how many males and females we have of each species of penguins.\n\n# Count penguins for each species / sex\npenguins |&gt;\n  count(species, sex, .drop = FALSE)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\nggplot(penguins, aes(x = sex, fill = species)) +\n  geom_bar(alpha = 0.8) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"), \n                    guide = FALSE) +\n  theme_minimal() +\n  facet_wrap(~species, ncol = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nExploring scatterplots\nThe penguins data also has four numeric variables. Lets plot those variables too!\n\npenguins |&gt;\n  dplyr::select(where(is.numeric)) |&gt;\n  glimpse()\n\nRows: 344\nColumns: 5\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n# Count how many penguins they evaluated each year\npenguins |&gt;\n  count(year, species, .drop = FALSE)\n\n# A tibble: 9 × 3\n   year species       n\n  &lt;int&gt; &lt;fct&gt;     &lt;int&gt;\n1  2007 Adelie       50\n2  2007 Chinstrap    26\n3  2007 Gentoo       34\n4  2008 Adelie       50\n5  2008 Chinstrap    18\n6  2008 Gentoo       46\n7  2009 Adelie       52\n8  2009 Chinstrap    24\n9  2009 Gentoo       44\n\ntable(penguins$year, penguins$species)\n\n      \n       Adelie Chinstrap Gentoo\n  2007     50        26     34\n  2008     50        18     46\n  2009     52        24     44\n\n\nScatterplot example 1: penguin flipper length versus body mass\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\nYou can add color and/or shape aesthetics in ggplot2 to layer in factor levels. With three factor variables to work with, you can add another factor layer with facets.\n\n# Scatterplot example 1: penguin flipper length versus body mass\nggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 2) +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) \n\n\n\n\n\n\n\n# Scatterplot example 2: penguin bill length versus bill depth\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species, \n                 shape = species),\n                 size = 2)  +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\"))\n\n\n\n\n\n\n\n# Scatterplot example 3: penguin flipper length versus body mass separated by species and sex\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(aes(color = sex)) +\n  scale_color_manual(values = c(\"deeppink2\",\"cornflowerblue\"), \n                     na.translate = FALSE) + # removes the \"NA\" values\n  facet_wrap(~species)\n\n\n\n\n\n\n\n# Scatterplot example 4: penguin flipper length versus body mass separated by all 3 factor variables\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(aes(color = sex, shape = island)) +\n  scale_color_manual(values = c(\"deeppink2\",\"cornflowerblue\"), \n                     na.translate = FALSE) + # removes the \"NA\" values\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\n\n\nExploring correlations\n\npenguins |&gt;\n  select(species, body_mass_g, ends_with(\"_mm\")) |&gt;\n  GGally::ggpairs(aes(color = species)) +\n  scale_colour_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))\n\n\n\n\n\n\n\n\n\n\nStatistically evaluating distributions\n\n# Jitter plot example: bill length by species\nggplot(data = penguins, aes(x = species, y = bill_length_mm)) +\n  geom_jitter(aes(color = species),\n              width = 0.1, \n              alpha = 0.7,\n              show.legend = FALSE) +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\"))\n\n\n\n\n\n\n\n# Boxplot examples: bill length by species\nggplot(data = penguins, aes(x = species, y = bill_length_mm)) + geom_boxplot(aes(fill = species)) +\n  scale_fill_manual(values = c(\"orange\",\"orchid\",\"cyan3\"))\n\n\n\n\n\n\n\nggplot(data = penguins, aes(x = species, y = bill_length_mm)) + \n  geom_boxplot(aes(fill = species), outlier.shape = NA, na.rm = TRUE) +\n  geom_jitter(aes(color = species),\n              width = 0.1, \n              alpha = 0.7, na.rm = TRUE,\n              show.legend = FALSE) +\n  scale_color_manual(values = c(\"darkorange2\",\"darkorchid\",\"cyan4\")) +\n  scale_fill_manual(values = c(\"orange\",\"orchid\",\"cyan3\"))\n\n\n\n\n\n\n\n# Histogram example: flipper length by species\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(fill = species), alpha = 0.5, position = \"identity\", na.rm = TRUE) +\n  scale_fill_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\"))"
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#acknowlegements",
    "href": "activities/classwork01/palmerpenguins_intro.html#acknowlegements",
    "title": "Introduction to the Palmer Penguins",
    "section": "Acknowlegements",
    "text": "Acknowlegements\nThis vignette was originally written by Allison Horst, Alison Hill, and Kristen Gorman. You can find info about the palmerpenguins package here!"
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#package-citation",
    "href": "activities/classwork01/palmerpenguins_intro.html#package-citation",
    "title": "Introduction to the Palmer Penguins",
    "section": "Package citation",
    "text": "Package citation\nPlease cite the palmerpenguins R package using:\n\ncitation(\"palmerpenguins\")\n\nTo cite palmerpenguins in publications use:\n\n  Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer\n  Archipelago (Antarctica) penguin data. R package version 0.1.0.\n  https://allisonhorst.github.io/palmerpenguins/. doi:\n  10.5281/zenodo.3960218.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {palmerpenguins: Palmer Archipelago (Antarctica) penguin data},\n    author = {Allison Marie Horst and Alison Presmanes Hill and Kristen B Gorman},\n    year = {2020},\n    note = {R package version 0.1.0},\n    doi = {10.5281/zenodo.3960218},\n    url = {https://allisonhorst.github.io/palmerpenguins/},\n  }"
  },
  {
    "objectID": "activities/classwork01/palmerpenguins_intro.html#references",
    "href": "activities/classwork01/palmerpenguins_intro.html#references",
    "title": "Introduction to the Palmer Penguins",
    "section": "References",
    "text": "References\nData originally published in:\n\nGorman KB, Williams TD, Fraser WR (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis). PLoS ONE 9(3):e90081. https://doi.org/10.1371/journal.pone.0090081\n\nIndividual datasets:\nIndividual data can be accessed directly via the Environmental Data Initiative:\n\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Adélie penguins (Pygoscelis adeliae) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/98b16d7d563f265cb52372c8ca99e60f (Accessed 2020-06-08).\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Gentoo penguin (Pygoscelis papua) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/7fca67fb28d56ee2ffa3d9370ebda689 (Accessed 2020-06-08).\nPalmer Station Antarctica LTER and K. Gorman, 2020. Structural size measurements and isotopic signatures of foraging among adult male and female Chinstrap penguin (Pygoscelis antarcticus) nesting along the Palmer Archipelago near Palmer Station, 2007-2009 ver 6. Environmental Data Initiative. https://doi.org/10.6073/pasta/c14dfcfada8ea13a17536e73eb6fbe9e (Accessed 2020-06-08).\n\nHave fun with the Palmer Archipelago penguins!"
  },
  {
    "objectID": "activities/classwork02/classwork_L02.html",
    "href": "activities/classwork02/classwork_L02.html",
    "title": "Navigating Data Types in R",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/classwork02/classwork_L02.html#about-the-activity",
    "href": "activities/classwork02/classwork_L02.html#about-the-activity",
    "title": "Navigating Data Types in R",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover using R as a calculator, creating R objects, and exploring the features of a data set."
  },
  {
    "objectID": "activities/classwork02/classwork_L02.html#first-load-the-tidyverse-package",
    "href": "activities/classwork02/classwork_L02.html#first-load-the-tidyverse-package",
    "title": "Navigating Data Types in R",
    "section": "First Load the Tidyverse Package",
    "text": "First Load the Tidyverse Package\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "activities/classwork02/classwork_L02.html#ways-to-use-r",
    "href": "activities/classwork02/classwork_L02.html#ways-to-use-r",
    "title": "Navigating Data Types in R",
    "section": "Ways to Use R",
    "text": "Ways to Use R\n\n1. Arithmetic\n\n# example: addition/subtraction/multiplication/division\n\n193 + 45\n\n[1] 238\n\n2050 - 2025\n\n[1] 25\n\n50/250 * 100\n\n[1] 20\n\n# activity: assign the variable my_age as the current year minus your year of birth. \n\n\n\n2. Create R objects\n\n# vector\n# c(..., recursive = FALSE, use.names = TRUE)\nx &lt;- c(1:225)\nclass(x)\n\n[1] \"integer\"\n\n# matrix\n# matrix(data, nrow, ncol, byrow, dimnames)\ny &lt;- matrix(1:225, nrow=15, ncol=15, byrow = FALSE)\nclass(y)\n\n[1] \"matrix\" \"array\" \n\n# logical\n# testing each variable in the vector and outputting TRUE or FALSE\nover100 &lt;- x &gt; 100\ntable(over100)\n\nover100\nFALSE  TRUE \n  100   125 \n\n\n\n\nYour Turn\n\n\nA. Create a vector of all the homeworlds in starwars using the starwars data.\n\n# create vector called \"homeworlds\" and assign it the value \"homeworld\" from the starwars data set\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\nhomeworlds &lt;- starwars$homeworld\n\nhomeworlds\n\n [1] \"Tatooine\"       \"Tatooine\"       \"Naboo\"          \"Tatooine\"      \n [5] \"Alderaan\"       \"Tatooine\"       \"Tatooine\"       \"Tatooine\"      \n [9] \"Tatooine\"       \"Stewjon\"        \"Tatooine\"       \"Eriadu\"        \n[13] \"Kashyyyk\"       \"Corellia\"       \"Rodia\"          \"Nal Hutta\"     \n[17] \"Corellia\"       \"Bestine IV\"     NA               \"Naboo\"         \n[21] \"Kamino\"         NA               \"Trandosha\"      \"Socorro\"       \n[25] \"Bespin\"         \"Mon Cala\"       \"Chandrila\"      NA              \n[29] \"Endor\"          \"Sullust\"        NA               \"Cato Neimoidia\"\n[33] \"Coruscant\"      \"Naboo\"          \"Naboo\"          \"Naboo\"         \n[37] \"Naboo\"          \"Naboo\"          \"Toydaria\"       \"Malastare\"     \n[41] \"Naboo\"          \"Tatooine\"       \"Dathomir\"       \"Ryloth\"        \n[45] \"Ryloth\"         \"Aleen Minor\"    \"Vulpter\"        \"Troiken\"       \n[49] \"Tund\"           \"Haruun Kal\"     \"Cerea\"          \"Glee Anselm\"   \n[53] \"Iridonia\"       \"Coruscant\"      \"Iktotch\"        \"Quermia\"       \n[57] \"Dorin\"          \"Champala\"       \"Naboo\"          \"Naboo\"         \n[61] \"Tatooine\"       \"Geonosis\"       \"Mirial\"         \"Mirial\"        \n[65] \"Naboo\"          \"Serenno\"        \"Alderaan\"       \"Concord Dawn\"  \n[69] \"Zolan\"          \"Ojom\"           \"Kamino\"         \"Kamino\"        \n[73] \"Coruscant\"      NA               \"Skako\"          \"Muunilinst\"    \n[77] \"Shili\"          \"Kalee\"          \"Kashyyyk\"       \"Alderaan\"      \n[81] \"Umbara\"         \"Utapau\"         NA               NA              \n[85] NA               NA               NA              \n\n# how many worlds are there? hint: use the unique function\n\nunique(homeworlds)\n\n [1] \"Tatooine\"       \"Naboo\"          \"Alderaan\"       \"Stewjon\"       \n [5] \"Eriadu\"         \"Kashyyyk\"       \"Corellia\"       \"Rodia\"         \n [9] \"Nal Hutta\"      \"Bestine IV\"     NA               \"Kamino\"        \n[13] \"Trandosha\"      \"Socorro\"        \"Bespin\"         \"Mon Cala\"      \n[17] \"Chandrila\"      \"Endor\"          \"Sullust\"        \"Cato Neimoidia\"\n[21] \"Coruscant\"      \"Toydaria\"       \"Malastare\"      \"Dathomir\"      \n[25] \"Ryloth\"         \"Aleen Minor\"    \"Vulpter\"        \"Troiken\"       \n[29] \"Tund\"           \"Haruun Kal\"     \"Cerea\"          \"Glee Anselm\"   \n[33] \"Iridonia\"       \"Iktotch\"        \"Quermia\"        \"Dorin\"         \n[37] \"Champala\"       \"Geonosis\"       \"Mirial\"         \"Serenno\"       \n[41] \"Concord Dawn\"   \"Zolan\"          \"Ojom\"           \"Skako\"         \n[45] \"Muunilinst\"     \"Shili\"          \"Kalee\"          \"Umbara\"        \n[49] \"Utapau\"        \n\nstarwars |&gt; count(homeworld) # NOTE IT INCLUDES missing values (NA)\n\n# A tibble: 49 × 2\n   homeworld          n\n   &lt;chr&gt;          &lt;int&gt;\n 1 Alderaan           3\n 2 Aleen Minor        1\n 3 Bespin             1\n 4 Bestine IV         1\n 5 Cato Neimoidia     1\n 6 Cerea              1\n 7 Champala           1\n 8 Chandrila          1\n 9 Concord Dawn       1\n10 Corellia           2\n# ℹ 39 more rows\n\n# is there a world called \"Ohio\"? how would you test this with code?\n\nstarwars |&gt; count(homeworld == \"Ohio\")\n\n# A tibble: 2 × 2\n  `homeworld == \"Ohio\"`     n\n  &lt;lgl&gt;                 &lt;int&gt;\n1 FALSE                    77\n2 NA                       10\n\ntable(homeworlds == \"Ohio\")\n\n\nFALSE \n   77 \n\n# How many characters live on Naboo?\n\ntable(homeworlds == \"Naboo\")\n\n\nFALSE  TRUE \n   66    11 \n\nstarwars |&gt; filter(homeworld == \"Naboo\") |&gt; count(homeworld)\n\n# A tibble: 1 × 2\n  homeworld     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Naboo        11\n\n# Who lives on Naboo? (hint use the \"names\" variable in the starwars data and the \"which\" function)\n\nstarwars |&gt; filter(homeworld == \"Naboo\") |&gt; select(name)\n\n# A tibble: 11 × 1\n   name         \n   &lt;chr&gt;        \n 1 R2-D2        \n 2 Palpatine    \n 3 Padmé Amidala\n 4 Jar Jar Binks\n 5 Roos Tarpals \n 6 Rugor Nass   \n 7 Ric Olié     \n 8 Quarsh Panaka\n 9 Gregar Typho \n10 Cordé        \n11 Dormé        \n\nhomeworlds == \"Naboo\"\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE    NA  TRUE FALSE    NA FALSE FALSE\n[25] FALSE FALSE FALSE    NA FALSE FALSE    NA FALSE FALSE  TRUE  TRUE  TRUE\n[37]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[61] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE    NA FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA\n[85]    NA    NA    NA\n\nwhich(homeworlds == \"Naboo\")\n\n [1]  3 20 34 35 36 37 38 41 59 60 65\n\nstarwars$name[which(homeworlds == \"Naboo\")]\n\n [1] \"R2-D2\"         \"Palpatine\"     \"Padmé Amidala\" \"Jar Jar Binks\"\n [5] \"Roos Tarpals\"  \"Rugor Nass\"    \"Ric Olié\"      \"Quarsh Panaka\"\n [9] \"Gregar Typho\"  \"Cordé\"         \"Dormé\"        \n\n\n\n\nB. Import the taylor package and explore the taylor_album_songs dataframe\n\n# Try loading in the taylor package and viewing the taylor_album_songs dataframe\n\n# install.packages(\"taylor\")\n\nlibrary(taylor)\ntaylor &lt;- taylor_album_songs\n\n# what is the \"class\" of the object taylor?\n\n\n\n# what is the \"class\" of the object taylor_album_songs?\n\n\n# what types of data are in the object taylor_album_songs?\n\n\n# change the \"key_mode\" from class \"character\" to class \"factor\"\n\n\n# How many albums are in the data set & how many songs on each album?\n\n\n\nC. Which numeric song features are correlated with one another? Hint create a correlation matrix.\n\n# look at the columns in taylor_album_songs and pick what features of the data you want to explore (hint choose all the numeric variables)\n\n# create a matrix of those features\n\n# pipe the matrix into GGally::ggpairs(), evaluate the correlation matrix\n\n# Which values show a positive correlation? Which values show a negative correlation?\n\n#var1 &lt;-\n#var2 &lt;-\n#var3 &lt;- \n#var4 &lt;- \n\n\n# Plot the correlations\n\n#ggplot(taylor_album_songs, aes(x=var1, y=var2)) + geom_point(size=3)\n\n#ggplot(taylor_album_songs, aes(x=var3, y=var4)) + geom_point(size=3)"
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html",
    "href": "activities/classwork03/classwork_L03.html",
    "title": "Data Wrangling with tidyverse",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover reshaping, filtering, and summarizing data using tidyverse principles."
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html#about-the-activity",
    "href": "activities/classwork03/classwork_L03.html#about-the-activity",
    "title": "Data Wrangling with tidyverse",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover reshaping, filtering, and summarizing data using tidyverse principles."
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html#load-the-tidyverse-package",
    "href": "activities/classwork03/classwork_L03.html#load-the-tidyverse-package",
    "title": "Data Wrangling with tidyverse",
    "section": "Load the Tidyverse Package",
    "text": "Load the Tidyverse Package\n\nlibrary(tidyverse)\nlibrary(cowplot)"
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html#which-month-had-the-most-and-least-passengers-in-the-airpassengers-data",
    "href": "activities/classwork03/classwork_L03.html#which-month-had-the-most-and-least-passengers-in-the-airpassengers-data",
    "title": "Data Wrangling with tidyverse",
    "section": "1. Which month had the most and least passengers in the AirPassengers data?",
    "text": "1. Which month had the most and least passengers in the AirPassengers data?\nThe AirPassengers data which is a time-series of data representing the monthly international airline passenger numbers from January 1949 to December 1960. Search for AirPassengers in the Help to learn more about the dataset.\n\n# Load and inspect the data, a little reshaping here to get in to an easy to read format for you.\nAP_matrix &lt;- matrix(AirPassengers, nrow = length(unique(floor(time(AirPassengers)))), byrow = TRUE)\ncolnames(AP_matrix) &lt;- month.abb\nrownames(AP_matrix) &lt;- unique(floor(time(AirPassengers)))\nAP_df &lt;- as.data.frame(AP_matrix)\nAP_df$Year &lt;- rownames(AP_matrix)\n\n\nA. Is the data long or wide? What form does it need to be in? How can you convert to the form you need?\n\nAP_df\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Year\n1949 112 118 132 129 121 135 148 148 136 119 104 118 1949\n1950 115 126 141 135 125 149 170 170 158 133 114 140 1950\n1951 145 150 178 163 172 178 199 199 184 162 146 166 1951\n1952 171 180 193 181 183 218 230 242 209 191 172 194 1952\n1953 196 196 236 235 229 243 264 272 237 211 180 201 1953\n1954 204 188 235 227 234 264 302 293 259 229 203 229 1954\n1955 242 233 267 269 270 315 364 347 312 274 237 278 1955\n1956 284 277 317 313 318 374 413 405 355 306 271 306 1956\n1957 315 301 356 348 355 422 465 467 404 347 305 336 1957\n1958 340 318 362 348 363 435 491 505 404 359 310 337 1958\n1959 360 342 406 396 420 472 548 559 463 407 362 405 1959\n1960 417 391 419 461 472 535 622 606 508 461 390 432 1960\n\nAP_long &lt;- AP_df |&gt; pivot_longer(cols = 1:12, names_to = \"months\", values_to = \"count\")\n\n\n\nB. How can we extract the the most and least traveled months each year?\n\nAP_long |&gt; \n  group_by(months) |&gt;\n  summarise(max_travel = max(count)) |&gt;\n  arrange(max_travel) |&gt;\n  ggplot(aes(x = months, y = max_travel)) + geom_col()"
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html#what-was-the-percent-increase-in-passengers-each-year-between-aug-and-nov",
    "href": "activities/classwork03/classwork_L03.html#what-was-the-percent-increase-in-passengers-each-year-between-aug-and-nov",
    "title": "Data Wrangling with tidyverse",
    "section": "2. What was the percent increase in passengers each year between Aug and Nov?",
    "text": "2. What was the percent increase in passengers each year between Aug and Nov?\n\n# To answer this question we need to find the ratio of Aug and Nov travelers. We need the data in the wide format.\n\n# how can we add the ratio to get the percent increase?\n\nAP_long\n\n# A tibble: 144 × 3\n   Year  months count\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 1949  Jan      112\n 2 1949  Feb      118\n 3 1949  Mar      132\n 4 1949  Apr      129\n 5 1949  May      121\n 6 1949  Jun      135\n 7 1949  Jul      148\n 8 1949  Aug      148\n 9 1949  Sep      136\n10 1949  Oct      119\n# ℹ 134 more rows\n\nAP_df |&gt; mutate(ratio = ((Aug/Nov)-1) *100) |&gt;\n  select(ratio)\n\n        ratio\n1949 42.30769\n1950 49.12281\n1951 36.30137\n1952 40.69767\n1953 51.11111\n1954 44.33498\n1955 46.41350\n1956 49.44649\n1957 53.11475\n1958 62.90323\n1959 54.41989\n1960 55.38462"
  },
  {
    "objectID": "activities/classwork03/classwork_L03.html#which-diet-lead-to-heavier-chicks",
    "href": "activities/classwork03/classwork_L03.html#which-diet-lead-to-heavier-chicks",
    "title": "Data Wrangling with tidyverse",
    "section": "3. Which diet lead to heavier chicks?",
    "text": "3. Which diet lead to heavier chicks?\nWe will use the ChickWeight data. Use the help to read more about the data.\n\n# First look at the data.\nglimpse(ChickWeight)\n\nRows: 578\nColumns: 4\n$ weight &lt;dbl&gt; 42, 51, 59, 64, 76, 93, 106, 125, 149, 171, 199, 205, 40, 49, 5…\n$ Time   &lt;dbl&gt; 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 0, 2, 4, 6, 8, 10, 1…\n$ Chick  &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\nA. Count how many timepoints were measured and how many chicks were on each Diet.\n\n# How can you count the timepoints, chicks, and diets, and chicks nested in diets?\n\nChickWeight |&gt; count(Diet)\n\n  Diet   n\n1    1 220\n2    2 120\n3    3 120\n4    4 118\n\nChickWeight |&gt; count(Time)\n\n   Time  n\n1     0 50\n2     2 50\n3     4 49\n4     6 49\n5     8 49\n6    10 49\n7    12 49\n8    14 48\n9    16 47\n10   18 47\n11   20 46\n12   21 45\n\nChickWeight |&gt; count(Chick)\n\n   Chick  n\n1     18  2\n2     16  7\n3     15  8\n4     13 12\n5      9 12\n6     20 12\n7     10 12\n8      8 11\n9     17 12\n10    19 12\n11     4 12\n12     6 12\n13    11 12\n14     3 12\n15     1 12\n16    12 12\n17     2 12\n18     5 12\n19    14 12\n20     7 12\n21    24 12\n22    30 12\n23    22 12\n24    23 12\n25    27 12\n26    28 12\n27    26 12\n28    25 12\n29    29 12\n30    21 12\n31    33 12\n32    37 12\n33    36 12\n34    31 12\n35    39 12\n36    38 12\n37    32 12\n38    40 12\n39    34 12\n40    35 12\n41    44 10\n42    45 12\n43    43 12\n44    41 12\n45    47 12\n46    49 12\n47    46 12\n48    50 12\n49    42 12\n50    48 12\n\nChickWeight |&gt; \n  filter(Time == \"0\") |&gt;\n  count(Diet) \n\n  Diet  n\n1    1 20\n2    2 10\n3    3 10\n4    4 10\n\nChickWeight |&gt;\n  group_by(Diet) |&gt;\n  summarise(no.chicks = n_distinct(Chick))\n\n# A tibble: 4 × 2\n  Diet  no.chicks\n  &lt;fct&gt;     &lt;int&gt;\n1 1            20\n2 2            10\n3 3            10\n4 4            10\n\ntable(ChickWeight$Time, ChickWeight$Diet)\n\n    \n      1  2  3  4\n  0  20 10 10 10\n  2  20 10 10 10\n  4  19 10 10 10\n  6  19 10 10 10\n  8  19 10 10 10\n  10 19 10 10 10\n  12 19 10 10 10\n  14 18 10 10 10\n  16 17 10 10 10\n  18 17 10 10 10\n  20 17 10 10  9\n  21 16 10 10  9\n\n\n\n\nB. Now figure out which diet leads to the heaviest chicks.\n\n# we can plot it to get a first view\nChickWeight |&gt;\n  ggplot(aes(x = Time, y = weight, group = Chick, color = Diet)) + \n  geom_line() +\n  theme_cowplot()\n\n\n\n\n\n\n\nChickWeight |&gt;\n  ggplot(aes(x = Time, y = weight, group = Chick, color = Diet)) + \n  geom_line() +\n  theme_cowplot() +\n  facet_wrap(~ Diet)"
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html",
    "href": "activities/classwork05/dimred_Penguins.html",
    "title": "Dimension Reduction using tidymodels",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover Principal Component Analysis (PCA) and UMAP using tidymodels."
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#about-the-activity",
    "href": "activities/classwork05/dimred_Penguins.html#about-the-activity",
    "title": "Dimension Reduction using tidymodels",
    "section": "",
    "text": "Access the Quarto document here.\nDownload the raw file.\nOpen it in RStudio.\n\nWe will work our way through this quarto document together during class. The activity will cover Principal Component Analysis (PCA) and UMAP using tidymodels."
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#load-the-packages",
    "href": "activities/classwork05/dimred_Penguins.html#load-the-packages",
    "title": "Dimension Reduction using tidymodels",
    "section": "Load the Packages",
    "text": "Load the Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#glimpse-at-the-data",
    "href": "activities/classwork05/dimred_Penguins.html#glimpse-at-the-data",
    "title": "Dimension Reduction using tidymodels",
    "section": "glimpse at the data",
    "text": "glimpse at the data\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#prep-the-data-and-perform-the-pca",
    "href": "activities/classwork05/dimred_Penguins.html#prep-the-data-and-perform-the-pca",
    "title": "Dimension Reduction using tidymodels",
    "section": "Prep the Data and Perform the PCA",
    "text": "Prep the Data and Perform the PCA\n\n# set a seed. Not really necessary for PCA because it is a reproducible analysis and doesn't use random data to model the data. But it is good practice. You can use any number your heart desires. \n\nset.seed(956)\n\n# don't need to split data for the PCA because its an exploratory data analysis and we want to use all the data. \n\n\n# Build a Recipe: this is an instruction manual does not perform any actual analysis, yet. \n\npenguin_recipe &lt;-\n  \n  # we want to use all the data to build the analysis so we us ~  and . to include all data numeric variables\n  recipe( ~ ., data = penguins) |&gt; \n  \n  # we want to maintain the categorical data so we give it a new role of id to identify the data\n  update_role(species, island, sex, new_role = \"id\") |&gt; \n  \n  # we want to omit rows with missing data because it can't be properly used by the PCA\n  step_naomit(all_predictors()) |&gt; \n  \n  # to make sure all the data is normally distributed and centered we want to normalize the data before running PCA\n  step_normalize(all_predictors()) |&gt;\n  \n  # finally we want to actually run a PCA analysis on all the data or predictors\n  step_pca(all_predictors(), id = \"pca\")\n\npenguin_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 5\nid:        3\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n# Prep Data: pull in the variables from the penguins data and runs PCA\n\npenguin_prep &lt;- prep(penguin_recipe, penguins)\n\npenguin_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 5\nid:        3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 344 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: bill_length_mm, ... | Trained\n\n\n• Centering and scaling for: bill_length_mm bill_depth_mm, ... | Trained\n\n\n• PCA extraction with: bill_length_mm bill_depth_mm, ... | Trained\n\npenguin_prep |&gt; tidy(id = \"pca\", type = \"coef\")\n\n# A tibble: 25 × 4\n   terms               value component id   \n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n 1 bill_length_mm     0.452  PC1       pca  \n 2 bill_depth_mm     -0.398  PC1       pca  \n 3 flipper_length_mm  0.576  PC1       pca  \n 4 body_mass_g        0.544  PC1       pca  \n 5 year               0.0957 PC1       pca  \n 6 bill_length_mm    -0.0935 PC2       pca  \n 7 bill_depth_mm      0.0164 PC2       pca  \n 8 flipper_length_mm  0.0256 PC2       pca  \n 9 body_mass_g       -0.111  PC2       pca  \n10 year               0.989  PC2       pca  \n# ℹ 15 more rows\n\npenguin_prep |&gt; tidy(id = \"pca\", type = \"variance\")\n\n# A tibble: 20 × 4\n   terms                          value component id   \n   &lt;chr&gt;                          &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;\n 1 variance                      2.77           1 pca  \n 2 variance                      0.993          2 pca  \n 3 variance                      0.772          3 pca  \n 4 variance                      0.365          4 pca  \n 5 variance                      0.0993         5 pca  \n 6 cumulative variance           2.77           1 pca  \n 7 cumulative variance           3.76           2 pca  \n 8 cumulative variance           4.54           3 pca  \n 9 cumulative variance           4.90           4 pca  \n10 cumulative variance           5.00           5 pca  \n11 percent variance             55.4            1 pca  \n12 percent variance             19.9            2 pca  \n13 percent variance             15.4            3 pca  \n14 percent variance              7.30           4 pca  \n15 percent variance              1.99           5 pca  \n16 cumulative percent variance  55.4            1 pca  \n17 cumulative percent variance  75.3            2 pca  \n18 cumulative percent variance  90.7            3 pca  \n19 cumulative percent variance  98.0            4 pca  \n20 cumulative percent variance 100              5 pca  \n\n# Bake Data: Extract the data from the PCA\n\npenguin_bake &lt;- bake(penguin_prep, new_data = NULL)\n\npenguin_bake\n\n# A tibble: 342 × 8\n   species island    sex       PC1    PC2     PC3     PC4     PC5\n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Adelie  Torgersen male   -1.95  -1.12   0.0126  0.241   0.405 \n 2 Adelie  Torgersen female -1.42  -1.14   0.489   0.0379  0.280 \n 3 Adelie  Torgersen female -1.48  -1.06   0.209  -0.192  -0.643 \n 4 Adelie  Torgersen female -1.99  -1.01   0.0549  0.624  -0.591 \n 5 Adelie  Torgersen male   -2.02  -1.08  -0.772   0.692  -0.321 \n 6 Adelie  Torgersen female -1.87  -1.11   0.411  -0.0192  0.385 \n 7 Adelie  Torgersen male   -0.927 -1.22  -0.458   1.34    0.213 \n 8 Adelie  Torgersen &lt;NA&gt;   -1.94  -0.983  0.821   0.695  -0.543 \n 9 Adelie  Torgersen &lt;NA&gt;   -1.31  -1.21  -0.962   0.737   0.212 \n10 Adelie  Torgersen &lt;NA&gt;   -1.84  -1.04   0.844  -0.198  -0.0969\n# ℹ 332 more rows"
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#evaluate-the-pca",
    "href": "activities/classwork05/dimred_Penguins.html#evaluate-the-pca",
    "title": "Dimension Reduction using tidymodels",
    "section": "Evaluate the PCA",
    "text": "Evaluate the PCA\n\n# tidy type can be: \"coef\", \"variance\", \n\n# Plot the percent of variance explained by each principal component\npenguin_prep |&gt; \n  tidy(id = \"pca\", type = \"variance\") |&gt; \n  dplyr::filter(terms == \"percent variance\") |&gt; \n  ggplot(aes(x = component, y = value)) + \n  geom_col(fill = \"#b6dfe2\") + \n  xlim(c(0, 5)) + \n  ylab(\"% of total variance\") +\n  geom_text(aes(label=round(value,1)), vjust = 1.5)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\n# Plot which predictors/variables are explained in each principal component\npenguin_prep |&gt;\n  tidy(id = \"pca\", type = \"coef\") |&gt; \n  ggplot(aes(abs(value), terms, fill = value &gt; 0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"#b6dfe2\", \"#0A537D\")) +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Pos Affil\") \n\n\n\n\n\n\n\n\n\n# Plot the PCA\npca_plot &lt;- penguin_bake |&gt; \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = species, shape = species), \n             alpha = 0.8, \n             size = 2) +\n  scale_colour_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_bw()\n\npca_plot\n\n\n\n\n\n\n\n# Add the vectors for how the variables are explained by each PC\n\npca_wider &lt;- penguin_prep |&gt; \n              tidy(id = \"pca\", type = \"coef\") |&gt;\n              pivot_wider(names_from = component, id_cols = terms)\n\narrow_style &lt;- arrow(length = unit(.05, \"inches\"), type = \"closed\")\n\npca_plot +\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC2), \n               x = 0, \n               y = 0, \n               arrow = arrow_style) + \n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC2, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5, \n            color = '#0A537D') \n\n\n\n\n\n\n\n\n\nWe can see that year is having a large impact of PC2 and obscuring the effect of the other metrics. So we can exclude PC2 and plot PC1 and PC3 instead.\n\npca_plot &lt;- penguin_bake |&gt; \n  ggplot(aes(PC1, PC3)) +\n  geom_point(aes(color = species, shape = species), \n             alpha = 0.8, \n             size = 2) +\n  scale_colour_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_bw()\n\npca_plot\n\n\n\n\n\n\n\narrow_style &lt;- arrow(length = unit(.05, \"inches\"), type = \"closed\")\n\npca_plot +\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC3), \n               x = 0, \n               y = 0, \n               arrow = arrow_style) + \n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC3, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5, \n            color = '#0A537D')"
  },
  {
    "objectID": "activities/classwork05/dimred_Penguins.html#prep-the-data-and-perform-the-umap-instead",
    "href": "activities/classwork05/dimred_Penguins.html#prep-the-data-and-perform-the-umap-instead",
    "title": "Dimension Reduction using tidymodels",
    "section": "Prep the Data and Perform the UMAP instead",
    "text": "Prep the Data and Perform the UMAP instead\n\nlibrary(embed)\nset.seed(956)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\npenguin_recipe &lt;-\n  recipe(species ~ ., data = penguins) |&gt; \n  update_role(species, island, sex, new_role = \"id\") |&gt; \n  step_rm(year) |&gt;\n  step_naomit(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt;\n  step_umap(all_numeric_predictors(), num_comp = 3)\n\npenguin_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 5\nid:        3\n\n\n\n\n\n── Operations \n\n\n• Variables removed: year\n\n\n• Removing rows with NA values in: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• UMAP embedding for: all_numeric_predictors()\n\n# Prep Data: pull in the variables from the penguins data and runs PCA\n\npenguin_prep &lt;- prep(penguin_recipe, penguins)\n\npenguin_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 5\nid:        3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 344 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variables removed: year | Trained\n\n\n• Removing rows with NA values in: bill_length_mm, ... | Trained\n\n\n• Centering and scaling for: bill_length_mm bill_depth_mm, ... | Trained\n\n\n• UMAP embedding for: bill_length_mm bill_depth_mm, ... | Trained\n\n# Bake Data: Extract the data from the UMAP\n\npenguin_bake &lt;- bake(penguin_prep, new_data = NULL)\n\npenguin_bake\n\n# A tibble: 342 × 6\n   island    sex    species UMAP1  UMAP2   UMAP3\n   &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Torgersen male   Adelie  -6.27 -0.582  0.339 \n 2 Torgersen female Adelie  -7.13 -0.441  0.824 \n 3 Torgersen female Adelie  -6.97 -0.581  1.80  \n 4 Torgersen female Adelie  -5.27 -0.684  1.12  \n 5 Torgersen male   Adelie  -4.33 -1.15   0.482 \n 6 Torgersen female Adelie  -6.92 -0.193  0.734 \n 7 Torgersen male   Adelie  -4.78 -1.95  -0.323 \n 8 Torgersen &lt;NA&gt;   Adelie  -6.21  0.140  1.90  \n 9 Torgersen &lt;NA&gt;   Adelie  -4.21 -1.78   0.0578\n10 Torgersen &lt;NA&gt;   Adelie  -7.47  0.300  1.52  \n# ℹ 332 more rows\n\n\n\npenguin_bake |&gt; ggplot(aes(x = UMAP1, y = UMAP2, color = species)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\npenguin_bake |&gt; ggplot(aes(x = UMAP1, y = UMAP3, color = species)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\npenguin_bake |&gt; ggplot(aes(x = UMAP2, y = UMAP3, color = species)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n# lets go back and try it without year and see how it looks? add in a step to the recipe: step_rm(year)\n\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] embed_1.1.5          palmerpenguins_0.1.1 yardstick_1.3.2     \n [4] workflowsets_1.1.1   workflows_1.2.0      tune_1.3.0          \n [7] rsample_1.3.0        recipes_1.3.1        parsnip_1.3.2       \n[10] modeldata_1.4.0      infer_1.0.8          dials_1.4.0         \n[13] scales_1.3.0         broom_1.0.7          tidymodels_1.3.0    \n[16] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n[19] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n[22] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.2       \n[25] tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    timeDate_4041.110   farver_2.1.2       \n [4] fastmap_1.2.0       digest_0.6.37       rpart_4.1.24       \n [7] timechange_0.3.0    lifecycle_1.0.4     survival_3.8-3     \n[10] magrittr_2.0.3      compiler_4.4.2      rlang_1.1.5        \n[13] tools_4.4.2         utf8_1.2.4          yaml_2.3.10        \n[16] data.table_1.17.0   knitr_1.50          labeling_0.4.3     \n[19] DiceDesign_1.10     withr_3.0.2         nnet_7.3-20        \n[22] grid_4.4.2          colorspace_2.1-1    future_1.58.0      \n[25] globals_0.18.0      iterators_1.0.14    MASS_7.3-65        \n[28] cli_3.6.4           rmarkdown_2.29      generics_0.1.3     \n[31] rstudioapi_0.17.1   future.apply_1.20.0 tzdb_0.5.0         \n[34] splines_4.4.2       parallel_4.4.2      vctrs_0.6.5        \n[37] hardhat_1.4.1       Matrix_1.7-3        jsonlite_1.9.1     \n[40] hms_1.1.3           listenv_0.9.1       foreach_1.5.2      \n[43] gower_1.0.2         glue_1.8.0          parallelly_1.45.0  \n[46] codetools_0.2-20    uwot_0.2.3          RcppAnnoy_0.0.22   \n[49] stringi_1.8.4       gtable_0.3.6        munsell_0.5.1      \n[52] GPfit_1.0-9         pillar_1.10.1       furrr_0.3.1        \n[55] htmltools_0.5.8.1   ipred_0.9-15        lava_1.8.1         \n[58] R6_2.6.1            lhs_1.2.0           evaluate_1.0.3     \n[61] lattice_0.22-6      backports_1.5.0     class_7.3-23       \n[64] Rcpp_1.0.14         prodlim_2025.04.28  xfun_0.51          \n[67] pkgconfig_2.0.3"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Course Content",
    "section": "",
    "text": "Course content for each lecture is below:\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-07-01\n\n\nWelcome to the Course!\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-03\n\n\nNavigating Data Types in R\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-08\n\n\nData Wrangling with tidyverse\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-10\n\n\nData Visualization with ggplot2\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-14\n\n\nDimension Reduction with tidymodels\n\n\nLindsay Hayes\n\n\n\n\n\n\n2025-07-16\n\n\nMachine Learning with tidymodels\n\n\nLindsay Hayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/index_L02.html",
    "href": "lectures/index_L02.html",
    "title": "Navigating Data Types in R",
    "section": "",
    "text": "Before class, read the pre-reading materials."
  },
  {
    "objectID": "lectures/index_L02.html#learning-objectives",
    "href": "lectures/index_L02.html#learning-objectives",
    "title": "Navigating Data Types in R",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nGenerate and navigate a quarto document\nUnderstand what packages are and how to use them\nGet data in and out of R\nRecognize and create various data types in R"
  },
  {
    "objectID": "lectures/index_L02.html#slides",
    "href": "lectures/index_L02.html#slides",
    "title": "Navigating Data Types in R",
    "section": "Slides",
    "text": "Slides\n\nLecture 02: Data Types\nIn-Class Activity"
  },
  {
    "objectID": "lectures/index_L04.html",
    "href": "lectures/index_L04.html",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "Before class, read chapter 4 titled “Visualizing Data in the Tidyverse” from the Tidyverse Skills for Data Science Book by Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks, and Roger D. Peng.\nReading: Visualizing Data in the Tidyverse\nFor more examples, you can checkout a YouTube video series on ggplot from Dr. Lace Padilla.\n\nMastering ggplot2 in R [17 min]\nMastering Axes in ggplot2 [16 min]\nColor Magic in ggplot2 [12.5 min]\nMastering Facet_Wrap in ggplot2 [15 min]"
  },
  {
    "objectID": "lectures/index_L04.html#learning-objectives",
    "href": "lectures/index_L04.html#learning-objectives",
    "title": "Data Visualization with ggplot2",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nDistinguish between good and bad plots\nMatch the type of plots with the type of data input\nList the features of the grammer of graphics\nBuild complex graphics in ggplot2"
  },
  {
    "objectID": "lectures/index_L04.html#slides",
    "href": "lectures/index_L04.html#slides",
    "title": "Data Visualization with ggplot2",
    "section": "Slides",
    "text": "Slides\n\nLecture 04: Data Visualization in ggplot\nIn-Class Activity"
  },
  {
    "objectID": "lectures/index_L06.html",
    "href": "lectures/index_L06.html",
    "title": "Machine Learning with tidymodels",
    "section": "",
    "text": "Before class, read the pre-reading materials."
  },
  {
    "objectID": "lectures/index_L06.html#learning-objectives",
    "href": "lectures/index_L06.html#learning-objectives",
    "title": "Machine Learning with tidymodels",
    "section": "Learning objectives",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of class you will be able to:\n\nUnderstand the purpose of data modeling, specifically machine learning classification.\nIdentify the packages and tools used to implement machine learning algorithms for data analysis."
  },
  {
    "objectID": "lectures/index_L06.html#slides",
    "href": "lectures/index_L06.html#slides",
    "title": "Machine Learning with tidymodels",
    "section": "Slides",
    "text": "Slides\n\nLecture 06: Predictive Modeling with tidymodels\nIn-Class Activity"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Assignments and Project",
    "section": "",
    "text": "Assignments:\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-07-03\n\n\nAssignment 1\n\n\nEleana Cabello\n\n\n\n\n\n\n2025-07-10\n\n\nAssignment 2\n\n\nEleana Cabello\n\n\n\n\n\n\n2025-07-18\n\n\nAssignment 3\n\n\nEleana Cabello\n\n\n\n\n\n\n2025-07-18\n\n\nFinal Project\n\n\nEleana Cabello\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index_A02.html",
    "href": "projects/index_A02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Overview\nDue date: 07/15/25 at 11:59pm\nThis week we introduced the Tidyverse collection of R packages. The packages covered are commonly used in biomedical research data analysis. These packages are well-documented online by their creators and user communities. If you have a question about any of the packages, googling will likely give you a solution from a forum of someone else having the same question or issue.\nIn this week’s Swirl lessons, you’ll explore one of the core Tidyverse packages: ggplot2. This powerful package makes it easy to transform tidydata into informative visualizations while allowing easy plot management and customization.\nThrough the lessons for this week, you will learn the following:\n\nUsing base methods to produce plots\nCustomizing plots\nThe basics of GGPlot2 and a little more\n\n\n\nSwirl Course Setup\n\nIn RStudio, load in the swirl package\nlibrary(\"swirl\")\nNavigate to this link. Download the Swirl lesson file 2_Exploratory_Data_Analysis.swc. Move it into your working directory for the class.\nOnce you have the file in your working directory, in the console of RStudio type the command below and after the swc_path = type \" and then hit tab to interactively choose the 2_Exploratory_Data_Analysis.swc file you just downloaded from your working directory. The command will look something like install_course(swc_path = \"2_Exploratory_Data_Analysis.swc\"). Run the command to install the course.\ninstall_course(swc_path = )\nStart Swirl by typing the following command\nswirl()\n\n\n\nWorking through the Lessons\n\nPlease enter your full name together when asked what you want to be called by e.g. JaneDoe\nComplete all lessons\n\n\n1: Principles of Analytic Graphs\n2: Exploratory Graphs\n3: Graphics Devices in R\n4: Plotting Systems\n5: Base Plotting System\n6: Lattice Plotting System\n7: Working with Colors\n8: GGPlot2 Part1\n9: GGPlot2 Part2\n10: GGPlot2 Extras\n\n\n\n\n\n\n\nImportant\n\n\n\n\nYou can take breaks between lessons (like shutting down Swirl and exiting RStudio), just use the same name when logging in for documenting purposes on the instructor’s end.\nIf you start a lesson, please complete it fully before taking a break. If you stop RStudio in the middle of a lesson, your progress for that lesson may still be saved in the history for that package locally, but will not be saved or sent to us on the instructor’s end.\nIf you do not get to the Google Form end of any lesson, please email one of the instructors as soon as possible.\n\n\n\n\n\nSubmission\n\n\n\n\n\n\nNote\n\n\n\n\nFYI: You need a google account to submit the assignment.\nSwirl will navigate you to the submission Google Form of each lesson. However, review the instructions below before submitting to make sure you are able to submit correctly.\nWhen submitting please make sure the box is not empty, if it is please email one of the instructors.\n\n\n\n\nAt the end of each lesson the following question will appear, enter the selection Yes to be redirected to your autofilled Google Form submission\n\nOnce you’ve been redirected, your Swirl activity will be autofilled into the submission box, just hit the Submit button\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf you do not get to the Google Form at the end of any lesson, please email one of the instructors as soon as possible."
  },
  {
    "objectID": "projects/index_P01.html",
    "href": "projects/index_P01.html",
    "title": "Final Project",
    "section": "",
    "text": "Background\nThroughout this summer course, you have gained hands-on experience programming in R, performing initial exploratory data analysis, and applying commonly used unsupervised learning methods to better understand complex datasets.\nThe goal of the final project is to give you an opportunity to put these skills into practice by working with real-world data that is relevant to your own research or interests. To support this, we are giving you the freedom to choose your own dataset and the methods you find most applicable.\n\n\nProject\n\nSelect a dataset you want to work with, either your own data or a publicly available data set.\nIdentify a question, hypothesis, or problem to solve with the data.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nSubmit a brief paragraph of your project proposal to Canvas by July 21, 2024.\nPresent your project proposal in 2-3 sentences to the class on July 22, 2024.\n\n\n\n\nMake a literate coding quarto document that combines the question, your approaches to solve the problem or answer the question, informative graphics of the dataset that demonstrate your findings, and your conclusions. Knit to an html and present your report in class.\n\nBelow are key dates and deadlines:\n\nTimeline and Deliverables\n\n\n\n07/22/25\nProject Proposal\n**DUE**: Brief paragraph on the data set you will be using for the project including goals of the analysis\n\n\n07/22/25\nProject Work Day\nGet help from instructors on any tasks you may be stuck on\n\n\n07/24/25 07/28/25\nPresentations\n5-10 min limit\n\n\n\n\n\n\nSubmission\nProject submission will be done through your Github website that you will create. Have your project file (.qmd file) in your website directory. However you would like to accomplish this: copying file into the directory, creating a new file and copying text over into the new file, or creating new qmd file in the directory and redoing your project). Whichever method is used what matters is you push the new file changes (your new project file) to your Github repository, where these changes will be changed to HTML files and displayed on your website. Afterwards, submit a link to your project page on your website to Canvas. Alternatively, you can submit the zipped qmd, folder, and html output to Canvas.\n\n\nNeed More Data?\nThere are a few data sets already installed in R and that come with the packages we used in this course. To view them run data() in the console of RStudio. Give them a look and if your are interested there is more information on them online.\nAnother option is online repositories that store datasets like Kaggle and UC Irvine’s Machine Learning Repository. Below are a few that might be interesting to look further into:\n\nCAIR-CVD-2025: Cardiovascular Risk from Bangladesh\nThyroid Disease Data from the Garavan Institute in Sydney, Australia\nMaternal Health Risks from Bangladesh\nFormula One Race Results 2025\n\n\n\nRubric\n\n\n\n\nDid the final project have a clearly stated analysis goal?\nDid the final project successfully achieve that goal?\n\n\n15pts (Good)\nStudent understood the assignment, picked a task that could be completed by data science, and clearly communicated the problem and goals.\nThe student was able to write a script to address their problem and present the problem, logic and organization of the script, and achieved the stated goals.\n\n\n9pts (Fair)\nThe student attempted the assignment, identified a problem, and was able to partially communicate the goals of the project.\nThe student was able to communicate the problem, attempted a script to achieve the goal, but was only partially able to achieve the stated goals.\n\n\n3pt (Poor)\nThe student did not understand the assignment, was not able to communicate a problem or identify achievable goals.\nThe student poorly communicated the problem, did not find a logical solution, and was not able to achieve the stated goals."
  },
  {
    "objectID": "readings/index_R01.html",
    "href": "readings/index_R01.html",
    "title": "R & RStudio",
    "section": "",
    "text": "What is R and RStudio and why is it important to learn and implement in all your data analysis pipelines from simple to complex? The reading below and the the materials covered during this course will answer this very question.\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nDescribe the purpose of the RStudio Script, Console, Environment, and Plots panes.\nOrganize files and directories for a set of analyses as an R project.\nUse the built-in RStudio help interface to search for more information on R functions.\n\n\n\n\n\nThe term R is used to refer to the programming language, the environment for statistical computing, and the software that interprets the scripts written using it.\nRStudio is currently a very popular way to not only write your R scripts but also to interact with the R software1. To function correctly, RStudio needs R and therefore both need to be installed on your computer.\nThe RStudio IDE Cheat Sheet provides much more information than will be covered here, but can be useful to learn keyboard shortcuts and discover new features.\n\n\n\nWhy you need R\n\n\n\n\nThe learning curve might be steeper than with other software, but with R, the results of your analysis do not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results; you just have to run your script again.\nWorking with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes.\nWorking with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use.\n\n\n\nReproducibility means that someone else (including your future self) can obtain the same results from the same dataset when using the same analysis code.\nR integrates with other tools to generate manuscripts or reports from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript or report are updated automatically.\nAn increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements.\n\n\n\nWith 10,000+ packages that can be installed to extend R’s capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyse your data.\n\n\n\nThe skills you learn with R scale easily with the size of your dataset. Whether your dataset has tens, hundreds, or millions of lines, it won’t make much difference to you as the programmer.\nR is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient. We will cover this more in the section on data structures.\nR can connect to spreadsheets, databases, and many other data formats, on your computer or on the web.\n\n\n\nThe plotting functions in R are extensive, and allow you to adjust any aspect of your graph to convey most effectively the message from your data.\n\n\n\nThousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community. These broad user communities extend to specialized areas such as bioinformatics. One such subset of the R community is Bioconductor, a scientific project for analysis of data from current and emerging biological assays.\n\n\n\nAnyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs.\n\n\n\n\nWe will use the RStudio IDE to write code, navigate the files on our computer, inspect the variables we are going to create, and visualize the plots we will generate.\n\n\n\nA schematic representing the layout of RStudio\n\n\nThe RStudio window is divided into 4 “Panes”:\n\nSource code for your scripts and documents (top-left)\nEnvironment/History (top-right)\nFiles/Plots/Packages/Help/Viewer (bottom-right)\nR Console, or outputs (bottom-left).\n\nThe placement of these panes and their content can be customized (Tools &gt; Global Options &gt; Pane Layout).\nAdvantages of using RStudio is that all the information you need to write code is available in a single window. Additionally, with many shortcuts, autocompletion, and highlighting RStudio makes typing easier and less error-prone. This very text was written in RStudio!\n\n\n\n\n\nThe scientific process is naturally incremental and many projects start life as random notes, some data, some code, then a manuscript, and eventually everything is a bit mixed together. Properly managing the progression of files as a project matures can be challenging. Good data management at the start will save you an immeasurable amount of time at the end.\nMost people tend to organize their projects like this:\n There are many reasons why we should ALWAYS avoid this:\n\nIt is hard to tell which version of the data is the original and which is the modified.\nIt is messy because it mixes files with various extensions together.\nIt takes a lot of time to actually find things, and relate the correct figures to the exact code used to generate it.\n\nA good project layout will ultimately make your life easier!!\n\nIt ensure the integrity of your data.\nIt is simpler to share your code with someone else (a lab-mate, collaborator, supervisor, and most importantly the future-you!).\nIt allows you to easily upload your data and code with your manuscript submission.\nIt makes it easier to pick the project back up after a break.\n\n\n\n\n\nOne day, you will need to quit R, go do something else, and return to your analysis later. Creating an RStudio project will keep your data analysis environment organized and accessible. Creating a RStudio project file will:\n\n\nCreate a .Rproj file.\nOpens a RStudio session and sets the directory as the working directory.\nLoads any .RData files to restore previously imported data.\nLoads an open scripts from the previous session.\nLoads a .Rhistory file to save the history of previous commands used.\nRestores any RStudio settings.\n\n\n\n\n\n\n\nNote\n\n\n\nWith Rstudio projects you don’t have to keep your session running all the time. You can easily pick up where you left off.\n\n\nRead about how to create a RStudio project\n\nR Project Directory Structure (for data science and for life)\n\nHere are examples of project data management.\n\n\n\nFile Structure for Data Analysis\n\n\n\n\n\nFile Structure for Experiments\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdvice on File Naming\n\nAvoid spaces in a name (ie: project 1.Rproj)\nDo use dashes (-), underscores (_), or periods (filename_project1.qmd)\nAvoid quotes and special characters (“, $, #)\nUse dates in year-month-day format for better file sorting (250701_plot.png)\nUse a numbering system to order steps in a pipeline (1.first.qmd, 2.second.qmd, 3.third.qmd)\n📢 ANNOTATE, LOG, and NOTEKEEP\n\n\n\n\n\n\n\nFinally, there are many ways to get help in the R community. The first choice is always to click on the Help tab in the Files pane. You can enter a R function and it will pull up the manual for that command along with the package its a part of. Alternatively you can type ? before function name in the Console. For example, ?mean will pull up the help page for the mean function.\nSecond you can reach out to many online communities. Most of your questions have probably already been asked so a quick google search will yield many results. Here a few of those resources:\n\nStack Overflow: https://stackoverflow.com/questions\n\nbe sure to read the instructions on how to ask a good question\n\nPosit Community: https://forum.posit.co/t/rstudio-community-find-help-guide/86831\nBioconductor: https://bioconductor.org/help/\nStats specific (not R specific): Stack Exchange: https://stats.stackexchange.com\nR documentation https://www.rdocumentation.org\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\nContent pulled from\n\nswcarpentry: R https://carpentries-incubator.github.io/bioc-intro/20-r-rstudio.html#why-learn-r\nswcarpentry: Project Management https://swcarpentry.github.io/r-novice-gapminder/instructor/02-project-intro.html"
  },
  {
    "objectID": "readings/index_R01.html#what-is-r-why-learn-it",
    "href": "readings/index_R01.html#what-is-r-why-learn-it",
    "title": "R & RStudio",
    "section": "",
    "text": "The term R is used to refer to the programming language, the environment for statistical computing, and the software that interprets the scripts written using it.\nRStudio is currently a very popular way to not only write your R scripts but also to interact with the R software1. To function correctly, RStudio needs R and therefore both need to be installed on your computer.\nThe RStudio IDE Cheat Sheet provides much more information than will be covered here, but can be useful to learn keyboard shortcuts and discover new features.\n\n\n\nWhy you need R\n\n\n\n\nThe learning curve might be steeper than with other software, but with R, the results of your analysis do not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results; you just have to run your script again.\nWorking with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes.\nWorking with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use.\n\n\n\nReproducibility means that someone else (including your future self) can obtain the same results from the same dataset when using the same analysis code.\nR integrates with other tools to generate manuscripts or reports from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript or report are updated automatically.\nAn increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements.\n\n\n\nWith 10,000+ packages that can be installed to extend R’s capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyse your data.\n\n\n\nThe skills you learn with R scale easily with the size of your dataset. Whether your dataset has tens, hundreds, or millions of lines, it won’t make much difference to you as the programmer.\nR is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient. We will cover this more in the section on data structures.\nR can connect to spreadsheets, databases, and many other data formats, on your computer or on the web.\n\n\n\nThe plotting functions in R are extensive, and allow you to adjust any aspect of your graph to convey most effectively the message from your data.\n\n\n\nThousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community. These broad user communities extend to specialized areas such as bioinformatics. One such subset of the R community is Bioconductor, a scientific project for analysis of data from current and emerging biological assays.\n\n\n\nAnyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs."
  },
  {
    "objectID": "readings/index_R01.html#navigating-r-in-rstudio",
    "href": "readings/index_R01.html#navigating-r-in-rstudio",
    "title": "R & RStudio",
    "section": "",
    "text": "We will use the RStudio IDE to write code, navigate the files on our computer, inspect the variables we are going to create, and visualize the plots we will generate.\n\n\n\nA schematic representing the layout of RStudio\n\n\nThe RStudio window is divided into 4 “Panes”:\n\nSource code for your scripts and documents (top-left)\nEnvironment/History (top-right)\nFiles/Plots/Packages/Help/Viewer (bottom-right)\nR Console, or outputs (bottom-left).\n\nThe placement of these panes and their content can be customized (Tools &gt; Global Options &gt; Pane Layout).\nAdvantages of using RStudio is that all the information you need to write code is available in a single window. Additionally, with many shortcuts, autocompletion, and highlighting RStudio makes typing easier and less error-prone. This very text was written in RStudio!"
  },
  {
    "objectID": "readings/index_R01.html#managing-projects-in-rstudio",
    "href": "readings/index_R01.html#managing-projects-in-rstudio",
    "title": "R & RStudio",
    "section": "",
    "text": "The scientific process is naturally incremental and many projects start life as random notes, some data, some code, then a manuscript, and eventually everything is a bit mixed together. Properly managing the progression of files as a project matures can be challenging. Good data management at the start will save you an immeasurable amount of time at the end.\nMost people tend to organize their projects like this:\n There are many reasons why we should ALWAYS avoid this:\n\nIt is hard to tell which version of the data is the original and which is the modified.\nIt is messy because it mixes files with various extensions together.\nIt takes a lot of time to actually find things, and relate the correct figures to the exact code used to generate it.\n\nA good project layout will ultimately make your life easier!!\n\nIt ensure the integrity of your data.\nIt is simpler to share your code with someone else (a lab-mate, collaborator, supervisor, and most importantly the future-you!).\nIt allows you to easily upload your data and code with your manuscript submission.\nIt makes it easier to pick the project back up after a break.\n\n\n\n\n\nOne day, you will need to quit R, go do something else, and return to your analysis later. Creating an RStudio project will keep your data analysis environment organized and accessible. Creating a RStudio project file will:\n\n\nCreate a .Rproj file.\nOpens a RStudio session and sets the directory as the working directory.\nLoads any .RData files to restore previously imported data.\nLoads an open scripts from the previous session.\nLoads a .Rhistory file to save the history of previous commands used.\nRestores any RStudio settings.\n\n\n\n\n\n\n\nNote\n\n\n\nWith Rstudio projects you don’t have to keep your session running all the time. You can easily pick up where you left off.\n\n\nRead about how to create a RStudio project\n\nR Project Directory Structure (for data science and for life)\n\nHere are examples of project data management.\n\n\n\nFile Structure for Data Analysis\n\n\n\n\n\nFile Structure for Experiments\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdvice on File Naming\n\nAvoid spaces in a name (ie: project 1.Rproj)\nDo use dashes (-), underscores (_), or periods (filename_project1.qmd)\nAvoid quotes and special characters (“, $, #)\nUse dates in year-month-day format for better file sorting (250701_plot.png)\nUse a numbering system to order steps in a pipeline (1.first.qmd, 2.second.qmd, 3.third.qmd)\n📢 ANNOTATE, LOG, and NOTEKEEP"
  },
  {
    "objectID": "readings/index_R01.html#getting-help",
    "href": "readings/index_R01.html#getting-help",
    "title": "R & RStudio",
    "section": "",
    "text": "Finally, there are many ways to get help in the R community. The first choice is always to click on the Help tab in the Files pane. You can enter a R function and it will pull up the manual for that command along with the package its a part of. Alternatively you can type ? before function name in the Console. For example, ?mean will pull up the help page for the mean function.\nSecond you can reach out to many online communities. Most of your questions have probably already been asked so a quick google search will yield many results. Here a few of those resources:\n\nStack Overflow: https://stackoverflow.com/questions\n\nbe sure to read the instructions on how to ask a good question\n\nPosit Community: https://forum.posit.co/t/rstudio-community-find-help-guide/86831\nBioconductor: https://bioconductor.org/help/\nStats specific (not R specific): Stack Exchange: https://stats.stackexchange.com\nR documentation https://www.rdocumentation.org"
  },
  {
    "objectID": "readings/index_R01.html#references",
    "href": "readings/index_R01.html#references",
    "title": "R & RStudio",
    "section": "",
    "text": "References\n\n\n\nContent pulled from\n\nswcarpentry: R https://carpentries-incubator.github.io/bioc-intro/20-r-rstudio.html#why-learn-r\nswcarpentry: Project Management https://swcarpentry.github.io/r-novice-gapminder/instructor/02-project-intro.html"
  },
  {
    "objectID": "readings/index_R01.html#footnotes",
    "href": "readings/index_R01.html#footnotes",
    "title": "R & RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs opposed to using R directly from the command line console.↩︎"
  },
  {
    "objectID": "readings/index_R03.html",
    "href": "readings/index_R03.html",
    "title": "Data Wrangling with tidyverse",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will be able to:\n\nRecognize and define an untidy dataset\nRecognize and convert between long and wide data formats\nUnderstand when to use both formats\nPipe functions together to perform multiple commands in sequence"
  },
  {
    "objectID": "readings/index_R03.html#the-problem-with-messy-data",
    "href": "readings/index_R03.html#the-problem-with-messy-data",
    "title": "Data Wrangling with tidyverse",
    "section": "The problem with messy data",
    "text": "The problem with messy data\nMost scientists are not trained to collect and organize data using tidydata principles. The results are data sets which are hard to follow, difficult to repeat, and challenging to report to journals. Understanding how to organize and store your data and analyses will make your life easier! Moreover, many journals request raw data submissions along with your manuscript. If you’ve analyzed and organized all your data in R using tidydata principles this process becomes a super simple, rather than tracking all the data across messy spreadsheets.\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham"
  },
  {
    "objectID": "readings/index_R03.html#the-principles-of-tidydata",
    "href": "readings/index_R03.html#the-principles-of-tidydata",
    "title": "Data Wrangling with tidyverse",
    "section": "The principles of tidydata",
    "text": "The principles of tidydata\n\nTreat your raw data as immutable.\n\nTry to manipulate the original form of the data as little as possible. The reason is to make the whole data analysis pipeline reproducible and transparent. If you start changing fields on the data spreadsheet you can’t track where those values came from. Manipulating data with code also decreases the chances for errors being introduced to the raw data.\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFor example, take the following data\n\n\n      groups val\n1   Con_male 134\n2 CON_female 106\n3      Exp_m  84\n4      Exp_F 105\n\n\nWhy might this data be a problem?\n\ncombining 2 observations into one field (group and sex).\nchanging the notation styles\n\nHere’s another example\n\n\n  groups      val2\n1    CON  78_ug/ul\n2    CON  67_ug/ul\n3    EXP  69_ug/ul\n4    EXP 115_ug/ul\n\n\nWhy might this data be a problem?\n\nthe data field contains 2 values the numeric value and the units\n\nThe second step after getting your data into R is data wrangling in order to modify the data so that you can perform an analysis. Luckily R has a whole suite of tools to makes this process easy and most importantly reproducible. This less will teach you practices for effective data wrangling."
  },
  {
    "objectID": "readings/index_R03.html#the-tidyverse",
    "href": "readings/index_R03.html#the-tidyverse",
    "title": "Data Wrangling with tidyverse",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\nThe Tidyverse which is a collection of R packages that share an underlying design, syntax, and grammer to streamline many main functions used in data science. You can install the complete tidyverse with install.packages(\"tidyverse\"), once the package is installed you can load it using library(tidyverse).\nThe packages installed in tidyverse include:\n\ndplyr and tidyr are packages for data manipulation to subset, re-arrange, and format your dataset.\ntibble is a tidy way of displaying data frames that are easier to view.\nreadr is a tidy way to input or read in data into R that we covered last class.\npurrr is a functional programming toolkit to handle looping functions.\nstringr is a way of handling text and character strings.\nforcats is a package providing tools to handle categorical variables and discrete (non-continuous) variables.\nlubridate is a package for working with times and dates.\nggplot2 is a graphic package to plot your data. We will cover this package in the next class!\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are nice cheatsheets for each of the packages to demonstrate what they do in detail!\nDon’t forget to check out the Help section in RStudio for any function you come across."
  },
  {
    "objectID": "readings/index_R03.html#the-big-picture",
    "href": "readings/index_R03.html#the-big-picture",
    "title": "Data Wrangling with tidyverse",
    "section": "The BIG picture",
    "text": "The BIG picture\nWe have now progressed to the Tidy part of the data analysis pipeline!\n\n\nFigure 1: In our model of the data science process, you start with data import and tidying. Next, you understand your data with an iterative cycle of transforming, visualizing, and modeling. You finish the process by communicating your results to other humans."
  },
  {
    "objectID": "readings/index_R03.html#examples-of-some-not-so-tidy-data",
    "href": "readings/index_R03.html#examples-of-some-not-so-tidy-data",
    "title": "Data Wrangling with tidyverse",
    "section": "Examples of some not so tidy data",
    "text": "Examples of some not so tidy data\nHave you seen or even generated data that looks like this?\n\nexample 1\n\n\n\n\n\nexample 2\n\n\n\n\n\nexample 3\nsome of my data!"
  },
  {
    "objectID": "readings/index_R03.html#tidy-data-with-tidyr",
    "href": "readings/index_R03.html#tidy-data-with-tidyr",
    "title": "Data Wrangling with tidyverse",
    "section": "Tidy data with tidyr",
    "text": "Tidy data with tidyr\ntidyr is a package with tools that help you create tidy data. tidyr functions reform the data so that other R functions can perform manipulation or analysis on it.\n\nWide vs Long\nWide data puts many variables for each observation in the same row. An example is a time series, where each time point for an observation is in a different column. Data in this format can be easier to data enter when the data is collected. However, it is difficult for R to then plot or evaluate across time, but we need to convert it in R.\nHere is an example of wide data, you can see different metrics across time are represented across the columns for each country.\n\nThis is an example of long data. Now each value is in a single column. The next step would be to split the observation type and the year into 2 separate columns that we’ll cover below.\n\n\n\n\n\nTo convert between wide to longer we use the function pivot_longer() and to convert between long to wide we use the function pivot_wider(). The pivot_longer() function takes the main arguments data, cols, names_to, and values_to. data is the input data (the wide data frame). cols is the columns of the data you want to convert, you don’t have to use the whole data set you can only select a few columns if you want. names_to is what you want to name the header of the new variable you are creating this is often called the “key” or for example the observation type in the above example. values_to is what you want to call the actual data “value” this could be the concentration or the units of the measurement.\n\nLets look at an example in code!\n\nlibrary(tidyverse)\n\ndata &lt;- read_csv(\"data/wide_data.csv\")\nglimpse(data) # first look at the data\n\nRows: 10\nColumns: 11\n$ Dam               &lt;chr&gt; \"HFD-25-01\", \"HFD-25-02\", \"HFD-25-03\", \"HFD-25-04\", …\n$ Group             &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"HFD\", \"HFD\", \"HF…\n$ `Baseline Weight` &lt;dbl&gt; 21.1, 22.4, 22.4, 22.8, 22.8, 21.8, 23.0, 23.6, 22.0…\n$ `Wk1 Weight`      &lt;dbl&gt; 20.0, 20.9, 22.2, 22.4, 21.8, 23.8, 25.8, 22.7, 23.2…\n$ `Wk2 Weight`      &lt;dbl&gt; 20.0, 21.2, 22.6, 23.1, 22.5, 23.6, 26.4, 21.8, 22.7…\n$ `Wk3 Weight`      &lt;dbl&gt; 19.9, 22.2, 22.3, 22.4, 22.0, 26.2, 29.4, 24.9, 25.4…\n$ `Wk4 Weight`      &lt;dbl&gt; 20.3, 21.7, 22.3, 24.1, 21.8, 28.1, 31.6, 22.9, 26.7…\n$ `Wk5 Weight`      &lt;dbl&gt; 20.7, 22.4, 22.6, 25.0, 21.9, 27.8, 35.8, 23.8, 31.5…\n$ `Wk6 Weight`      &lt;dbl&gt; 21.4, 22.7, 23.4, 23.1, 23.4, 31.6, 34.8, 25.5, 33.8…\n$ `Wk7 Weight`      &lt;dbl&gt; 21.1, 22.0, 23.2, 24.1, 23.0, 32.0, 36.7, 28.5, 30.8…\n$ `Wk8 Weight`      &lt;dbl&gt; 21.1, 21.0, 22.0, 23.2, 22.8, 32.8, 38.2, 25.3, 33.3…\n\n\nYou can see the weight each week but its spread across columns lets convert to long format\nUsing the pivot_longer() function we tell it to take columns 3 through 11 and put the column names into a new column called week and the weights into a new column called grams using this code\n\nlong.data &lt;- data |&gt; \n  pivot_longer(cols = c(3:11), names_to = \"week\", values_to = \"grams\")\n\nglimpse(long.data) # this is what the new data looks like. \n\nRows: 90\nColumns: 4\n$ Dam   &lt;chr&gt; \"HFD-25-01\", \"HFD-25-01\", \"HFD-25-01\", \"HFD-25-01\", \"HFD-25-01\",…\n$ Group &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"…\n$ week  &lt;chr&gt; \"Baseline Weight\", \"Wk1 Weight\", \"Wk2 Weight\", \"Wk3 Weight\", \"Wk…\n$ grams &lt;dbl&gt; 21.1, 20.0, 20.0, 19.9, 20.3, 20.7, 21.4, 21.1, 21.1, 22.4, 20.9…\n\n\nThe data is separated into weeks and weight. Now we could plot it across time.\n\n\n\n\n\n\n\n\n\nOther times we need the data to be wide format if we want to do computations on the data. For example, maybe we want to determine the percent weight change from baseline and plot that instead of the absolute weight. Lets convert it back to wide format using pivot_wider().\nWe tell pivot_wider() it to take the “week” column and make a new columnn for each unique value and put the value of grams in the cells as the values for each unique week column.\n\nwide.data &lt;- pivot_wider(long.data, names_from = week, values_from = grams)\nglimpse(wide.data) # look at the data again\n\nRows: 10\nColumns: 11\n$ Dam               &lt;chr&gt; \"HFD-25-01\", \"HFD-25-02\", \"HFD-25-03\", \"HFD-25-04\", …\n$ Group             &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"HFD\", \"HFD\", \"HF…\n$ `Baseline Weight` &lt;dbl&gt; 21.1, 22.4, 22.4, 22.8, 22.8, 21.8, 23.0, 23.6, 22.0…\n$ `Wk1 Weight`      &lt;dbl&gt; 20.0, 20.9, 22.2, 22.4, 21.8, 23.8, 25.8, 22.7, 23.2…\n$ `Wk2 Weight`      &lt;dbl&gt; 20.0, 21.2, 22.6, 23.1, 22.5, 23.6, 26.4, 21.8, 22.7…\n$ `Wk3 Weight`      &lt;dbl&gt; 19.9, 22.2, 22.3, 22.4, 22.0, 26.2, 29.4, 24.9, 25.4…\n$ `Wk4 Weight`      &lt;dbl&gt; 20.3, 21.7, 22.3, 24.1, 21.8, 28.1, 31.6, 22.9, 26.7…\n$ `Wk5 Weight`      &lt;dbl&gt; 20.7, 22.4, 22.6, 25.0, 21.9, 27.8, 35.8, 23.8, 31.5…\n$ `Wk6 Weight`      &lt;dbl&gt; 21.4, 22.7, 23.4, 23.1, 23.4, 31.6, 34.8, 25.5, 33.8…\n$ `Wk7 Weight`      &lt;dbl&gt; 21.1, 22.0, 23.2, 24.1, 23.0, 32.0, 36.7, 28.5, 30.8…\n$ `Wk8 Weight`      &lt;dbl&gt; 21.1, 21.0, 22.0, 23.2, 22.8, 32.8, 38.2, 25.3, 33.3…\n\n\nNext, we want to calculate the percent weight gain by performing this calculation for each row Wk8 Weight/Baseline Weight * 100 to determine the percent of weight gain. This computation is done more easily if the data is in wide format.\n\nwide.data &lt;- wide.data |&gt; mutate(perc.gain = `Wk8 Weight`/`Baseline Weight`*100)\nglimpse(wide.data) # check it out\n\nRows: 10\nColumns: 12\n$ Dam               &lt;chr&gt; \"HFD-25-01\", \"HFD-25-02\", \"HFD-25-03\", \"HFD-25-04\", …\n$ Group             &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"HFD\", \"HFD\", \"HF…\n$ `Baseline Weight` &lt;dbl&gt; 21.1, 22.4, 22.4, 22.8, 22.8, 21.8, 23.0, 23.6, 22.0…\n$ `Wk1 Weight`      &lt;dbl&gt; 20.0, 20.9, 22.2, 22.4, 21.8, 23.8, 25.8, 22.7, 23.2…\n$ `Wk2 Weight`      &lt;dbl&gt; 20.0, 21.2, 22.6, 23.1, 22.5, 23.6, 26.4, 21.8, 22.7…\n$ `Wk3 Weight`      &lt;dbl&gt; 19.9, 22.2, 22.3, 22.4, 22.0, 26.2, 29.4, 24.9, 25.4…\n$ `Wk4 Weight`      &lt;dbl&gt; 20.3, 21.7, 22.3, 24.1, 21.8, 28.1, 31.6, 22.9, 26.7…\n$ `Wk5 Weight`      &lt;dbl&gt; 20.7, 22.4, 22.6, 25.0, 21.9, 27.8, 35.8, 23.8, 31.5…\n$ `Wk6 Weight`      &lt;dbl&gt; 21.4, 22.7, 23.4, 23.1, 23.4, 31.6, 34.8, 25.5, 33.8…\n$ `Wk7 Weight`      &lt;dbl&gt; 21.1, 22.0, 23.2, 24.1, 23.0, 32.0, 36.7, 28.5, 30.8…\n$ `Wk8 Weight`      &lt;dbl&gt; 21.1, 21.0, 22.0, 23.2, 22.8, 32.8, 38.2, 25.3, 33.3…\n$ perc.gain         &lt;dbl&gt; 100.00000, 93.75000, 98.21429, 101.75439, 100.00000,…\n\n\nnow you can see the new column of data called perc.gain as a new column at the end. We’ll talk more about the mutate() function below.\n\n\n\n\n\n\n\n\n\n\n\nUnite vs Separate\nAs we saw in countries data above sometimes we get data that has multiple pieces of data in one field. We can fix this problem with separate_wider_delim() function that will split a cell into 2 columns based on a specified delimiter. If you’ve ever used the “text to columns” feature in excel its essentially the same thing.\n\nglimpse(data) # lets look at the countries data.\n\nRows: 5,112\nColumns: 4\n$ country      &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan…\n$ continent    &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi…\n$ obstype_year &lt;chr&gt; \"pop_1952\", \"lifeExp_1952\", \"gdpPercap_1952\", \"pop_1957\",…\n$ obs_value    &lt;dbl&gt; 8.425333e+06, 2.880100e+01, 7.794453e+02, 9.240934e+06, 3…\n\n\nAs you can see the “obstype_year” has too many pieces of information in it. Before we can pivot_wider(), we need to split out the observation by type and the year using the separate_wider_delim() function. We tell the function which column to split, what the names of the splits will be, and how the fields are deliminated.\n\ndat &lt;- separate_wider_delim(data, cols = obstype_year, names = c(\"obstype\", \"year\"), delim = \"_\")\nglimpse(dat)\n\nRows: 5,112\nColumns: 5\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ obstype   &lt;chr&gt; \"pop\", \"lifeExp\", \"gdpPercap\", \"pop\", \"lifeExp\", \"gdpPercap\"…\n$ year      &lt;chr&gt; \"1952\", \"1952\", \"1952\", \"1957\", \"1957\", \"1957\", \"1962\", \"196…\n$ obs_value &lt;dbl&gt; 8.425333e+06, 2.880100e+01, 7.794453e+02, 9.240934e+06, 3.03…\n\n\nNow the data is in a format that can be put into pivot_wider() to get out each of the observation types.\n\n\nNesting and Unnesting\nAnother cool function related to spliting data is a function called unnest_longer. We talked a little in the first class about a class of data called list which is very flexible on what can be put there, but you need special functions to access that data. Unnest is a way to unnest the listed items that are embedded into a dataframe. Lets look at the taylor swift data as an example.\n\nlibrary(taylor)\n\ntaylor &lt;- taylor_album_songs\nglimpse(taylor) # look at some data\n\nRows: 240\nColumns: 29\n$ album_name          &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ ep                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ album_release       &lt;date&gt; 2006-10-24, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ track_number        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ track_name          &lt;chr&gt; \"Tim McGraw\", \"Picture To Burn\", \"Teardrops On My …\n$ artist              &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ featuring           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ bonus_track         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ promotional_release &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ single_release      &lt;date&gt; 2006-06-19, 2008-02-03, 2007-02-19, NA, NA, NA, N…\n$ track_release       &lt;date&gt; 2006-06-19, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ danceability        &lt;dbl&gt; 0.580, 0.658, 0.621, 0.576, 0.418, 0.589, 0.479, 0…\n$ energy              &lt;dbl&gt; 0.491, 0.877, 0.417, 0.777, 0.482, 0.805, 0.578, 0…\n$ key                 &lt;int&gt; 0, 7, 10, 9, 5, 5, 2, 8, 4, 2, 2, 8, 7, 4, 10, 5, …\n$ loudness            &lt;dbl&gt; -6.462, -2.098, -6.941, -2.881, -5.769, -4.055, -4…\n$ mode                &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ speechiness         &lt;dbl&gt; 0.0251, 0.0323, 0.0231, 0.0324, 0.0266, 0.0293, 0.…\n$ acousticness        &lt;dbl&gt; 0.57500, 0.17300, 0.28800, 0.05100, 0.21700, 0.004…\n$ instrumentalness    &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, …\n$ liveness            &lt;dbl&gt; 0.1210, 0.0962, 0.1190, 0.3200, 0.1230, 0.2400, 0.…\n$ valence             &lt;dbl&gt; 0.425, 0.821, 0.289, 0.428, 0.261, 0.591, 0.192, 0…\n$ tempo               &lt;dbl&gt; 76.009, 105.586, 99.953, 115.028, 175.558, 112.982…\n$ time_signature      &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ duration_ms         &lt;int&gt; 232107, 173067, 203040, 199200, 239013, 207107, 24…\n$ explicit            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ key_name            &lt;chr&gt; \"C\", \"G\", \"A#\", \"A\", \"F\", \"F\", \"D\", \"G#\", \"E\", \"D\"…\n$ mode_name           &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"major\", \"majo…\n$ key_mode            &lt;chr&gt; \"C major\", \"G major\", \"A# major\", \"A major\", \"F ma…\n$ lyrics              &lt;list&gt; [&lt;tbl_df[55 x 4]&gt;], [&lt;tbl_df[33 x 4]&gt;], [&lt;tbl_df[…\n\n\nIf you look at the last line the lyrics is a list which is a tibble (or table) that is the all the lines of lyrics for each song. Since each line is a song and the lyrics can be many lines long its a nice way to be able to store a table inside a data frame. cool huh! If you wanted to access the lyrics you need to unnest the lyric data in the table. The function unnest_longer() does that.\n\nlyr &lt;- taylor |&gt; select(album_name, track_name, lyrics) |&gt; unnest_longer(lyrics)\ndim(lyr) # the dimensions of the data number of rows and number of columns\n\n[1] 12151     3\n\nhead(lyr)\n\n# A tibble: 6 × 3\n  album_name   track_name lyrics$line $lyric            $element $element_artist\n  &lt;chr&gt;        &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;          \n1 Taylor Swift Tim McGraw           1 \"He said the way… Verse 1  Taylor Swift   \n2 Taylor Swift Tim McGraw           2 \"Put those Georg… Verse 1  Taylor Swift   \n3 Taylor Swift Tim McGraw           3 \"I said, \\\"That'… Verse 1  Taylor Swift   \n4 Taylor Swift Tim McGraw           4 \"Just a boy in a… Verse 1  Taylor Swift   \n5 Taylor Swift Tim McGraw           5 \"That had a tend… Verse 1  Taylor Swift   \n6 Taylor Swift Tim McGraw           6 \"On backroads at… Verse 1  Taylor Swift   \n\n\nYou can see the dataframe is much longer (12,151 rows) as each row is a lyric. If you wanted to see how many times specific words were used this is how you could do it."
  },
  {
    "objectID": "readings/index_R03.html#transform-data-with-dplyr",
    "href": "readings/index_R03.html#transform-data-with-dplyr",
    "title": "Data Wrangling with tidyverse",
    "section": "Transform data with dplyr",
    "text": "Transform data with dplyr\n\nOnce the data is tidy we can do transformation and calculations for downstream analysis. You’ve already seen some of these tools in action throughout the course. We’ll formally go over them now so you’ll recognize it when you see it!\nThe functions we’ll use in combinations include:\n\nselect()\nfilter()\narrange()\ndistinct()\ncount()\nsummarize()\ngroup_by()\nmutate()\n\n\n\n\n\n\n\nRemember your pipes!\n\n\n\n%&gt;% & |&gt; These allow you to link functions together\n\n\n\n\n\n\n\n\nA note on style\n\n\n\nWhen using pipes it is tidy style to put everything after a pipe on a new line so each line of code is noting one operation so its easier to understand. This may seem silly when you’re only doing one operation, but the standard style is for it to be on a new line.\n\n\n\nSummary functions\n\nselect()\n\nlibrary(palmerpenguins)\nglimpse(penguins) # lets look at the penguins data\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIf we only want to look at the penguin body mass we can use the select() function to pull out only the data we want to see.\n\npenguins |&gt; select(species, island, body_mass_g) #(example, on one line of code)\n\n# A tibble: 344 × 3\n   species island    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n 1 Adelie  Torgersen        3750\n 2 Adelie  Torgersen        3800\n 3 Adelie  Torgersen        3250\n 4 Adelie  Torgersen          NA\n 5 Adelie  Torgersen        3450\n 6 Adelie  Torgersen        3650\n 7 Adelie  Torgersen        3625\n 8 Adelie  Torgersen        4675\n 9 Adelie  Torgersen        3475\n10 Adelie  Torgersen        4250\n# ℹ 334 more rows\n\n\n\n\nfilter()\nNext, if we want to select only certain rows that meet a specific criteria we use the filter() function to filter out only the data we want. We can combine this with select() too.\n\npenguins |&gt; \n  select(species, island, body_mass_g) |&gt; \n  filter(body_mass_g &gt; 4000)\n\n# A tibble: 172 × 3\n   species island    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n 1 Adelie  Torgersen        4675\n 2 Adelie  Torgersen        4250\n 3 Adelie  Torgersen        4400\n 4 Adelie  Torgersen        4500\n 5 Adelie  Torgersen        4200\n 6 Adelie  Dream            4150\n 7 Adelie  Dream            4650\n 8 Adelie  Dream            4400\n 9 Adelie  Dream            4600\n10 Adelie  Dream            4150\n# ℹ 162 more rows\n\n\n\n\narrange()\narrange will re-order the data based on a specific column or columns. It we want to quickly see the lightest or heaviest penguins, for example.\n\n# lighest\npenguins |&gt; \n  select(species, island, body_mass_g) |&gt; \n  arrange(body_mass_g)\n\n# A tibble: 344 × 3\n   species   island    body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;           &lt;int&gt;\n 1 Chinstrap Dream            2700\n 2 Adelie    Biscoe           2850\n 3 Adelie    Biscoe           2850\n 4 Adelie    Biscoe           2900\n 5 Adelie    Dream            2900\n 6 Adelie    Torgersen        2900\n 7 Chinstrap Dream            2900\n 8 Adelie    Biscoe           2925\n 9 Adelie    Dream            2975\n10 Adelie    Dream            3000\n# ℹ 334 more rows\n\n# heaviest using the `desc()` descending function.\npenguins |&gt; \n  select(species, island, body_mass_g) |&gt; \n  arrange(desc(body_mass_g))\n\n# A tibble: 344 × 3\n   species island body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;\n 1 Gentoo  Biscoe        6300\n 2 Gentoo  Biscoe        6050\n 3 Gentoo  Biscoe        6000\n 4 Gentoo  Biscoe        6000\n 5 Gentoo  Biscoe        5950\n 6 Gentoo  Biscoe        5950\n 7 Gentoo  Biscoe        5850\n 8 Gentoo  Biscoe        5850\n 9 Gentoo  Biscoe        5850\n10 Gentoo  Biscoe        5800\n# ℹ 334 more rows\n\n\n\n\ndistinct()\ndistinct is a function similar to unique. It identifies the distinct or unique features across one or multiple columns\n\n# what are the distinct islands in the data? Note it is not counting the occurances (thats a different function)\npenguins |&gt; \n  distinct(island)\n\n# A tibble: 3 × 1\n  island   \n  &lt;fct&gt;    \n1 Torgersen\n2 Biscoe   \n3 Dream    \n\n# what are the distinct islands and species?\npenguins |&gt; \n  distinct(island, species)\n\n# A tibble: 5 × 2\n  island    species  \n  &lt;fct&gt;     &lt;fct&gt;    \n1 Torgersen Adelie   \n2 Biscoe    Adelie   \n3 Dream     Adelie   \n4 Biscoe    Gentoo   \n5 Dream     Chinstrap\n\n\n\n\ncount()\ncount is similar to the table() function in base R. It will tally the number of times that unique variable is listed. So its similar to distinct, but it includes the count of the number of occurances.\n\npenguins |&gt; \n  count(island, species)\n\n# A tibble: 5 × 3\n  island    species       n\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;\n1 Biscoe    Adelie       44\n2 Biscoe    Gentoo      124\n3 Dream     Adelie       56\n4 Dream     Chinstrap    68\n5 Torgersen Adelie       52\n\n\n\n\nsummarize\nsummarize is a powerful tool that summarizes many variables across the data. It can calculate mean, median, standard deviation, variance, sum, and many more features. It outputs a new table that includes just the summary data.\n\n# lets look at the average mass of all the penguins\npenguins |&gt; \n  summarize(avg_mass = mean(body_mass_g))\n\n# A tibble: 1 × 1\n  avg_mass\n     &lt;dbl&gt;\n1       NA\n\n# this doesn't work... why? because there are missing values! We have to add a special note to remove the NA values using the na.rm = TRUE setting. We can also look at more than one summary data at a time!\n\npenguins |&gt; \n  summarize(avg_mass = mean(body_mass_g, na.rm = TRUE),\n            sd = sd(body_mass_g, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  avg_mass    sd\n     &lt;dbl&gt; &lt;dbl&gt;\n1    4202.  802.\n\n\n\n\ngroup_by\nIf we wanted to summarize the data across categorical variables in the data we use the group_by() function that will first group the data and then perform a summary function on each grouping.\n\n# just running group_by() you won't see any difference, but under the hood it is sorting the observations.\npenguins |&gt; \n  group_by(island, species)\n\n# A tibble: 344 × 8\n# Groups:   island, species [5]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins |&gt; \n  group_by(island, species) |&gt; \n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE))\n\n`summarise()` has grouped output by 'island'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n# Groups:   island [3]\n  island    species   mean_mass\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;\n1 Biscoe    Adelie        3710.\n2 Biscoe    Gentoo        5076.\n3 Dream     Adelie        3688.\n4 Dream     Chinstrap     3733.\n5 Torgersen Adelie        3706.\n\n# we can also add in multiple variables for spread or count\npenguins |&gt; \n  group_by(island, species) |&gt; \n  summarize(n=n(), \n            mean_mass = mean(body_mass_g, na.rm = TRUE), \n            sd = sd(body_mass_g, na.rm = TRUE))\n\n`summarise()` has grouped output by 'island'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 5\n# Groups:   island [3]\n  island    species       n mean_mass    sd\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Biscoe    Adelie       44     3710.  488.\n2 Biscoe    Gentoo      124     5076.  504.\n3 Dream     Adelie       56     3688.  455.\n4 Dream     Chinstrap    68     3733.  384.\n5 Torgersen Adelie       52     3706.  445.\n\n\nExample calculations you can run in summarize: n(), sum(),mean(), median(), min(), max(), IQR(), sd(), var()\nThese commands output a new table and did not modify the original data. However, sometimes you need to add a new calculation or mutate how the data is presented you can do that with the mutate function. If you paid attention above when we looked at the percent weight gain on high fat diet we used the mutate() function to add the new variable to the data.\n\n\n\nmutate()\nmutate() takes all the same functions that summarize did above. The big difference is that instead of returning a new table or a single value it returns the whole dataset with a new column.\n\n# if we run the same command as above but with mutate instead of summarize look at how the results differ\npenguins |&gt; \n  group_by(island, species) |&gt; \n  mutate(mean_mass = mean(body_mass_g, na.rm = TRUE)) |&gt; glimpse()\n\nRows: 344\nColumns: 9\nGroups: island, species [5]\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ mean_mass         &lt;dbl&gt; 3706.373, 3706.373, 3706.373, 3706.373, 3706.373, 37…\n\n\nNote the mean_mass column that has now put the mean value in every row. Be aware of the type of output you will get out of the functions you implement!!\n\nLets look at another example\nremember our wide.data that was weight measurements of animals on a high fat diet\n\nwide.data &lt;- read.csv(\"data/wide_data.csv\")\nglimpse(wide.data)\n\nRows: 10\nColumns: 11\n$ Dam             &lt;chr&gt; \"HFD-25-01\", \"HFD-25-02\", \"HFD-25-03\", \"HFD-25-04\", \"H…\n$ Group           &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"HFD\", \"HFD\", \"HFD\"…\n$ Baseline.Weight &lt;dbl&gt; 21.1, 22.4, 22.4, 22.8, 22.8, 21.8, 23.0, 23.6, 22.0, …\n$ Wk1.Weight      &lt;dbl&gt; 20.0, 20.9, 22.2, 22.4, 21.8, 23.8, 25.8, 22.7, 23.2, …\n$ Wk2.Weight      &lt;dbl&gt; 20.0, 21.2, 22.6, 23.1, 22.5, 23.6, 26.4, 21.8, 22.7, …\n$ Wk3.Weight      &lt;dbl&gt; 19.9, 22.2, 22.3, 22.4, 22.0, 26.2, 29.4, 24.9, 25.4, …\n$ Wk4.Weight      &lt;dbl&gt; 20.3, 21.7, 22.3, 24.1, 21.8, 28.1, 31.6, 22.9, 26.7, …\n$ Wk5.Weight      &lt;dbl&gt; 20.7, 22.4, 22.6, 25.0, 21.9, 27.8, 35.8, 23.8, 31.5, …\n$ Wk6.Weight      &lt;dbl&gt; 21.4, 22.7, 23.4, 23.1, 23.4, 31.6, 34.8, 25.5, 33.8, …\n$ Wk7.Weight      &lt;dbl&gt; 21.1, 22.0, 23.2, 24.1, 23.0, 32.0, 36.7, 28.5, 30.8, …\n$ Wk8.Weight      &lt;dbl&gt; 21.1, 21.0, 22.0, 23.2, 22.8, 32.8, 38.2, 25.3, 33.3, …\n\n# mutate is doing the calculation of percent gain for each row so the output is a unique value in each row instead of repeating the mean of a column of data. \nwide.data |&gt; \n  mutate(perc.gain = Wk8.Weight/Baseline.Weight*100)\n\n         Dam Group Baseline.Weight Wk1.Weight Wk2.Weight Wk3.Weight Wk4.Weight\n1  HFD-25-01   CON            21.1       20.0       20.0       19.9       20.3\n2  HFD-25-02   CON            22.4       20.9       21.2       22.2       21.7\n3  HFD-25-03   CON            22.4       22.2       22.6       22.3       22.3\n4  HFD-25-04   CON            22.8       22.4       23.1       22.4       24.1\n5  HFD-25-05   CON            22.8       21.8       22.5       22.0       21.8\n6  HFD-25-06   HFD            21.8       23.8       23.6       26.2       28.1\n7  HFD-25-07   HFD            23.0       25.8       26.4       29.4       31.6\n8  HFD-25-08   HFD            23.6       22.7       21.8       24.9       22.9\n9  HFD-25-09   HFD            22.0       23.2       22.7       25.4       26.7\n10 HFD-25-10   HFD            21.0       22.1       22.2       24.6       23.3\n   Wk5.Weight Wk6.Weight Wk7.Weight Wk8.Weight perc.gain\n1        20.7       21.4       21.1       21.1 100.00000\n2        22.4       22.7       22.0       21.0  93.75000\n3        22.6       23.4       23.2       22.0  98.21429\n4        25.0       23.1       24.1       23.2 101.75439\n5        21.9       23.4       23.0       22.8 100.00000\n6        27.8       31.6       32.0       32.8 150.45872\n7        35.8       34.8       36.7       38.2 166.08696\n8        23.8       25.5       28.5       25.3 107.20339\n9        31.5       33.8       30.8       33.3 151.36364\n10       22.7       25.3       25.4       25.9 123.33333\n\n\nThis is what we wanted a unique variable for every row rather than a summary data such as mean.\n\n\n\nrename()\nAnother cool function is rename(). There are many ways to rename columns in R, but rename() is in the convenient tidy style. In the weight data the word “weight” is repeated over and over and in the graph it can clutter the axis. We can rename it for brevity. For rename() the function takes the format new_name = old_name. Alternatively, you can use the column number too\n\nwide.data |&gt; rename(baseline = Baseline.Weight,\n                    Wk1 = 4,\n                    Wk2 = 5,\n                    Wk3 = 6,\n                    Wk4 = 7,\n                    Wk5 = 8,\n                    Wk6 = 9,\n                    Wk7 = 10,\n                    Wk8 = 11)\n\n         Dam Group baseline  Wk1  Wk2  Wk3  Wk4  Wk5  Wk6  Wk7  Wk8\n1  HFD-25-01   CON     21.1 20.0 20.0 19.9 20.3 20.7 21.4 21.1 21.1\n2  HFD-25-02   CON     22.4 20.9 21.2 22.2 21.7 22.4 22.7 22.0 21.0\n3  HFD-25-03   CON     22.4 22.2 22.6 22.3 22.3 22.6 23.4 23.2 22.0\n4  HFD-25-04   CON     22.8 22.4 23.1 22.4 24.1 25.0 23.1 24.1 23.2\n5  HFD-25-05   CON     22.8 21.8 22.5 22.0 21.8 21.9 23.4 23.0 22.8\n6  HFD-25-06   HFD     21.8 23.8 23.6 26.2 28.1 27.8 31.6 32.0 32.8\n7  HFD-25-07   HFD     23.0 25.8 26.4 29.4 31.6 35.8 34.8 36.7 38.2\n8  HFD-25-08   HFD     23.6 22.7 21.8 24.9 22.9 23.8 25.5 28.5 25.3\n9  HFD-25-09   HFD     22.0 23.2 22.7 25.4 26.7 31.5 33.8 30.8 33.3\n10 HFD-25-10   HFD     21.0 22.1 22.2 24.6 23.3 22.7 25.3 25.4 25.9"
  },
  {
    "objectID": "readings/index_R03.html#stringr-lubridate-forcats-purrr",
    "href": "readings/index_R03.html#stringr-lubridate-forcats-purrr",
    "title": "Data Wrangling with tidyverse",
    "section": "stringr, lubridate, forcats, purrr",
    "text": "stringr, lubridate, forcats, purrr\nThese packages contain function to handle other tricky data or processes in R that we won’t cover as heavily here. I wanted to just give a few notes.\n\nstringr\nstringr is for handing text and character strings to perform operations such as\n\nfind character strings (ie, find the work “love” in song lyrics)\nsubset strings of characters (ie, extract the first or last 5 characters of a string)\njoin or split charcter strings (ie, extract the year, month, and day from a date string\n\n\n\nlubridate\nlubridate is used to more easily handle dates and times. It can handle am/pm/24-h time notations, and change dates from yymmdd to mmddyy and more formats.\n\n\nforcats\nforcats is for working with factors and categorical data in R. Some of the functions are factoring or converting a character vector to a factor vector. Setting the levels of the factors, such as which group is the reference group. It can combine 2 sets of factors or reorder how the factors are plotted.\n\n\npurrr\npurrr is a powerful set of functions for performing for loops or iterations of operations across your data either row-wise or column-wise. This is commonly done in data science, but is a slighly more advanced topic. I encourage you to read about purrr and for loops in R. If we have time at the end of the course we can cover this topic.\n\n\n\n\n\n\nNote\n\n\n\nDo you learn better by listening? Check out these videos covering some of this material.\nIntro to Tidyverse\nData Wrangling this video is a little older and has some different syntax on the functions, but its all the same logic given by Garrett Grolemund himself (co-author of R for Data Science book)"
  },
  {
    "objectID": "readings/index_R05.html",
    "href": "readings/index_R05.html",
    "title": "PCA with tidymodels",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will be able to:\n\nDefine dimension reduction\nExplain why dimension reduction is used\nBuild a model to perform PCA on a dataset\n\n\n\nBefore class, watch this video from Data Scientists Julie Silge from RStudio. She does a real-time data analysis using Principal Component Analysis (PCA) of the best hip hop songs of all time according to critics ratings. This video is not a detailed explanation of what PCA is or how it works (there is a lot of that maths on the internet if you want). In contrast, it is a live, real-time, analysis that asks the question what song features make a hip hop song the best. I want you to see that PCA is often a first line of inquiry when exploring a dataset.\n\nFor background, TidyTuesday is a weekly podcast and community activity that brings an interesting dataset to the data science community each week to do some cool plotting or analysis on. It provides interesting data to use for teaching purposes or code testing.\nThis video goes pretty fast but all the code she uses is below the video."
  },
  {
    "objectID": "readings/index_R07.html",
    "href": "readings/index_R07.html",
    "title": "Building a Quarto Website and Using GitHub",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will be able to:\n\nCreate a GitHub repository\nSetup a Quarto Website\nPublish your Quarto website through Github Pages\nCustomize your Quarto Website\n\n\n\n\nPre-class Tasks\nGitHub is a web-based platform that allows you to save files online, track changes to files, and share project files easily for improved collaboration. It has gained popularity in the scientific community as an effective tool for documenting research scripts, tracking edits or errors during analysis, and sharing project workflows with others.\nPlease create accounts with GitHub and GitHub Desktop before our next class time:\n\nSet up a Github Account Online - Follow the steps from the Signing up for a new personal account section.\nSet up Github Desktop Account on your computer - If you run into any problems with the installer for your operating system, email us as soon as possible.\n\nPlease write down your login information for Friday’s class!"
  }
]