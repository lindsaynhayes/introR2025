---
title: "Data Wrangling with tidyverse"
description: "Reading for Class 03"
author: 
  - name: Lindsay Hayes
    url: https://lindsaynhayes.github.io
    affiliation: Department of Cell Biology, OUHSC
date: 2025-07-08
draft: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| message: false
library(here)
```

::: callout-note
### Learning objectives

**At the end of this lesson you will be able to:**

-   Recognize and define an untidy dataset
-   Recognize and convert between long and wide data formats
-   Understand when to use both formats
-   Pipe functions together to perform multiple commands in sequence
:::

## The problem with messy data

Most scientists are not trained to collect and organize data using `tidydata` principles. The results are data sets which are hard to follow, difficult to repeat, and challenging to report to journals. Understanding how to organize and store your data and analyses will make your life easier! Moreover, many journals request raw data submissions along with your manuscript. If you've analyzed and organized all your data in `R` using `tidydata` principles this process becomes a super simple, rather than tracking all the data across messy spreadsheets.

> "Tidy datasets are all alike, but every messy dataset is messy in its own way."\
> --- Hadley Wickham

## The principles of `tidydata`

1.  **Treat your raw data as immutable.**

*Try to manipulate the original form of the data as little as possible. The reason is to make the whole data analysis pipeline reproducible and transparent. If you start changing fields on the data spreadsheet you can't track where those values came from. Manipulating data with code also decreases the chances for errors being introduced to the raw data.*

2.  **Each variable is a column; each column is a variable.**

3.  **Each observation is a row; each row is an observation.**

4.  **Each value is a cell; each cell is a single value.**

For example, take the following data

```{r}
#| echo: false

groups <- c("Con_male", "CON_female", "Exp_m", "Exp_F")
val <- round(rnorm(4, mean = 100, sd = 25),0)
df <- as.data.frame(cbind(groups, val))
df
```

**Why might this data be a problem?**

-   combining 2 observations into one field (group and sex).
-   changing the notation styles

Here's another example

```{r}
#| echo: false

groups <- c("CON", "CON", "EXP", "EXP")
val <- round(rnorm(4, mean = 100, sd = 25),0)
val2 <- paste(val, "ug/ul", sep = "_")
df <- as.data.frame(cbind(groups, val2))
df
```

**Why might this data be a problem?**

-   the data field contains 2 values the numeric value and the units

The second step after getting your data into R is **data wrangling** in order to modify the data so that you can perform an analysis. Luckily R has a whole suite of tools to makes this process easy and most importantly reproducible. This less will teach you practices for effective data wrangling.

## The Tidyverse

<img src="figs/tidy_hex.png" width="50%" style="float:right"/>

The [Tidyverse](https://www.tidyverse.org) which is a collection of R packages that share an underlying design, syntax, and grammer to streamline many main functions used in data science. You can install the complete tidyverse with `install.packages("tidyverse")`, once the package is installed you can load it using `library(tidyverse)`.

The packages installed in tidyverse include:

-   `dplyr` and `tidyr` are packages for data manipulation to subset, re-arrange, and format your dataset.
-   `tibble` is a tidy way of displaying data frames that are easier to view.
-   `readr` is a tidy way to input or read in data into R that we covered last class.
-   `purrr` is a functional programming toolkit to handle looping functions.
-   `stringr` is a way of handling text and character strings.
-   `forcats` is a package providing tools to handle categorical variables and discrete (non-continuous) variables.
-   `lubridate` is a package for working with times and dates.
-   `ggplot2` is a graphic package to plot your data. We will cover this package in the next class!

::: callout-important
There are nice [cheatsheets](https://posit.co/resources/cheatsheets/) for each of the packages to demonstrate what they do in detail!

Don't forget to check out the *Help* section in RStudio for any function you come across.
:::

## The BIG picture

We have now progressed to the Tidy part of the data analysis pipeline!

<img src="figs/tidy.png" width="90%" style="display:block; margin-left: auto; margin-right: auto"/>

<figcaption>Figure 1: In our model of the data science process, you start with data import and tidying. Next, you understand your data with an iterative cycle of transforming, visualizing, and modeling. You finish the process by communicating your results to other humans.</figcaption>

## Examples of some not so tidy data

Have you seen or even generated data that looks like this?

#### example 1

::: img-float
<img src="figs/untidy1.png" width="90%" style="float:center"/>
:::

#### example 2

::: img-float
<img src="figs/untidy2.png" width="90%" style="float:center"/>
:::

#### example 3
some of my data! 

::: {.img-float} 
<img src="figs/untidy3.png" width="90%" style="float:center"/> 
:::

## Tidy data with `tidyr`

`tidyr` is a package with tools that help you create **tidy data**. `tidyr` functions reform the data so that other R functions can perform manipulation or analysis on it.

### Wide vs Long

Wide data puts many variables for each observation in the same row. An example is a time series, where each time point for an observation is in a different column. Data in this format can be easier to data enter when the data is collected. However, it is difficult for R to then plot or evaluate across time, but we need to convert it in R.

Here is an example of wide data, you can see different metrics across time are represented across the columns for each country.

![](figs/tidyr-wide.png)

This is an example of long data. Now each value is in a single column. The next step would be to split the observation type and the year into 2 separate columns that we'll cover below.

![](figs/tidyr-long.png){fig-align="center" width="341"}

To convert between wide to longer we use the function `pivot_longer()` and to convert between long to wide we use the function `pivot_wider()`. The `pivot_longer()` function takes the main arguments `data`, `cols`, `names_to`, and `values_to`. `data` is the input data (the wide data frame). `cols` is the columns of the data you want to convert, you don't have to use the whole data set you can only select a few columns if you want. `names_to` is what you want to name the header of the new variable you are creating this is often called the "key" or for example the observation type in the above example. `values_to` is what you want to call the actual data "value" this could be the concentration or the units of the measurement.

![](figs/tidyr-wide_long2.png)

Lets look at an example in code!

```{r}
#| label: wide2long
#| message: false
#| fig-width: 5
#| fig-height: 4

library(tidyverse)

data <- read_csv("data/wide_data.csv")
glimpse(data) # first look at the data
```
You can see the weight each week but its spread across columns lets convert to long format

Using the `pivot_longer()` function we tell it to take columns 3 through 11 and put the column names into a new column called week and the weights into a new column called grams using this code
```{r}
long.data <- data |> 
  pivot_longer(cols = c(3:11), names_to = "week", values_to = "grams")

glimpse(long.data) # this is what the new data looks like. 
```
The data is separated into weeks and weight. Now we could plot it across time.

```{r}
#| echo: false

ggplot(long.data, aes(x=week, y=grams, group = Dam, color = Group)) + geom_line() + 
  theme_classic() +
  scale_color_manual(values = c("black", "springgreen")) +
  theme(axis.text.x=element_text(angle=45, hjust = 1))
```

Other times we need the data to be **wide** format if we want to do computations on the data. For example, maybe we want to determine the percent weight change from baseline and plot that instead of the absolute weight. Lets convert it back to wide format using `pivot_wider()`.

We tell `pivot_wider()` it to take the "week" column and make a new columnn for each unique value and put the value of grams in the cells as the values for each unique week column.
```{r}
#| label: long2wide
#| message: false

wide.data <- pivot_wider(long.data, names_from = week, values_from = grams)
glimpse(wide.data) # look at the data again
```

Next, we want to calculate the percent weight gain by performing this calculation for each row `Wk8 Weight`/`Baseline Weight` * 100 to determine the percent of weight gain. This computation is done more easily if the data is in wide format.

```{r}
#| message: false
#| fig-width: 4
#| fig-height: 4
wide.data <- wide.data |> mutate(perc.gain = `Wk8 Weight`/`Baseline Weight`*100)
glimpse(wide.data) # check it out
```
now you can see the new column of data called perc.gain as a new column at the end. We'll talk more about the `mutate()` function below. 

```{r}
#| message: false
#| echo: false
#| fig-width: 4
#| fig-height: 4
ggplot(wide.data, aes(x=Group, y=perc.gain, fill = Group)) + geom_boxplot(outlier.shape = NA) + 
  theme_classic() + geom_point() +
  scale_fill_manual(values = c("grey", "springgreen")) + 
  ylab("% Weight Gain") + ylim(90,180)
```

### Unite vs Separate
As we saw in countries data above sometimes we get data that has multiple pieces of data in one field. We can fix this problem with `separate_wider_delim()` function that will split a cell into 2 columns based on a specified delimiter. If you've ever used the "text to columns" feature in excel its essentially the same thing.

```{r}
#| echo: false

library(gapminder)
data <- gapminder
data <- pivot_longer(data, cols = c(pop, lifeExp, gdpPercap), names_to = "obstype", values_to = "obs_value")
data <- unite(data, col = "obstype_year", 4:3, remove = TRUE)
```


```{r}
#| label: unite.separate
#| message: false

glimpse(data) # lets look at the countries data.
```
As you can see the "obstype_year" has too many pieces of information in it. Before we can `pivot_wider()`, we need to split out the observation by type and the year using the `separate_wider_delim()` function. We tell the function which column to split, what the names of the splits will be, and how the fields are deliminated. 


```{r}
dat <- separate_wider_delim(data, cols = obstype_year, names = c("obstype", "year"), delim = "_")
glimpse(dat)
```
Now the data is in a format that can be put into pivot_wider() to get out each of the observation types.


### Nesting and Unnesting
Another cool function related to spliting data is a function called `unnest_longer`. We talked a little in the first class about a class of data called list which is very flexible on what can be put there, but you need special functions to access that data. Unnest is a way to unnest the listed items that are embedded into a dataframe. Lets look at the taylor swift data as an example.

```{r}
#| label: unnest
#| message: false

library(taylor)

taylor <- taylor_album_songs
glimpse(taylor) # look at some data
```
If you look at the last line the lyrics is a list which is a tibble (or table) that is the all the lines of lyrics for each song. Since each line is a song and the lyrics can be many lines long its a nice way to be able to store a table inside a data frame. cool huh! If you wanted to access the lyrics you need to unnest the lyric data in the table. The function unnest_longer() does that.

```{r}
lyr <- taylor |> select(album_name, track_name, lyrics) |> unnest_longer(lyrics)
dim(lyr) # the dimensions of the data number of rows and number of columns
head(lyr)
```
You can see the dataframe is much longer (12,151 rows) as each row is a lyric. If you wanted to see how many times specific words were used this is how you could do it.

## Transform data with `dplyr`

<img src="figs/transform.png" width="90%" style="display:block; margin-left: auto; margin-right: auto"/>

Once the data is tidy we can do transformation and calculations for downstream analysis. You've already seen some of these tools in action throughout the course. We'll formally go over them now so you'll recognize it when you see it!

The functions we'll use *in combinations* include: 

- `select()`
- `filter()`
- `arrange()`
- `distinct()`
- `count()`
- `summarize()`
- `group_by()`
- `mutate()`

::: callout-important
### Remember your pipes!

`%>%` & `|>` These allow you to link functions together
:::

::: callout-note
### A note on style
When using pipes it is tidy style to put everything after a pipe on a new line so each line of code is noting one operation so its easier to understand. This may seem silly when you're only doing one operation, but the standard style is for it to be on a new line. 
:::

### Summary functions

#### select()
```{r}
#| label: transform.data
#| message: false

library(palmerpenguins)
glimpse(penguins) # lets look at the penguins data
```
If we only want to look at the penguin body mass we can use the `select()` function to pull out only the data we want to see.

```{r}
penguins |> select(species, island, body_mass_g) #(example, on one line of code)
```

#### filter()
Next, if we want to select only certain rows that meet a specific criteria we use the `filter()` function to filter out only the data we want. We can combine this with `select()` too.
```{r}
penguins |> 
  select(species, island, body_mass_g) |> 
  filter(body_mass_g > 4000)
```

#### arrange()
arrange will re-order the data based on a specific column or columns. It we want to quickly see the lightest or heaviest penguins, for example.
```{r}
# lighest
penguins |> 
  select(species, island, body_mass_g) |> 
  arrange(body_mass_g)

# heaviest using the `desc()` descending function.
penguins |> 
  select(species, island, body_mass_g) |> 
  arrange(desc(body_mass_g))
```

#### distinct()
distinct is a function similar to unique. It identifies the distinct or unique features across one or multiple columns
```{r}
# what are the distinct islands in the data? Note it is not counting the occurances (thats a different function)
penguins |> 
  distinct(island)

# what are the distinct islands and species?
penguins |> 
  distinct(island, species)
```

#### count()
count is similar to the `table()` function in base R. It will tally the number of times that unique variable is listed. So its similar to distinct, but it includes the count of the number of occurances.

```{r}
penguins |> 
  count(island, species)
```

#### summarize
summarize is a powerful tool that summarizes many variables across the data. It can calculate mean, median, standard deviation, variance, sum, and many more features. It outputs a new table that includes just the summary data.

```{r}
# lets look at the average mass of all the penguins
penguins |> 
  summarize(avg_mass = mean(body_mass_g))
# this doesn't work... why? because there are missing values! We have to add a special note to remove the NA values using the na.rm = TRUE setting. We can also look at more than one summary data at a time!

penguins |> 
  summarize(avg_mass = mean(body_mass_g, na.rm = TRUE),
            sd = sd(body_mass_g, na.rm=TRUE))
```

#### group_by
If we wanted to summarize the data across categorical variables in the data we use the `group_by()` function that will first group the data and then perform a summary function on each grouping.
```{r}

# just running group_by() you won't see any difference, but under the hood it is sorting the observations.
penguins |> 
  group_by(island, species)

penguins |> 
  group_by(island, species) |> 
  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE))

# we can also add in multiple variables for spread or count
penguins |> 
  group_by(island, species) |> 
  summarize(n=n(), 
            mean_mass = mean(body_mass_g, na.rm = TRUE), 
            sd = sd(body_mass_g, na.rm = TRUE))
```
Example calculations you can run in summarize: n(), sum(),mean(), median(), min(), max(), IQR(), sd(), var()

These commands output a new table and did not modify the original data. However, sometimes you need to add a new calculation or mutate how the data is presented you can do that with the mutate function. If you paid attention above when we looked at the percent weight gain on high fat diet we used the `mutate()` function to add the new variable to the data. 

### mutate()
`mutate()` takes all the same functions that summarize did above. The big difference is that instead of returning a new table or a single value it returns the whole dataset with a new column.

```{r}
# if we run the same command as above but with mutate instead of summarize look at how the results differ
penguins |> 
  group_by(island, species) |> 
  mutate(mean_mass = mean(body_mass_g, na.rm = TRUE)) |> glimpse()
```

**Note the mean_mass column that has now put the mean value in every row. Be aware of the type of output you will get out of the functions you implement!!**

#### Lets look at another example
remember our wide.data that was weight measurements of animals on a high fat diet
```{r}
wide.data <- read.csv("data/wide_data.csv")
glimpse(wide.data)

# mutate is doing the calculation of percent gain for each row so the output is a unique value in each row instead of repeating the mean of a column of data. 
wide.data |> 
  mutate(perc.gain = Wk8.Weight/Baseline.Weight*100)
```
This is what we wanted a unique variable for every row rather than a summary data such as mean.


### rename()

Another cool function is `rename()`. There are many ways to rename columns in R, but `rename()` is in the convenient tidy style. In the weight data the word "weight" is repeated over and over and in the graph it can clutter the axis. We can rename it for brevity. For `rename()` the function takes the format new_name = old_name. Alternatively, you can use the column number too

```{r}
wide.data |> rename(baseline = Baseline.Weight,
                    Wk1 = 4,
                    Wk2 = 5,
                    Wk3 = 6,
                    Wk4 = 7,
                    Wk5 = 8,
                    Wk6 = 9,
                    Wk7 = 10,
                    Wk8 = 11)
```

## stringr, lubridate, forcats, purrr

These packages contain function to handle other tricky data or processes in R that we won't cover as heavily here. I wanted to just give a few notes. 

#### stringr
stringr is for handing text and character strings to perform operations such as

- find character strings (ie, find the work "love" in song lyrics)
- subset strings of characters (ie, extract the first or last 5 characters of a string)
- join or split charcter strings (ie, extract the year, month, and day from a date string

#### lubridate
lubridate is used to more easily handle dates and times. It can handle am/pm/24-h time notations, and change dates from yymmdd to mmddyy and more formats.

#### forcats
forcats is for working with factors and categorical data in R. Some of the functions are factoring or converting a character vector to a factor vector. Setting the levels of the factors, such as which group is the reference group. It can combine 2 sets of factors or reorder how the factors are plotted.

#### purrr
purrr is a powerful set of functions for performing for loops or iterations of operations across your data either row-wise or column-wise. This is commonly done in data science, but is a slighly more advanced topic. I encourage you to read about purrr and for loops in R. If we have time at the end of the course we can cover this topic.



::: {.callout-note}
Do you learn better by listening? Check out these videos covering some of this material.

[Intro to Tidyverse](https://www.youtube.com/watch?v=MKwyauo8nSI)

[Data Wrangling](https://posit.co/resources/videos/data-wrangling-with-r-and-rstudio/)
this video is a little older and has some different syntax on the functions, but its all the same logic given by Garrett Grolemund himself (co-author of R for Data Science book)
:::
